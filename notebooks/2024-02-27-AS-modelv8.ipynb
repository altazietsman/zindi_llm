{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/zindi_llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import torch\n",
    "from utils.embeddings import Embedder\n",
    "from utils.preprocess import create_sentance_booklet, create_faise_index\n",
    "import faiss\n",
    "from utils.utils import search_content, read_booklets, retrieve_booklet_text, clean_text, reformat_abbreviations\n",
    "from models.ollama import Ollama\n",
    "from utils.response_generator import get_response, extract_keyword, get_paragraph_info, find_matching_paragraphs\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silly Mac that forces me to change the environmental variable to prevent issues running transformers\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd  = str(pathlib.Path().cwd().parent.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet = read_booklets((pwd + \"/data/data/booklets/\"))\n",
    "df_train = pd.read_csv(pwd + \"/data/data/Train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Clean some of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet['cleanText'] = df_booklet['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data\n",
    "We will refromat the abbreviation text. We will then also remove any other unicode characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = df_booklet[df_booklet['text'].str.lstrip().str.startswith(\"AAR\")].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx in abbreviations:\n",
    "    abrv_dict = reformat_abbreviations(df_booklet.iloc[indx]['text'])\n",
    "    new_abrv_text = [(key +\" : \"+ value) for key, value in  abrv_dict.items()]\n",
    "    abrv_df = pd.DataFrame({\"text\": new_abrv_text,\n",
    "                            \"cleanText\": new_abrv_text})\n",
    "    abrv_df['index'] = df_booklet.iloc[indx]['index']\n",
    "    abrv_df['book'] = df_booklet.iloc[indx]['book']\n",
    "    df_booklet = pd.concat([df_booklet, abrv_df[df_booklet.drop(\"level_0\", axis=1).columns]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove abbreviation indexes\n",
    "df_booklet = df_booklet.drop(abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all empty strings\n",
    "df_booklet = df_booklet[df_booklet['cleanText'] != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet = df_booklet.drop(['level_0', 'text'], axis=1)\n",
    "df_booklet.columns = ['paragraph',  'book', 'cleanText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed and Create Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(dataset: pd.DataFrame, chunk_size: int=500, chunk_overlap: int=10) -> list:\n",
    "    \"\"\"\n",
    "    Create chunks from the dataset\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): Dataset\n",
    "        chunk_size (int): Chunk size\n",
    "        chunk_overlap (int): Chunk overlap\n",
    "\n",
    "    Returns:\n",
    "        list: List of chunks\n",
    "    \"\"\"\n",
    "    text_chunks = DataFrameLoader(\n",
    "        dataset, page_content_column=\"cleanText\"\n",
    "    ).load_and_split(\n",
    "        text_splitter=RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "\n",
    "def create_or_get_vector_store(chunks: list, recreate_embeddings=False) -> FAISS:\n",
    "    \"\"\"\n",
    "    Create or get vector store\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of chunks\n",
    "\n",
    "    Returns:\n",
    "        FAISS: Vector store\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "    )\n",
    "    if not os.path.exists(\"./db\") or recreate_embeddings:\n",
    "        print(\"CREATING DB\")\n",
    "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "        vectorstore.save_local(\"./db\")\n",
    "    else:\n",
    "        # print(\"LOADING DB\")\n",
    "        vectorstore = FAISS.load_local(\"./db\", embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def embed_booklets(df, recreate_embeddings=False):\n",
    "    chunks = create_chunks(df, 2000, 0)\n",
    "    vector_store = create_or_get_vector_store(chunks, recreate_embeddings=recreate_embeddings)\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet['cleanText'] = [x+\" \" for x in df_booklet['cleanText']]\n",
    "df_booklet.sort_values(by=['book', 'paragraph'], inplace=True)\n",
    "docs = \"\".join(df_booklet['cleanText'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet['len'] = [len(text) for text in df_booklet['cleanText'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get new splits \n",
    "sum = 0\n",
    "counter = 0\n",
    "splits = []\n",
    "\n",
    "for x in df_booklet['len']:\n",
    "    sum = sum + x\n",
    "    if sum == 500:\n",
    "        splits.append(counter)\n",
    "        counter = counter + 1\n",
    "        sum = 0\n",
    "    elif sum > 500:\n",
    "        counter = counter + 1\n",
    "        splits.append(counter)\n",
    "        sum = sum - 500\n",
    "    else:\n",
    "        splits.append(counter)\n",
    "\n",
    "\n",
    "df_booklet['group'] = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = df_booklet.groupby('group')['book'].agg(lambda x: list(set(x)))\n",
    "paragraphs = df_booklet.groupby('group')['paragraph'].agg(lambda x: sorted(list(set(x))))\n",
    "groups = list(set(df_booklet.group.values))\n",
    "text = df_booklet.groupby('group')['cleanText'].agg(lambda x: \"\".join(list(x)))\n",
    "\n",
    "df_booklet_new = pd.DataFrame({\"books\": books, \"paragraphs\": paragraphs, \"group\": groups, \"cleanText\":text}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets reformat the paragraphs. If they all appear in the same book we will only report the min and max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet_new['paragraphs'] = df_booklet_new.apply(lambda row: \",\".join([str(x) for x in row['paragraphs']]) if len(row['books']) > 1 else\n",
    "                                                                str(row['paragraphs'][0]) if (len(row['books']) == 1) and (len(row['paragraphs']) == 1)\n",
    "                                                                else \"-\".join([str(row[\"paragraphs\"][0]), str(row[\"paragraphs\"][-1])]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet_new['books'] = df_booklet_new.apply(lambda row: \",\".join(list(set(row['books']))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING DB\n"
     ]
    }
   ],
   "source": [
    "vector_store = embed_booklets(df_booklet_new,recreate_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem like the following steps will have to be taken:\n",
    "\n",
    "- embed booklet\n",
    "- embed search phrase\n",
    "- use search phrase embedding to search for relevant text in booklet\n",
    "- retrive all relevant text from booklet\n",
    "- format search phrase and into prompt for LLM\n",
    "- Send promt to LLM and return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "To use ollama install follow these instructions:\n",
    "\n",
    "- Download and install Ollama onto the available supported platforms (including Windows Subsystem for Linux) (https://ollama.com/)\n",
    "- Fetch available LLM model via ollama pull llama2\n",
    "\n",
    "This will download the default tagged version of the model. Typically, the default points to the latest, smallest sized-parameter model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = Ollama(model=\"phi\", gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(pwd +  \"/data/data/Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_letter(s):\n",
    "    m = re.search(r'[a-z]', s, re.I)\n",
    "    if m is not None:\n",
    "        return m.start()\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_keywords(keywords:str):\n",
    "    keywords_list = keywords.split(\"Keywords:\")[-1].split(',')\n",
    "    if len(keywords_list) == 1:\n",
    "        keywords_list = keywords.split(\"Keywords:\")[-1].split('\\n')\n",
    "    keywords_indexes = [first_letter(word) for word in keywords_list]\n",
    "    clean_keywords = [keywords_list[i][keywords_indexes[i]:] for i in range(len(keywords_list))]\n",
    "    parsed_keywords = \", \".join(list(set([i.lstrip().capitalize().replace('\\n', \"\") for i in clean_keywords])))\n",
    "    return parsed_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_book_info(book_info: dict):\n",
    "    if len(set(book_info['book'].split(\",\"))) > 1:\n",
    "        book_info['book'] = book_info['book'].split(\",\")[0]\n",
    "        book_info['paragraph'] = book_info['paragraph'].split(\",\")[0]\n",
    "\n",
    "    else: \n",
    "        book_info['book'] = book_info['book'].split(\",\")[0]\n",
    "    return book_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = []\n",
    "responses = []\n",
    "for index, row in df_train.head(20).iterrows():\n",
    "    question = row['Question Text']\n",
    "    docs = vector_store.similarity_search_with_score(question, k=2)\n",
    "    booklet_matches = [doc[0].page_content for doc in docs]\n",
    "    response = get_response(text=question, llm=llm_model, booklet_matches=booklet_matches, text_column=\"text\")\n",
    "    keywords = llm_model.generate(f\"Generate keywords form the following text {response['answer']}\")\n",
    "    book_docs = vector_store.similarity_search(response['answer'], k=1)\n",
    "\n",
    "    response[\"book\"] = \"TG Booklet \" + book_docs[0].metadata[\"books\"][-1]\n",
    "    response[\"paragraph\"] = book_docs[0].metadata[\"paragraphs\"]\n",
    "    response['keywords'] = format_keywords(keywords)\n",
    "    responses.append(response)\n",
    "\n",
    "    scores = scorer.score(response['answer'], row['Question Answer'])\n",
    "    rouge_scores.append(scores['rouge1'][-1])\n",
    "    scores = scorer.score(response['keywords'], row['Keywords'])\n",
    "    rouge_scores.append(scores['rouge1'][-1])\n",
    "    scores = scorer.score(response['book'], row['Reference Document'])\n",
    "    rouge_scores.append(scores['rouge1'][-1])\n",
    "    scores = scorer.score(response['paragraph'], row['Paragraph(s) Number'])\n",
    "    rouge_scores.append(scores['rouge1'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4724537893321026"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(pwd +  \"/data/data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(columns=['answer', 'book', 'paragraphs', 'keywords', 'ID', 'Question'])\n",
    "df_submission.to_csv(pwd + \"/data/submissions/submission_v8_temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "responses = []\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    id = row['ID']\n",
    "    question = row['Question Text']\n",
    "    docs = vector_store.similarity_search_with_score(question, k=3)\n",
    "    booklet_matches = [doc[0].page_content for doc in docs]\n",
    "    response = get_response(text=question, llm=llm_model, booklet_matches=booklet_matches, text_column=\"text\")\n",
    "    keywords = llm_model.generate(f\"Generate keywords form the following text {response['answer']}\")\n",
    "    book_docs = vector_store.similarity_search(response['answer'], k=1)\n",
    "\n",
    "    response[\"book\"] = \"TG Booklet \" + book_docs[0].metadata[\"books\"][-1]\n",
    "    response[\"paragraph\"] = book_docs[0].metadata[\"paragraphs\"]\n",
    "    response['keywords'] = format_keywords(keywords)\n",
    "    df_responses = pd.DataFrame([response])\n",
    "    df_responses['ID'] = id\n",
    "    df_responses['Question'] = question\n",
    "    csv_file_path = pwd + \"/data/submissions/submission_v8_temp.csv\"\n",
    "    df_responses[['answer', 'book', 'paragraph', 'keywords', 'ID', 'Question']].to_csv(csv_file_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses = pd.read_csv(pwd +  \"/data/submissions/submission_v8_temp.csv\")\n",
    "df_responses.columns = ['answer', 'book', 'paragraphs', 'keywords', 'ID', 'Question', 'None']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submissoion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.drop(['None'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.columns = ['question_answer', 'reference_document', 'paragraph(s)_number', 'keywords', 'ID', 'Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.melt(df_responses, id_vars=['ID'], value_vars=['question_answer', 'reference_document', 'paragraph(s)_number', \"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission['ID'] = df_submission['ID'] + '_' + df_submission['variable']\n",
    "df_submission.columns = [\"ID\", \"variable\", \"Target\"]\n",
    "df_submission = df_submission[['ID', \"Target\"]].set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(pwd + \"/data/submissions/submission_v8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.1 ('zindi_llm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1b94373ed21143aa54ae29a501b4c41cca272fcc00b21ffb9f53282b803de8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
