{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import torch\n",
    "from utils.embeddings import Embedder\n",
    "from utils.preprocess import create_sentance_booklet, create_faise_index\n",
    "import faiss\n",
    "from utils.utils import search_content, read_booklets, retrieve_booklet_text\n",
    "from models.llama import llama_cpp\n",
    "from utils.response_generator import get_response, extract_keyword\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silly Mac that forces me to change the environmental variable to prevent issues running transformers\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd  = str(pathlib.Path().cwd().parent.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet = read_booklets((pwd + \"/data/data/booklets/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to create embeddings of the text, it is important to know how long text is since it can influence the tokenization for some models (can end up truncating text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Character lenght is:  18747\n"
     ]
    }
   ],
   "source": [
    "df_booklet['textLength'] = [len(text) for text in df_booklet['text']]\n",
    "print(\"Maximum Character lenght is: \", df_booklet['textLength'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Clean some of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove newline characters\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    \n",
    "    # Remove non-alphabetic characters and extra spaces\n",
    "    text = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "\n",
    "    # Remove tabs\n",
    "    text = re.sub(r'\\\\t', '', text)\n",
    "    \n",
    "    # Remove specific substring 'BOOKLET ONE'\n",
    "    text = text.replace('BOOKLET ONE', '')\n",
    "    \n",
    "    # Check if the cleaned text is empty or contains only spaces\n",
    "    if text.isupper():\n",
    "        text = ''  # Return None for rows to be removed\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet['cleanText'] = df_booklet['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>book</th>\n",
       "      <th>textLength</th>\n",
       "      <th>cleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>BOOKLET SIX: SECTION 11</td>\n",
       "      <td>booklet6</td>\n",
       "      <td>23</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>TECHNICAL GUIDELINES FOR INTEGRATED DISEASE SU...</td>\n",
       "      <td>booklet6</td>\n",
       "      <td>92</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>BOOKLET SIX: SECTION 11</td>\n",
       "      <td>booklet6</td>\n",
       "      <td>23</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>This booklet comprises the following sections ...</td>\n",
       "      <td>booklet6</td>\n",
       "      <td>193</td>\n",
       "      <td>This booklet comprises the following sections ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>Table of Contents</td>\n",
       "      <td>booklet6</td>\n",
       "      <td>17</td>\n",
       "      <td>Table of Contents</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                               text      book  \\\n",
       "1       3                            BOOKLET SIX: SECTION 11  booklet6   \n",
       "5       7  TECHNICAL GUIDELINES FOR INTEGRATED DISEASE SU...  booklet6   \n",
       "7       9                            BOOKLET SIX: SECTION 11  booklet6   \n",
       "10     12  This booklet comprises the following sections ...  booklet6   \n",
       "11     13                                  Table of Contents  booklet6   \n",
       "\n",
       "    textLength                                          cleanText  \n",
       "1           23                                                     \n",
       "5           92                                                     \n",
       "7           23                                                     \n",
       "10         193  This booklet comprises the following sections ...  \n",
       "11          17                                  Table of Contents  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_booklet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty rows from dataframe\n",
    "df_booklet = df_booklet[df_booklet['cleanText'].str.strip() != '']\n",
    "\n",
    "# Remove rows where cleanText length is less than 15 chars\n",
    "df_booklet = df_booklet[(df_booklet['cleanText'].str.len() >= 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>book</th>\n",
       "      <th>textLength</th>\n",
       "      <th>cleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>This booklet comprises the following sections ...</td>\n",
       "      <td>booklet6</td>\n",
       "      <td>193</td>\n",
       "      <td>This booklet comprises the following sections ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>Table of Contents</td>\n",
       "      <td>booklet6</td>\n",
       "      <td>17</td>\n",
       "      <td>Table of Contents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>ACKNOWLEDGMENTS\\tvii</td>\n",
       "      <td>booklet6</td>\n",
       "      <td>19</td>\n",
       "      <td>ACKNOWLEDGMENTSvii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>LIST OF ABBREVIATIONS\\tix</td>\n",
       "      <td>booklet6</td>\n",
       "      <td>24</td>\n",
       "      <td>LIST OF ABBREVIATIONSix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>Acute haemorrhagic fever syndrome\\t4</td>\n",
       "      <td>booklet6</td>\n",
       "      <td>35</td>\n",
       "      <td>Acute haemorrhagic fever syndrome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                               text      book  \\\n",
       "10     12  This booklet comprises the following sections ...  booklet6   \n",
       "11     13                                  Table of Contents  booklet6   \n",
       "13     15                               ACKNOWLEDGMENTS\\tvii  booklet6   \n",
       "14     16                          LIST OF ABBREVIATIONS\\tix  booklet6   \n",
       "16     18               Acute haemorrhagic fever syndrome\\t4  booklet6   \n",
       "\n",
       "    textLength                                          cleanText  \n",
       "10         193  This booklet comprises the following sections ...  \n",
       "11          17                                  Table of Contents  \n",
       "13          19                                 ACKNOWLEDGMENTSvii  \n",
       "14          24                            LIST OF ABBREVIATIONSix  \n",
       "16          35                  Acute haemorrhagic fever syndrome  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_booklet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet.to_csv(pwd + \"/data/data/resources/booklet_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There paragraphs are long, so we might need to consider spliting text on sentences to make them shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem like the following steps will have to be taken:\n",
    "\n",
    "- embed booklet\n",
    "- embed search phrase\n",
    "- use search phrase embedding to search for relevant text in booklet\n",
    "- retrive all relevant text from booklet\n",
    "- format search phrase and into prompt for LLM\n",
    "- Send promt to LLM and return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try simple model\n",
    "I will first use all-mpnet-base-v2  as the sentance embedder and then I will use Llama as the LLM .\n",
    "\n",
    "- Download: `wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_1.bin`\n",
    "- Then run: pip install llama-cpp-python==0.1.78\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Embed all sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SentenceTransformer.__init__() got an unexpected keyword argument 'normalize_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m----> 2\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: SentenceTransformer.__init__() got an unexpected keyword argument 'normalize_embeddings'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72abe1135e2840f3be418fc0f7a016d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_file = pwd + \"/data/data/resources/embeddings_clean.npy\"\n",
    "if not os.path.isfile(embeddings_file):\n",
    "    booklet_embeddings = embedding_model.encode(df_booklet['cleanText'].values, show_progress_bar=True)\n",
    "    np.save(file=(pwd + \"/data/data/resources/embeddings_clean\" ), arr=booklet_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = Embedder(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not create embeddings uncomment below\n",
    "# embeddings_file = pwd + \"/data/data/resources/embeddings_clean.npy\"\n",
    "# if not os.path.isfile(embeddings_file):\n",
    "#     booklet_embeddings = embedding_model.embed(df_booklet['cleanText'].values)\n",
    "#     np.save(file=(pwd + \"/data/data/resources/embeddings_clean\" ), arr=booklet_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creat faiss index for search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not create the index uncomment below\n",
    "index_file = pwd + \"/data/data/resources/paragraph_index_clean.faiss\"\n",
    "if not os.path.isfile(index_file):\n",
    "    fastIndex = create_faise_index(booklet_embeddings)\n",
    "    faiss.write_index(fastIndex, pwd + \"/data/data/resources/paragraph_index_clean.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in index\n",
    "fastIndex = faiss.read_index( pwd + \"/data/data/resources/paragraph_index_clean.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the man eating?\"\n",
    "query_embedding = embedding_model.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_content(query, df_sentances, index, embedder, k=5):\n",
    "    \"\"\"Function used to to returns relevant text based on query\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    query: str\n",
    "            query text\n",
    "\n",
    "    df_sentances: pandas dataframe\n",
    "                  data frame with text columns that match index\n",
    "\n",
    "    index: faiss index\n",
    "           index of text embeddings\n",
    "    \n",
    "    k: int\n",
    "       top number of matches to return\n",
    "\n",
    "    embedder: Embedder\n",
    "              embedding model class\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas dataframe: dataframe with text from top matches\n",
    "    \"\"\"\n",
    "    \n",
    "#     query_vector = embedder.embed(query)\n",
    "    query_vector = query\n",
    "    query_vector = np.expand_dims(query_vector, axis=0)\n",
    "\n",
    "    # We set k to limit the number of vectors we want to return\n",
    "    matched_em, matched_indexes = index.search(query_vector, k)\n",
    "    ids = matched_indexes[0][0:k]\n",
    "\n",
    "    df = df_sentances.iloc[ids.tolist() ]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>book</th>\n",
       "      <th>textLength</th>\n",
       "      <th>cleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>404</td>\n",
       "      <td>Food</td>\n",
       "      <td>booklet1</td>\n",
       "      <td>4</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  text      book  textLength cleanText\n",
       "402    404  Food  booklet1           4      Food"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_content(query=query_embedding, embedder=embedding_model, df_sentances=df_booklet, index=fastIndex, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Search embeddings and get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/brendentaylor/Documents/zindi_llm/llama-2-7b-chat.ggmlv3.q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 3 (mostly Q4_1)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4017.35 MB (+ 1024.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =  153.35 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm_model = llama_cpp(pwd + \"/llama-2-7b-chat.ggmlv3.q4_1.bin\", gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment if you want to test on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(pwd +  \"/data/data/Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.51it/s]\n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rouge_scores = []\n",
    "for question in tqdm(df_train.head(1)['Question Text'].values.tolist()):\n",
    "    print(\"start\")\n",
    "    df_search_results = search_content(query=question, df_sentances=df_booklet, index=fastIndex, embedder=embedding_model, k=5)\n",
    "    response = get_response(text=question, llm=llm_model, df_matches=df_search_results)\n",
    "    response[\"keywords\"] = extract_keyword(str(df_search_results['text'].values), top_n=6)\n",
    "    scores = scorer.score(response['answer'], question)\n",
    "    rouge_scores.append(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(pwd +  \"/data/data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'￼'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_booklet.iloc[2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Question Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Q1125</td>\n",
       "      <td>What constitutes a complex emergency?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                          Question Text\n",
       "451  Q1125  What constitutes a complex emergency?"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['ID']=='Q1125']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.58 ms /   128 runs   (    0.87 ms per token,  1147.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47539.18 ms /   329 tokens (  144.50 ms per token,     6.92 tokens per second)\n",
      "llama_print_timings:        eval time = 101783.86 ms /   127 runs   (  801.45 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 149723.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.34 ms /   128 runs   (    0.89 ms per token,  1119.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 108047.17 ms /   619 tokens (  174.55 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:        eval time = 116352.68 ms /   127 runs   (  916.16 ms per token,     1.09 tokens per second)\n",
      "llama_print_timings:       total time = 224845.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.56 ms /   128 runs   (    0.87 ms per token,  1147.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29890.75 ms /   233 tokens (  128.29 ms per token,     7.80 tokens per second)\n",
      "llama_print_timings:        eval time = 97535.74 ms /   127 runs   (  768.00 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 127831.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.39 ms /   128 runs   (    0.89 ms per token,  1128.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23519.39 ms /   191 tokens (  123.14 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:        eval time = 115247.07 ms /   127 runs   (  907.46 ms per token,     1.10 tokens per second)\n",
      "llama_print_timings:       total time = 139218.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    37.34 ms /    43 runs   (    0.87 ms per token,  1151.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28496.50 ms /   229 tokens (  124.44 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:        eval time = 31952.84 ms /    42 runs   (  760.78 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 60585.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    86.75 ms /   100 runs   (    0.87 ms per token,  1152.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43783.98 ms /   323 tokens (  135.55 ms per token,     7.38 tokens per second)\n",
      "llama_print_timings:        eval time = 76056.61 ms /    99 runs   (  768.25 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 120163.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.18 ms /   128 runs   (    0.88 ms per token,  1141.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 103958.82 ms /   620 tokens (  167.68 ms per token,     5.96 tokens per second)\n",
      "llama_print_timings:        eval time = 100227.85 ms /   127 runs   (  789.20 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 204601.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.77 ms /   128 runs   (    0.88 ms per token,  1135.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 100881.14 ms /   629 tokens (  160.38 ms per token,     6.24 tokens per second)\n",
      "llama_print_timings:        eval time = 98733.41 ms /   127 runs   (  777.43 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 200031.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    63.90 ms /    74 runs   (    0.86 ms per token,  1157.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35381.62 ms /   276 tokens (  128.19 ms per token,     7.80 tokens per second)\n",
      "llama_print_timings:        eval time = 55703.60 ms /    73 runs   (  763.06 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 91318.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.68 ms /   128 runs   (    0.86 ms per token,  1156.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35763.57 ms /   279 tokens (  128.18 ms per token,     7.80 tokens per second)\n",
      "llama_print_timings:        eval time = 96798.22 ms /   127 runs   (  762.19 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 132971.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.81 ms /   128 runs   (    0.87 ms per token,  1155.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15488.20 ms /   133 tokens (  116.45 ms per token,     8.59 tokens per second)\n",
      "llama_print_timings:        eval time = 96280.33 ms /   127 runs   (  758.11 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 112175.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.81 ms /   128 runs   (    0.87 ms per token,  1155.13 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20691.11 ms /   177 tokens (  116.90 ms per token,     8.55 tokens per second)\n",
      "llama_print_timings:        eval time = 96204.69 ms /   127 runs   (  757.52 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 117304.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.38 ms /   128 runs   (    0.86 ms per token,  1159.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21411.35 ms /   181 tokens (  118.29 ms per token,     8.45 tokens per second)\n",
      "llama_print_timings:        eval time = 96901.69 ms /   127 runs   (  763.01 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 118722.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    19.12 ms /    22 runs   (    0.87 ms per token,  1150.33 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15406.96 ms /   134 tokens (  114.98 ms per token,     8.70 tokens per second)\n",
      "llama_print_timings:        eval time = 16035.95 ms /    21 runs   (  763.62 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 31513.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    55.97 ms /    64 runs   (    0.87 ms per token,  1143.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10592.97 ms /    88 tokens (  120.37 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:        eval time = 47215.13 ms /    63 runs   (  749.45 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 58009.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    64.74 ms /    75 runs   (    0.86 ms per token,  1158.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15175.00 ms /   132 tokens (  114.96 ms per token,     8.70 tokens per second)\n",
      "llama_print_timings:        eval time = 55878.37 ms /    74 runs   (  755.11 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 71291.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.05 ms /   128 runs   (    0.87 ms per token,  1152.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50883.86 ms /   367 tokens (  138.65 ms per token,     7.21 tokens per second)\n",
      "llama_print_timings:        eval time = 98987.34 ms /   127 runs   (  779.43 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 150287.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.88 ms /   128 runs   (    0.87 ms per token,  1144.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 93015.21 ms /   528 tokens (  176.17 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:        eval time = 97685.16 ms /   127 runs   (  769.17 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 191116.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.90 ms /   128 runs   (    0.87 ms per token,  1154.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28799.82 ms /   230 tokens (  125.22 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:        eval time = 97346.79 ms /   127 runs   (  766.51 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 126561.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.27 ms /   128 runs   (    0.87 ms per token,  1150.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53858.98 ms /   383 tokens (  140.62 ms per token,     7.11 tokens per second)\n",
      "llama_print_timings:        eval time = 97409.75 ms /   127 runs   (  767.01 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 151680.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.83 ms /   128 runs   (    0.87 ms per token,  1154.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14353.08 ms /   123 tokens (  116.69 ms per token,     8.57 tokens per second)\n",
      "llama_print_timings:        eval time = 95775.62 ms /   127 runs   (  754.14 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 110536.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    53.75 ms /    62 runs   (    0.87 ms per token,  1153.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47396.09 ms /   352 tokens (  134.65 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:        eval time = 46567.42 ms /    61 runs   (  763.40 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 94161.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.29 ms /   128 runs   (    0.88 ms per token,  1139.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43374.51 ms /   323 tokens (  134.29 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:        eval time = 97248.37 ms /   127 runs   (  765.74 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 141034.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.52 ms /   128 runs   (    0.88 ms per token,  1137.61 tokens per second)\n",
      "llama_print_timings: prompt eval time = 92652.87 ms /   584 tokens (  158.65 ms per token,     6.30 tokens per second)\n",
      "llama_print_timings:        eval time = 98353.16 ms /   127 runs   (  774.43 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 191431.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.65 ms /   128 runs   (    0.87 ms per token,  1146.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45396.60 ms /   338 tokens (  134.31 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:        eval time = 97810.87 ms /   127 runs   (  770.16 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 143623.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    57.83 ms /    67 runs   (    0.86 ms per token,  1158.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20194.71 ms /   172 tokens (  117.41 ms per token,     8.52 tokens per second)\n",
      "llama_print_timings:        eval time = 50385.24 ms /    66 runs   (  763.41 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 70791.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.60 ms /   128 runs   (    0.88 ms per token,  1136.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23864.25 ms /   198 tokens (  120.53 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:        eval time = 97019.24 ms /   127 runs   (  763.93 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 121294.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.71 ms /   128 runs   (    0.86 ms per token,  1156.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 37623.39 ms /   291 tokens (  129.29 ms per token,     7.73 tokens per second)\n",
      "llama_print_timings:        eval time = 96662.32 ms /   127 runs   (  761.12 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 134694.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.57 ms /   128 runs   (    0.86 ms per token,  1157.65 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19839.28 ms /   169 tokens (  117.39 ms per token,     8.52 tokens per second)\n",
      "llama_print_timings:        eval time = 95795.17 ms /   127 runs   (  754.29 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 116042.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   104.61 ms /   121 runs   (    0.86 ms per token,  1156.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 36246.39 ms /   283 tokens (  128.08 ms per token,     7.81 tokens per second)\n",
      "llama_print_timings:        eval time = 91322.63 ms /   120 runs   (  761.02 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 127953.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.75 ms /   128 runs   (    0.87 ms per token,  1145.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 39350.18 ms /   297 tokens (  132.49 ms per token,     7.55 tokens per second)\n",
      "llama_print_timings:        eval time = 100746.10 ms /   127 runs   (  793.28 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 140527.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.53 ms /   128 runs   (    0.86 ms per token,  1158.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46415.64 ms /   343 tokens (  135.32 ms per token,     7.39 tokens per second)\n",
      "llama_print_timings:        eval time = 97294.59 ms /   127 runs   (  766.10 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 144118.68 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.91 ms /   128 runs   (    0.87 ms per token,  1154.08 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17795.01 ms /    24 tokens (  741.46 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 95705.43 ms /   127 runs   (  753.59 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 113914.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    89.16 ms /   103 runs   (    0.87 ms per token,  1155.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22043.21 ms /   185 tokens (  119.15 ms per token,     8.39 tokens per second)\n",
      "llama_print_timings:        eval time = 77510.01 ms /   102 runs   (  759.90 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 99881.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.06 ms /   128 runs   (    0.87 ms per token,  1152.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 60582.38 ms /   421 tokens (  143.90 ms per token,     6.95 tokens per second)\n",
      "llama_print_timings:        eval time = 98812.46 ms /   127 runs   (  778.05 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 159810.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.50 ms /   128 runs   (    0.86 ms per token,  1158.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30192.81 ms /   244 tokens (  123.74 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:        eval time = 96652.97 ms /   127 runs   (  761.05 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 127253.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.45 ms /   128 runs   (    0.86 ms per token,  1158.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44610.43 ms /   333 tokens (  133.97 ms per token,     7.46 tokens per second)\n",
      "llama_print_timings:        eval time = 97301.81 ms /   127 runs   (  766.16 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 142321.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.61 ms /   128 runs   (    0.86 ms per token,  1157.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26601.77 ms /   218 tokens (  122.03 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:        eval time = 96948.49 ms /   127 runs   (  763.37 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 123960.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    84.52 ms /    98 runs   (    0.86 ms per token,  1159.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27236.26 ms /   220 tokens (  123.80 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:        eval time = 73325.94 ms /    97 runs   (  755.94 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 100873.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.68 ms /   128 runs   (    0.86 ms per token,  1156.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20914.09 ms /   177 tokens (  118.16 ms per token,     8.46 tokens per second)\n",
      "llama_print_timings:        eval time = 95975.63 ms /   127 runs   (  755.71 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 117298.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.22 ms /   128 runs   (    0.86 ms per token,  1161.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26299.93 ms /   216 tokens (  121.76 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:        eval time = 95705.17 ms /   127 runs   (  753.58 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 122412.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.09 ms /   128 runs   (    0.87 ms per token,  1152.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 60256.13 ms /   419 tokens (  143.81 ms per token,     6.95 tokens per second)\n",
      "llama_print_timings:        eval time = 97325.94 ms /   127 runs   (  766.35 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 157995.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   100.43 ms /   116 runs   (    0.87 ms per token,  1155.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18855.38 ms /   162 tokens (  116.39 ms per token,     8.59 tokens per second)\n",
      "llama_print_timings:        eval time = 87677.49 ms /   115 runs   (  762.41 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 106904.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.52 ms /   128 runs   (    0.86 ms per token,  1158.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18621.29 ms /   156 tokens (  119.37 ms per token,     8.38 tokens per second)\n",
      "llama_print_timings:        eval time = 95817.36 ms /   127 runs   (  754.47 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 114847.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.61 ms /   128 runs   (    0.87 ms per token,  1146.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 41717.38 ms /   311 tokens (  134.14 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:        eval time = 101860.39 ms /   127 runs   (  802.05 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 144005.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.93 ms /   128 runs   (    0.87 ms per token,  1153.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32043.57 ms /   255 tokens (  125.66 ms per token,     7.96 tokens per second)\n",
      "llama_print_timings:        eval time = 96867.68 ms /   127 runs   (  762.74 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 129329.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.19 ms /   128 runs   (    0.88 ms per token,  1140.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 78323.63 ms /   484 tokens (  161.83 ms per token,     6.18 tokens per second)\n",
      "llama_print_timings:        eval time = 108861.20 ms /   127 runs   (  857.17 ms per token,     1.17 tokens per second)\n",
      "llama_print_timings:       total time = 187618.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.77 ms /   128 runs   (    0.87 ms per token,  1155.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40725.15 ms /   307 tokens (  132.66 ms per token,     7.54 tokens per second)\n",
      "llama_print_timings:        eval time = 98102.98 ms /   127 runs   (  772.46 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 139243.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.00 ms /   128 runs   (    0.87 ms per token,  1153.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49204.89 ms /   359 tokens (  137.06 ms per token,     7.30 tokens per second)\n",
      "llama_print_timings:        eval time = 97538.85 ms /   127 runs   (  768.02 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 147157.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    94.82 ms /   107 runs   (    0.89 ms per token,  1128.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 119372.62 ms /   720 tokens (  165.80 ms per token,     6.03 tokens per second)\n",
      "llama_print_timings:        eval time = 82732.39 ms /   106 runs   (  780.49 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 202458.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.79 ms /   128 runs   (    0.87 ms per token,  1155.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15131.35 ms /   127 tokens (  119.14 ms per token,     8.39 tokens per second)\n",
      "llama_print_timings:        eval time = 96505.75 ms /   127 runs   (  759.89 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 112049.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.15 ms /   128 runs   (    0.87 ms per token,  1151.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18265.81 ms /   154 tokens (  118.61 ms per token,     8.43 tokens per second)\n",
      "llama_print_timings:        eval time = 96208.46 ms /   127 runs   (  757.55 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 114887.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.96 ms /   128 runs   (    0.87 ms per token,  1153.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29271.58 ms /   236 tokens (  124.03 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:        eval time = 97566.93 ms /   127 runs   (  768.24 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 127253.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.70 ms /   128 runs   (    0.86 ms per token,  1156.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19993.15 ms /   169 tokens (  118.30 ms per token,     8.45 tokens per second)\n",
      "llama_print_timings:        eval time = 96802.87 ms /   127 runs   (  762.23 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 117208.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.10 ms /   128 runs   (    0.87 ms per token,  1152.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13197.13 ms /   111 tokens (  118.89 ms per token,     8.41 tokens per second)\n",
      "llama_print_timings:        eval time = 95997.30 ms /   127 runs   (  755.88 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 109607.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.98 ms /   128 runs   (    0.87 ms per token,  1153.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18554.45 ms /   160 tokens (  115.97 ms per token,     8.62 tokens per second)\n",
      "llama_print_timings:        eval time = 96722.79 ms /   127 runs   (  761.60 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 115691.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.90 ms /   128 runs   (    0.87 ms per token,  1143.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45973.92 ms /   341 tokens (  134.82 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:        eval time = 97238.95 ms /   127 runs   (  765.66 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 143631.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.49 ms /   128 runs   (    0.87 ms per token,  1148.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16846.12 ms /   141 tokens (  119.48 ms per token,     8.37 tokens per second)\n",
      "llama_print_timings:        eval time = 100141.05 ms /   127 runs   (  788.51 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 117414.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.29 ms /   128 runs   (    0.87 ms per token,  1150.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21200.67 ms /   172 tokens (  123.26 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:        eval time = 97473.46 ms /   127 runs   (  767.51 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 119135.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     2.63 ms /     3 runs   (    0.88 ms per token,  1139.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14436.00 ms /   125 tokens (  115.49 ms per token,     8.66 tokens per second)\n",
      "llama_print_timings:        eval time =  1776.70 ms /     2 runs   (  888.35 ms per token,     1.13 tokens per second)\n",
      "llama_print_timings:       total time = 16222.64 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.25 ms /   128 runs   (    0.87 ms per token,  1150.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6154.43 ms /    44 tokens (  139.87 ms per token,     7.15 tokens per second)\n",
      "llama_print_timings:        eval time = 95880.79 ms /   127 runs   (  754.97 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 102446.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.88 ms /     1 runs   (    0.88 ms per token,  1138.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 404630.72 ms /  1677 tokens (  241.28 ms per token,     4.14 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 404651.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     2.63 ms /     3 runs   (    0.88 ms per token,  1141.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 401441.63 ms /  1624 tokens (  247.19 ms per token,     4.05 tokens per second)\n",
      "llama_print_timings:        eval time =  3688.80 ms /     2 runs   ( 1844.40 ms per token,     0.54 tokens per second)\n",
      "llama_print_timings:       total time = 405152.17 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.27 ms /   128 runs   (    0.90 ms per token,  1110.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11602.24 ms /    15 tokens (  773.48 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:        eval time = 104053.29 ms /   127 runs   (  819.32 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 116094.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    56.12 ms /    64 runs   (    0.88 ms per token,  1140.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32258.77 ms /   238 tokens (  135.54 ms per token,     7.38 tokens per second)\n",
      "llama_print_timings:        eval time = 50023.87 ms /    63 runs   (  794.03 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 82490.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.80 ms /   128 runs   (    0.89 ms per token,  1124.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 37290.64 ms /   273 tokens (  136.60 ms per token,     7.32 tokens per second)\n",
      "llama_print_timings:        eval time = 103060.51 ms /   127 runs   (  811.50 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 140798.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.38 ms /   128 runs   (    0.87 ms per token,  1149.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30362.92 ms /   232 tokens (  130.87 ms per token,     7.64 tokens per second)\n",
      "llama_print_timings:        eval time = 101141.41 ms /   127 runs   (  796.39 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 131929.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    50.80 ms /    58 runs   (    0.88 ms per token,  1141.62 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29728.68 ms /   233 tokens (  127.59 ms per token,     7.84 tokens per second)\n",
      "llama_print_timings:        eval time = 46494.62 ms /    57 runs   (  815.70 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 76416.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.76 ms /   128 runs   (    0.89 ms per token,  1125.21 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31250.51 ms /   246 tokens (  127.03 ms per token,     7.87 tokens per second)\n",
      "llama_print_timings:        eval time = 106995.66 ms /   127 runs   (  842.49 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time = 138699.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.61 ms /   128 runs   (    0.90 ms per token,  1116.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31096.44 ms /   232 tokens (  134.04 ms per token,     7.46 tokens per second)\n",
      "llama_print_timings:        eval time = 115384.80 ms /   127 runs   (  908.54 ms per token,     1.10 tokens per second)\n",
      "llama_print_timings:       total time = 146949.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.24 ms /   128 runs   (    0.88 ms per token,  1130.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 99441.06 ms /   533 tokens (  186.57 ms per token,     5.36 tokens per second)\n",
      "llama_print_timings:        eval time = 102001.53 ms /   127 runs   (  803.16 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 201876.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.76 ms /   128 runs   (    0.87 ms per token,  1155.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29728.91 ms /   235 tokens (  126.51 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time = 101766.14 ms /   127 runs   (  801.31 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 131926.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    97.07 ms /   112 runs   (    0.87 ms per token,  1153.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24888.99 ms /   198 tokens (  125.70 ms per token,     7.96 tokens per second)\n",
      "llama_print_timings:        eval time = 85948.24 ms /   111 runs   (  774.31 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 111206.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.87 ms /     1 runs   (    0.87 ms per token,  1150.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 122135.01 ms /   711 tokens (  171.78 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 122142.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.37 ms /   128 runs   (    0.89 ms per token,  1119.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25894.38 ms /   199 tokens (  130.12 ms per token,     7.69 tokens per second)\n",
      "llama_print_timings:        eval time = 100459.81 ms /   127 runs   (  791.02 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 126791.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.83 ms /   128 runs   (    0.87 ms per token,  1154.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20113.14 ms /   163 tokens (  123.39 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:        eval time = 98216.52 ms /   127 runs   (  773.36 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 118744.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.79 ms /   128 runs   (    0.87 ms per token,  1155.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 41178.25 ms /   308 tokens (  133.70 ms per token,     7.48 tokens per second)\n",
      "llama_print_timings:        eval time = 101024.28 ms /   127 runs   (  795.47 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 142631.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    79.86 ms /    92 runs   (    0.87 ms per token,  1152.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15170.40 ms /   126 tokens (  120.40 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:        eval time = 72346.54 ms /    91 runs   (  795.02 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 87823.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.95 ms /   128 runs   (    0.87 ms per token,  1153.67 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20354.52 ms /   169 tokens (  120.44 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:        eval time = 100472.42 ms /   127 runs   (  791.12 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 121248.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.04 ms /   128 runs   (    0.88 ms per token,  1132.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 185842.65 ms /   967 tokens (  192.18 ms per token,     5.20 tokens per second)\n",
      "llama_print_timings:        eval time = 105469.09 ms /   127 runs   (  830.47 ms per token,     1.20 tokens per second)\n",
      "llama_print_timings:       total time = 291750.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.62 ms /   128 runs   (    0.90 ms per token,  1116.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 326575.16 ms /  1438 tokens (  227.10 ms per token,     4.40 tokens per second)\n",
      "llama_print_timings:        eval time = 111548.97 ms /   127 runs   (  878.34 ms per token,     1.14 tokens per second)\n",
      "llama_print_timings:       total time = 438570.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.25 ms /   128 runs   (    0.87 ms per token,  1150.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21169.53 ms /   173 tokens (  122.37 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:        eval time = 99619.35 ms /   127 runs   (  784.40 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 121209.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   103.53 ms /   119 runs   (    0.87 ms per token,  1149.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29320.26 ms /   235 tokens (  124.77 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:        eval time = 92146.11 ms /   118 runs   (  780.90 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 121854.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.63 ms /   128 runs   (    0.88 ms per token,  1136.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27334.36 ms /   222 tokens (  123.13 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:        eval time = 111190.16 ms /   127 runs   (  875.51 ms per token,     1.14 tokens per second)\n",
      "llama_print_timings:       total time = 139004.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.65 ms /   128 runs   (    0.89 ms per token,  1126.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11972.77 ms /    91 tokens (  131.57 ms per token,     7.60 tokens per second)\n",
      "llama_print_timings:        eval time = 101285.79 ms /   127 runs   (  797.53 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 113705.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.47 ms /   128 runs   (    0.87 ms per token,  1148.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42120.65 ms /   314 tokens (  134.14 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:        eval time = 101914.28 ms /   127 runs   (  802.47 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 144467.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.84 ms /   128 runs   (    0.87 ms per token,  1154.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30088.31 ms /   236 tokens (  127.49 ms per token,     7.84 tokens per second)\n",
      "llama_print_timings:        eval time = 100511.65 ms /   127 runs   (  791.43 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 131024.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.57 ms /   126 runs   (    0.89 ms per token,  1129.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 95853.04 ms /   577 tokens (  166.12 ms per token,     6.02 tokens per second)\n",
      "llama_print_timings:        eval time = 99479.51 ms /   125 runs   (  795.84 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 195762.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.09 ms /   128 runs   (    0.87 ms per token,  1152.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22844.56 ms /   184 tokens (  124.16 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:        eval time = 98606.84 ms /   127 runs   (  776.43 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 121874.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    67.79 ms /    78 runs   (    0.87 ms per token,  1150.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25304.77 ms /   200 tokens (  126.52 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time = 60449.64 ms /    77 runs   (  785.06 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 86010.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.45 ms /   128 runs   (    0.87 ms per token,  1148.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 41376.23 ms /   309 tokens (  133.90 ms per token,     7.47 tokens per second)\n",
      "llama_print_timings:        eval time = 100476.97 ms /   127 runs   (  791.16 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 142282.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    85.88 ms /    99 runs   (    0.87 ms per token,  1152.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30142.82 ms /   238 tokens (  126.65 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time = 76449.40 ms /    98 runs   (  780.10 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 106924.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.16 ms /   128 runs   (    0.88 ms per token,  1141.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 33284.99 ms /   261 tokens (  127.53 ms per token,     7.84 tokens per second)\n",
      "llama_print_timings:        eval time = 112443.66 ms /   127 runs   (  885.38 ms per token,     1.13 tokens per second)\n",
      "llama_print_timings:       total time = 146180.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.14 ms /   128 runs   (    0.89 ms per token,  1121.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42688.86 ms /   309 tokens (  138.15 ms per token,     7.24 tokens per second)\n",
      "llama_print_timings:        eval time = 109586.63 ms /   127 runs   (  862.89 ms per token,     1.16 tokens per second)\n",
      "llama_print_timings:       total time = 152733.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.29 ms /   128 runs   (    0.87 ms per token,  1150.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13936.85 ms /   111 tokens (  125.56 ms per token,     7.96 tokens per second)\n",
      "llama_print_timings:        eval time = 98511.89 ms /   127 runs   (  775.68 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 112872.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.85 ms /   128 runs   (    0.87 ms per token,  1154.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18857.05 ms /   154 tokens (  122.45 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:        eval time = 99437.78 ms /   127 runs   (  782.97 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 118722.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.79 ms /   128 runs   (    0.87 ms per token,  1155.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14259.26 ms /   118 tokens (  120.84 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:        eval time = 98326.12 ms /   127 runs   (  774.22 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 113006.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.32 ms /   128 runs   (    0.87 ms per token,  1149.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 41716.99 ms /   308 tokens (  135.44 ms per token,     7.38 tokens per second)\n",
      "llama_print_timings:        eval time = 101801.70 ms /   127 runs   (  801.59 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 143952.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.73 ms /   128 runs   (    0.87 ms per token,  1145.67 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13393.26 ms /   107 tokens (  125.17 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:        eval time = 99873.82 ms /   127 runs   (  786.41 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 113704.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.18 ms /   128 runs   (    0.87 ms per token,  1151.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11983.04 ms /    98 tokens (  122.28 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:        eval time = 99068.73 ms /   127 runs   (  780.07 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 111481.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.07 ms /   128 runs   (    0.87 ms per token,  1152.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11639.50 ms /    93 tokens (  125.16 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:        eval time = 100863.36 ms /   127 runs   (  794.20 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 112930.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.23 ms /   128 runs   (    0.89 ms per token,  1120.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25307.67 ms /   203 tokens (  124.67 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:        eval time = 123227.83 ms /   127 runs   (  970.30 ms per token,     1.03 tokens per second)\n",
      "llama_print_timings:       total time = 149030.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.07 ms /   128 runs   (    0.87 ms per token,  1152.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30919.39 ms /   234 tokens (  132.13 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:        eval time = 102374.10 ms /   127 runs   (  806.10 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 133724.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.18 ms /   128 runs   (    0.87 ms per token,  1151.29 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25534.08 ms /   199 tokens (  128.31 ms per token,     7.79 tokens per second)\n",
      "llama_print_timings:        eval time = 102228.94 ms /   127 runs   (  804.95 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 128203.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.82 ms /   128 runs   (    0.87 ms per token,  1155.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31107.42 ms /   240 tokens (  129.61 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 103032.51 ms /   127 runs   (  811.28 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 134577.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.85 ms /   128 runs   (    0.87 ms per token,  1154.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18357.81 ms /   145 tokens (  126.61 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time = 102902.80 ms /   127 runs   (  810.26 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 121698.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.26 ms /   128 runs   (    0.88 ms per token,  1140.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19513.78 ms /   158 tokens (  123.50 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:        eval time = 117502.77 ms /   127 runs   (  925.22 ms per token,     1.08 tokens per second)\n",
      "llama_print_timings:       total time = 137486.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.30 ms /   128 runs   (    0.89 ms per token,  1129.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17848.71 ms /   123 tokens (  145.11 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:        eval time = 104552.70 ms /   127 runs   (  823.25 ms per token,     1.21 tokens per second)\n",
      "llama_print_timings:       total time = 122850.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.07 ms /   128 runs   (    0.87 ms per token,  1152.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34639.33 ms /   263 tokens (  131.71 ms per token,     7.59 tokens per second)\n",
      "llama_print_timings:        eval time = 100531.38 ms /   127 runs   (  791.59 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 135603.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   108.57 ms /   125 runs   (    0.87 ms per token,  1151.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20284.34 ms /   166 tokens (  122.19 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:        eval time = 97754.48 ms /   124 runs   (  788.34 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 118461.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    31.09 ms /    35 runs   (    0.89 ms per token,  1125.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74885.57 ms /   489 tokens (  153.14 ms per token,     6.53 tokens per second)\n",
      "llama_print_timings:        eval time = 31944.96 ms /    34 runs   (  939.56 ms per token,     1.06 tokens per second)\n",
      "llama_print_timings:       total time = 106957.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.01 ms /   128 runs   (    0.89 ms per token,  1122.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45086.57 ms /   309 tokens (  145.91 ms per token,     6.85 tokens per second)\n",
      "llama_print_timings:        eval time = 102995.72 ms /   127 runs   (  810.99 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 148526.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.38 ms /   128 runs   (    0.87 ms per token,  1149.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16795.95 ms /   137 tokens (  122.60 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:        eval time = 97892.11 ms /   127 runs   (  770.80 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 115111.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    71.19 ms /    82 runs   (    0.87 ms per token,  1151.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16558.60 ms /   134 tokens (  123.57 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:        eval time = 64530.34 ms /    81 runs   (  796.67 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 81365.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.93 ms /   128 runs   (    0.87 ms per token,  1153.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17195.95 ms /   140 tokens (  122.83 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:        eval time = 99109.15 ms /   127 runs   (  780.39 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 116728.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.41 ms /   128 runs   (    0.87 ms per token,  1148.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24299.90 ms /   198 tokens (  122.73 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:        eval time = 99791.61 ms /   127 runs   (  785.76 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 124534.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    62.03 ms /    70 runs   (    0.89 ms per token,  1128.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67669.86 ms /   451 tokens (  150.04 ms per token,     6.66 tokens per second)\n",
      "llama_print_timings:        eval time = 55207.81 ms /    69 runs   (  800.11 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 123115.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.41 ms /   128 runs   (    0.87 ms per token,  1148.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 39804.21 ms /   299 tokens (  133.12 ms per token,     7.51 tokens per second)\n",
      "llama_print_timings:        eval time = 98847.43 ms /   127 runs   (  778.33 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 139075.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    27.98 ms /    32 runs   (    0.87 ms per token,  1143.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52483.45 ms /   376 tokens (  139.58 ms per token,     7.16 tokens per second)\n",
      "llama_print_timings:        eval time = 24636.19 ms /    31 runs   (  794.72 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 77234.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.86 ms /   128 runs   (    0.90 ms per token,  1114.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42112.58 ms /   307 tokens (  137.17 ms per token,     7.29 tokens per second)\n",
      "llama_print_timings:        eval time = 100069.32 ms /   127 runs   (  787.95 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 142618.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.78 ms /   128 runs   (    0.87 ms per token,  1155.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44816.75 ms /   329 tokens (  136.22 ms per token,     7.34 tokens per second)\n",
      "llama_print_timings:        eval time = 99533.66 ms /   127 runs   (  783.73 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 144779.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.55 ms /   128 runs   (    0.87 ms per token,  1147.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14518.00 ms /   123 tokens (  118.03 ms per token,     8.47 tokens per second)\n",
      "llama_print_timings:        eval time = 105089.53 ms /   127 runs   (  827.48 ms per token,     1.21 tokens per second)\n",
      "llama_print_timings:       total time = 120044.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.60 ms /   128 runs   (    0.90 ms per token,  1116.89 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58915.57 ms /   391 tokens (  150.68 ms per token,     6.64 tokens per second)\n",
      "llama_print_timings:        eval time = 102944.83 ms /   127 runs   (  810.59 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 162305.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    73.50 ms /    85 runs   (    0.86 ms per token,  1156.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51343.48 ms /   369 tokens (  139.14 ms per token,     7.19 tokens per second)\n",
      "llama_print_timings:        eval time = 64670.05 ms /    84 runs   (  769.88 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 116295.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.10 ms /   128 runs   (    0.88 ms per token,  1141.80 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56120.14 ms /   392 tokens (  143.16 ms per token,     6.99 tokens per second)\n",
      "llama_print_timings:        eval time = 98982.92 ms /   127 runs   (  779.39 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 155535.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    93.00 ms /   107 runs   (    0.87 ms per token,  1150.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14776.72 ms /   123 tokens (  120.14 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:        eval time = 81806.71 ms /   106 runs   (  771.76 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 96938.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    98.79 ms /   114 runs   (    0.87 ms per token,  1153.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 37506.97 ms /   288 tokens (  130.23 ms per token,     7.68 tokens per second)\n",
      "llama_print_timings:        eval time = 87322.49 ms /   113 runs   (  772.77 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 125213.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    79.45 ms /    91 runs   (    0.87 ms per token,  1145.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12832.12 ms /   108 tokens (  118.82 ms per token,     8.42 tokens per second)\n",
      "llama_print_timings:        eval time = 68464.47 ms /    90 runs   (  760.72 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 81600.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   107.26 ms /   124 runs   (    0.86 ms per token,  1156.08 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29173.90 ms /   231 tokens (  126.29 ms per token,     7.92 tokens per second)\n",
      "llama_print_timings:        eval time = 95647.20 ms /   123 runs   (  777.62 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 125232.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.89 ms /   128 runs   (    0.87 ms per token,  1144.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48853.37 ms /   351 tokens (  139.18 ms per token,     7.18 tokens per second)\n",
      "llama_print_timings:        eval time = 99791.22 ms /   127 runs   (  785.76 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 149081.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.23 ms /   128 runs   (    0.88 ms per token,  1140.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19032.14 ms /   158 tokens (  120.46 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:        eval time = 119153.85 ms /   127 runs   (  938.22 ms per token,     1.07 tokens per second)\n",
      "llama_print_timings:       total time = 138665.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    97.96 ms /   110 runs   (    0.89 ms per token,  1122.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16411.93 ms /   123 tokens (  133.43 ms per token,     7.49 tokens per second)\n",
      "llama_print_timings:        eval time = 96936.19 ms /   109 runs   (  889.32 ms per token,     1.12 tokens per second)\n",
      "llama_print_timings:       total time = 113755.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.41 ms /   128 runs   (    0.87 ms per token,  1148.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35785.42 ms /   256 tokens (  139.79 ms per token,     7.15 tokens per second)\n",
      "llama_print_timings:        eval time = 102955.09 ms /   127 runs   (  810.67 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 139178.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.14 ms /   128 runs   (    0.87 ms per token,  1151.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23615.89 ms /   189 tokens (  124.95 ms per token,     8.00 tokens per second)\n",
      "llama_print_timings:        eval time = 104586.30 ms /   127 runs   (  823.51 ms per token,     1.21 tokens per second)\n",
      "llama_print_timings:       total time = 128647.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.96 ms /   128 runs   (    0.87 ms per token,  1153.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15767.36 ms /   121 tokens (  130.31 ms per token,     7.67 tokens per second)\n",
      "llama_print_timings:        eval time = 100573.42 ms /   127 runs   (  791.92 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 116779.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.98 ms /   128 runs   (    0.87 ms per token,  1143.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14558.85 ms /   118 tokens (  123.38 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:        eval time = 110854.51 ms /   127 runs   (  872.87 ms per token,     1.15 tokens per second)\n",
      "llama_print_timings:       total time = 125878.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    14.94 ms /    17 runs   (    0.88 ms per token,  1137.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17107.66 ms /   132 tokens (  129.60 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 14673.71 ms /    16 runs   (  917.11 ms per token,     1.09 tokens per second)\n",
      "llama_print_timings:       total time = 31844.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.54 ms /   128 runs   (    0.90 ms per token,  1107.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53881.79 ms /   364 tokens (  148.03 ms per token,     6.76 tokens per second)\n",
      "llama_print_timings:        eval time = 102059.69 ms /   127 runs   (  803.62 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 156388.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   106.72 ms /   123 runs   (    0.87 ms per token,  1152.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21858.99 ms /   171 tokens (  127.83 ms per token,     7.82 tokens per second)\n",
      "llama_print_timings:        eval time = 93755.20 ms /   122 runs   (  768.49 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 116017.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.25 ms /   128 runs   (    0.87 ms per token,  1150.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15803.81 ms /   124 tokens (  127.45 ms per token,     7.85 tokens per second)\n",
      "llama_print_timings:        eval time = 98766.92 ms /   127 runs   (  777.69 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 114996.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.88 ms /     1 runs   (    0.88 ms per token,  1133.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 394695.21 ms /  1636 tokens (  241.26 ms per token,     4.14 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 394713.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     1.38 ms /     1 runs   (    1.38 ms per token,   723.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 286944.65 ms /  1294 tokens (  221.75 ms per token,     4.51 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 286961.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    93.99 ms /   101 runs   (    0.93 ms per token,  1074.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20920.22 ms /   151 tokens (  138.54 ms per token,     7.22 tokens per second)\n",
      "llama_print_timings:        eval time = 80929.25 ms /   100 runs   (  809.29 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 102212.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.22 ms /   128 runs   (    0.89 ms per token,  1120.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 100923.01 ms /   595 tokens (  169.62 ms per token,     5.90 tokens per second)\n",
      "llama_print_timings:        eval time = 101341.10 ms /   127 runs   (  797.96 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 202715.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.91 ms /   128 runs   (    0.87 ms per token,  1154.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30965.06 ms /   245 tokens (  126.39 ms per token,     7.91 tokens per second)\n",
      "llama_print_timings:        eval time = 99056.18 ms /   127 runs   (  779.97 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 130452.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.66 ms /   128 runs   (    0.86 ms per token,  1156.65 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58359.68 ms /   403 tokens (  144.81 ms per token,     6.91 tokens per second)\n",
      "llama_print_timings:        eval time = 99505.65 ms /   127 runs   (  783.51 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 158290.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.94 ms /   128 runs   (    0.87 ms per token,  1153.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11142.40 ms /    93 tokens (  119.81 ms per token,     8.35 tokens per second)\n",
      "llama_print_timings:        eval time = 103602.63 ms /   127 runs   (  815.77 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 115182.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    41.97 ms /    48 runs   (    0.87 ms per token,  1143.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24358.22 ms /   196 tokens (  124.28 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:        eval time = 47932.98 ms /    47 runs   ( 1019.85 ms per token,     0.98 tokens per second)\n",
      "llama_print_timings:       total time = 72473.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.93 ms /   128 runs   (    0.89 ms per token,  1123.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 185644.64 ms /   958 tokens (  193.78 ms per token,     5.16 tokens per second)\n",
      "llama_print_timings:        eval time = 106465.11 ms /   127 runs   (  838.31 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time = 292565.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    41.01 ms /    47 runs   (    0.87 ms per token,  1146.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48259.14 ms /   352 tokens (  137.10 ms per token,     7.29 tokens per second)\n",
      "llama_print_timings:        eval time = 36779.60 ms /    46 runs   (  799.56 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 85198.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.94 ms /   128 runs   (    0.87 ms per token,  1153.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16142.92 ms /   134 tokens (  120.47 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:        eval time = 97843.76 ms /   127 runs   (  770.42 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 114417.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.67 ms /   128 runs   (    0.89 ms per token,  1126.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 99056.44 ms /   604 tokens (  164.00 ms per token,     6.10 tokens per second)\n",
      "llama_print_timings:        eval time = 100314.54 ms /   127 runs   (  789.88 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 199814.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.31 ms /   128 runs   (    0.87 ms per token,  1149.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40078.77 ms /   303 tokens (  132.27 ms per token,     7.56 tokens per second)\n",
      "llama_print_timings:        eval time = 99440.69 ms /   127 runs   (  783.00 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 139968.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.94 ms /   128 runs   (    0.87 ms per token,  1153.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20778.50 ms /   174 tokens (  119.42 ms per token,     8.37 tokens per second)\n",
      "llama_print_timings:        eval time = 98836.40 ms /   127 runs   (  778.24 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 120043.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.34 ms /   128 runs   (    0.87 ms per token,  1149.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30365.50 ms /   244 tokens (  124.45 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:        eval time = 99177.88 ms /   127 runs   (  780.93 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 129976.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   105.42 ms /   121 runs   (    0.87 ms per token,  1147.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15283.95 ms /   127 tokens (  120.35 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:        eval time = 94242.91 ms /   120 runs   (  785.36 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 109934.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.40 ms /   128 runs   (    0.87 ms per token,  1149.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27085.82 ms /   217 tokens (  124.82 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:        eval time = 99629.44 ms /   127 runs   (  784.48 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 127148.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.62 ms /   128 runs   (    0.86 ms per token,  1157.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21944.99 ms /   180 tokens (  121.92 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:        eval time = 97693.55 ms /   127 runs   (  769.24 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 120065.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.10 ms /   128 runs   (    0.87 ms per token,  1152.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26548.40 ms /   216 tokens (  122.91 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:        eval time = 98787.45 ms /   127 runs   (  777.85 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 125769.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.14 ms /   128 runs   (    0.88 ms per token,  1141.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35844.04 ms /   277 tokens (  129.40 ms per token,     7.73 tokens per second)\n",
      "llama_print_timings:        eval time = 109169.83 ms /   127 runs   (  859.60 ms per token,     1.16 tokens per second)\n",
      "llama_print_timings:       total time = 145457.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.48 ms /   128 runs   (    0.89 ms per token,  1118.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 77487.25 ms /   482 tokens (  160.76 ms per token,     6.22 tokens per second)\n",
      "llama_print_timings:        eval time = 106941.98 ms /   127 runs   (  842.06 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time = 184888.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.64 ms /   128 runs   (    0.86 ms per token,  1156.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24329.07 ms /   188 tokens (  129.41 ms per token,     7.73 tokens per second)\n",
      "llama_print_timings:        eval time = 97941.74 ms /   127 runs   (  771.19 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 122695.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.27 ms /   128 runs   (    0.87 ms per token,  1150.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15685.45 ms /   124 tokens (  126.50 ms per token,     7.91 tokens per second)\n",
      "llama_print_timings:        eval time = 99641.39 ms /   127 runs   (  784.58 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 115767.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.44 ms /   128 runs   (    0.88 ms per token,  1138.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34574.53 ms /   260 tokens (  132.98 ms per token,     7.52 tokens per second)\n",
      "llama_print_timings:        eval time = 102969.61 ms /   127 runs   (  810.78 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 138005.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.54 ms /   128 runs   (    0.87 ms per token,  1147.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31166.94 ms /   242 tokens (  128.79 ms per token,     7.76 tokens per second)\n",
      "llama_print_timings:        eval time = 99293.25 ms /   127 runs   (  781.84 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 130894.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.26 ms /   128 runs   (    0.89 ms per token,  1120.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 275180.46 ms /  1272 tokens (  216.34 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:        eval time = 107451.39 ms /   127 runs   (  846.07 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:       total time = 383079.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.60 ms /   128 runs   (    0.88 ms per token,  1136.80 tokens per second)\n",
      "llama_print_timings: prompt eval time = 93480.04 ms /   572 tokens (  163.43 ms per token,     6.12 tokens per second)\n",
      "llama_print_timings:        eval time = 100724.53 ms /   127 runs   (  793.11 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 194655.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.70 ms /   128 runs   (    0.86 ms per token,  1156.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27558.43 ms /   222 tokens (  124.14 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:        eval time = 99654.17 ms /   127 runs   (  784.68 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 127656.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   104.59 ms /   119 runs   (    0.88 ms per token,  1137.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64421.16 ms /   436 tokens (  147.75 ms per token,     6.77 tokens per second)\n",
      "llama_print_timings:        eval time = 93359.76 ms /   118 runs   (  791.18 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 158187.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.89 ms /   128 runs   (    0.87 ms per token,  1154.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29749.96 ms /   234 tokens (  127.14 ms per token,     7.87 tokens per second)\n",
      "llama_print_timings:        eval time = 97563.26 ms /   127 runs   (  768.21 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 127752.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.98 ms /   128 runs   (    0.87 ms per token,  1143.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34084.82 ms /   262 tokens (  130.09 ms per token,     7.69 tokens per second)\n",
      "llama_print_timings:        eval time = 104419.51 ms /   127 runs   (  822.20 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 138970.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.16 ms /   128 runs   (    0.87 ms per token,  1151.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29732.51 ms /   235 tokens (  126.52 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time = 102047.66 ms /   127 runs   (  803.52 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 132228.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.84 ms /   128 runs   (    0.87 ms per token,  1154.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35066.46 ms /   269 tokens (  130.36 ms per token,     7.67 tokens per second)\n",
      "llama_print_timings:        eval time = 97726.99 ms /   127 runs   (  769.50 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 133232.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    65.74 ms /    75 runs   (    0.88 ms per token,  1140.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21762.20 ms /   178 tokens (  122.26 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:        eval time = 57988.86 ms /    74 runs   (  783.63 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 80008.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.70 ms /   128 runs   (    0.86 ms per token,  1156.29 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23738.39 ms /   192 tokens (  123.64 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:        eval time = 98917.46 ms /   127 runs   (  778.88 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 123094.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    62.53 ms /    72 runs   (    0.87 ms per token,  1151.39 tokens per second)\n",
      "llama_print_timings: prompt eval time = 36145.99 ms /   281 tokens (  128.63 ms per token,     7.77 tokens per second)\n",
      "llama_print_timings:        eval time = 55188.14 ms /    71 runs   (  777.30 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 91579.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.89 ms /   128 runs   (    0.87 ms per token,  1154.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22620.47 ms /   184 tokens (  122.94 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:        eval time = 98280.79 ms /   127 runs   (  773.86 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 121339.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.81 ms /   128 runs   (    0.87 ms per token,  1144.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40110.07 ms /   296 tokens (  135.51 ms per token,     7.38 tokens per second)\n",
      "llama_print_timings:        eval time = 98734.94 ms /   127 runs   (  777.44 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 139300.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.94 ms /   128 runs   (    0.87 ms per token,  1153.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26214.11 ms /   209 tokens (  125.43 ms per token,     7.97 tokens per second)\n",
      "llama_print_timings:        eval time = 98824.68 ms /   127 runs   (  778.15 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 125471.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.56 ms /   128 runs   (    0.86 ms per token,  1157.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27555.90 ms /   223 tokens (  123.57 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:        eval time = 98284.12 ms /   127 runs   (  773.89 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 126269.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    96.33 ms /   111 runs   (    0.87 ms per token,  1152.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18591.20 ms /   149 tokens (  124.77 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:        eval time = 84625.48 ms /   110 runs   (  769.32 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 103594.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.92 ms /   128 runs   (    0.87 ms per token,  1154.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43140.74 ms /   317 tokens (  136.09 ms per token,     7.35 tokens per second)\n",
      "llama_print_timings:        eval time = 99035.35 ms /   127 runs   (  779.81 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 142607.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.06 ms /   126 runs   (    0.87 ms per token,  1155.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20854.67 ms /   170 tokens (  122.67 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:        eval time = 97223.41 ms /   125 runs   (  777.79 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 118504.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.62 ms /   128 runs   (    0.87 ms per token,  1146.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23698.63 ms /   192 tokens (  123.43 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:        eval time = 99028.14 ms /   127 runs   (  779.75 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 123166.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.80 ms /   128 runs   (    0.87 ms per token,  1155.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42243.76 ms /   314 tokens (  134.53 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:        eval time = 99080.12 ms /   127 runs   (  780.16 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 141752.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.97 ms /   128 runs   (    0.87 ms per token,  1143.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45603.88 ms /   327 tokens (  139.46 ms per token,     7.17 tokens per second)\n",
      "llama_print_timings:        eval time = 111207.14 ms /   127 runs   (  875.65 ms per token,     1.14 tokens per second)\n",
      "llama_print_timings:       total time = 157274.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   102.25 ms /   118 runs   (    0.87 ms per token,  1154.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23024.03 ms /   181 tokens (  127.20 ms per token,     7.86 tokens per second)\n",
      "llama_print_timings:        eval time = 90675.41 ms /   117 runs   (  775.00 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 114101.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.80 ms /   128 runs   (    0.87 ms per token,  1155.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20760.02 ms /   170 tokens (  122.12 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:        eval time = 97924.84 ms /   127 runs   (  771.06 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 119114.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    28.75 ms /    33 runs   (    0.87 ms per token,  1147.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24558.41 ms /   199 tokens (  123.41 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:        eval time = 25469.55 ms /    32 runs   (  795.92 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 50145.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.77 ms /   128 runs   (    0.89 ms per token,  1125.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64636.74 ms /   432 tokens (  149.62 ms per token,     6.68 tokens per second)\n",
      "llama_print_timings:        eval time = 110136.31 ms /   127 runs   (  867.22 ms per token,     1.15 tokens per second)\n",
      "llama_print_timings:       total time = 175252.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.87 ms /     1 runs   (    0.87 ms per token,  1144.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 285779.64 ms /  1310 tokens (  218.15 ms per token,     4.58 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 285791.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    98.39 ms /   108 runs   (    0.91 ms per token,  1097.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35984.14 ms /   266 tokens (  135.28 ms per token,     7.39 tokens per second)\n",
      "llama_print_timings:        eval time = 87766.01 ms /   107 runs   (  820.24 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 124144.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.06 ms /   128 runs   (    0.87 ms per token,  1152.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24144.06 ms /   188 tokens (  128.43 ms per token,     7.79 tokens per second)\n",
      "llama_print_timings:        eval time = 101378.32 ms /   127 runs   (  798.25 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 125967.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    95.92 ms /   109 runs   (    0.88 ms per token,  1136.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 39898.60 ms /   296 tokens (  134.79 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:        eval time = 85158.73 ms /   108 runs   (  788.51 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 125433.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.72 ms /   128 runs   (    0.87 ms per token,  1156.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12392.23 ms /   100 tokens (  123.92 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:        eval time = 98792.38 ms /   127 runs   (  777.89 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 111618.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.27 ms /   128 runs   (    0.87 ms per token,  1150.33 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16164.72 ms /   127 tokens (  127.28 ms per token,     7.86 tokens per second)\n",
      "llama_print_timings:        eval time = 99825.37 ms /   127 runs   (  786.03 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 116432.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   104.95 ms /   121 runs   (    0.87 ms per token,  1152.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15864.73 ms /   126 tokens (  125.91 ms per token,     7.94 tokens per second)\n",
      "llama_print_timings:        eval time = 93246.19 ms /   120 runs   (  777.05 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 109519.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    48.65 ms /    56 runs   (    0.87 ms per token,  1151.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11946.21 ms /    95 tokens (  125.75 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:        eval time = 43182.12 ms /    55 runs   (  785.13 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 55318.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.52 ms /   128 runs   (    0.88 ms per token,  1137.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14576.55 ms /   116 tokens (  125.66 ms per token,     7.96 tokens per second)\n",
      "llama_print_timings:        eval time = 99949.33 ms /   127 runs   (  787.00 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 114987.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    74.24 ms /    85 runs   (    0.87 ms per token,  1144.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11954.32 ms /    93 tokens (  128.54 ms per token,     7.78 tokens per second)\n",
      "llama_print_timings:        eval time = 74142.59 ms /    84 runs   (  882.65 ms per token,     1.13 tokens per second)\n",
      "llama_print_timings:       total time = 86402.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    84.28 ms /    96 runs   (    0.88 ms per token,  1139.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16266.99 ms /   126 tokens (  129.10 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 80351.53 ms /    95 runs   (  845.81 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:       total time = 96952.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   104.57 ms /   121 runs   (    0.86 ms per token,  1157.13 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20566.99 ms /   170 tokens (  120.98 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:        eval time = 95185.09 ms /   120 runs   (  793.21 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 116166.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.37 ms /   126 runs   (    0.87 ms per token,  1152.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12552.83 ms /   105 tokens (  119.55 ms per token,     8.36 tokens per second)\n",
      "llama_print_timings:        eval time = 96668.24 ms /   125 runs   (  773.35 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 109647.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.97 ms /   128 runs   (    0.87 ms per token,  1153.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22597.90 ms /   183 tokens (  123.49 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:        eval time = 97959.12 ms /   127 runs   (  771.33 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 120995.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.62 ms /   128 runs   (    0.86 ms per token,  1157.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34368.53 ms /   269 tokens (  127.76 ms per token,     7.83 tokens per second)\n",
      "llama_print_timings:        eval time = 99982.24 ms /   127 runs   (  787.26 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 134790.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.64 ms /   128 runs   (    0.86 ms per token,  1156.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31930.15 ms /   250 tokens (  127.72 ms per token,     7.83 tokens per second)\n",
      "llama_print_timings:        eval time = 98241.85 ms /   127 runs   (  773.56 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 130610.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.47 ms /   128 runs   (    0.87 ms per token,  1148.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23754.08 ms /   192 tokens (  123.72 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:        eval time = 97746.07 ms /   127 runs   (  769.65 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 121935.17 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.88 ms /   128 runs   (    0.87 ms per token,  1154.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15922.41 ms /    21 tokens (  758.21 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:        eval time = 96542.23 ms /   127 runs   (  760.18 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 112890.50 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.98 ms /   128 runs   (    0.87 ms per token,  1153.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6496.45 ms /    43 tokens (  151.08 ms per token,     6.62 tokens per second)\n",
      "llama_print_timings:        eval time = 99377.01 ms /   127 runs   (  782.50 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 106315.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.78 ms /   128 runs   (    0.87 ms per token,  1155.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40254.66 ms /   306 tokens (  131.55 ms per token,     7.60 tokens per second)\n",
      "llama_print_timings:        eval time = 99366.76 ms /   127 runs   (  782.42 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 140059.00 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.66 ms /   128 runs   (    0.86 ms per token,  1156.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6916.67 ms /    46 tokens (  150.36 ms per token,     6.65 tokens per second)\n",
      "llama_print_timings:        eval time = 98048.91 ms /   127 runs   (  772.04 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 105394.68 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.57 ms /   128 runs   (    0.86 ms per token,  1157.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6424.43 ms /    41 tokens (  156.69 ms per token,     6.38 tokens per second)\n",
      "llama_print_timings:        eval time = 97974.85 ms /   127 runs   (  771.46 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 104826.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    54.55 ms /    63 runs   (    0.87 ms per token,  1154.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16379.85 ms /   130 tokens (  126.00 ms per token,     7.94 tokens per second)\n",
      "llama_print_timings:        eval time = 47037.88 ms /    62 runs   (  758.68 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 63632.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    79.75 ms /    92 runs   (    0.87 ms per token,  1153.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16431.01 ms /   136 tokens (  120.82 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:        eval time = 70522.86 ms /    91 runs   (  774.98 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 87259.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.14 ms /   128 runs   (    0.88 ms per token,  1131.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 139607.69 ms /   788 tokens (  177.17 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:        eval time = 113485.83 ms /   127 runs   (  893.59 ms per token,     1.12 tokens per second)\n",
      "llama_print_timings:       total time = 253561.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.98 ms /     1 runs   (    0.98 ms per token,  1024.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 277847.61 ms /  1281 tokens (  216.90 ms per token,     4.61 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 277860.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   102.29 ms /   112 runs   (    0.91 ms per token,  1094.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17552.22 ms /   121 tokens (  145.06 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:        eval time = 88219.51 ms /   111 runs   (  794.77 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 106158.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    40.27 ms /    45 runs   (    0.89 ms per token,  1117.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 129726.16 ms /   731 tokens (  177.46 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:        eval time = 36714.76 ms /    44 runs   (  834.43 ms per token,     1.20 tokens per second)\n",
      "llama_print_timings:       total time = 166606.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   116.05 ms /   128 runs   (    0.91 ms per token,  1102.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 183169.71 ms /   954 tokens (  192.00 ms per token,     5.21 tokens per second)\n",
      "llama_print_timings:        eval time = 107718.50 ms /   127 runs   (  848.18 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:       total time = 291340.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.55 ms /   128 runs   (    0.89 ms per token,  1117.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 248857.94 ms /  1198 tokens (  207.73 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:        eval time = 106663.76 ms /   127 runs   (  839.87 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time = 355976.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.89 ms /     1 runs   (    0.89 ms per token,  1118.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 258289.43 ms /  1218 tokens (  212.06 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 258305.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    22.74 ms /    25 runs   (    0.91 ms per token,  1099.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 126390.24 ms /   715 tokens (  176.77 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:        eval time = 20867.55 ms /    24 runs   (  869.48 ms per token,     1.15 tokens per second)\n",
      "llama_print_timings:       total time = 147354.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    66.56 ms /    72 runs   (    0.92 ms per token,  1081.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 153143.00 ms /   833 tokens (  183.85 ms per token,     5.44 tokens per second)\n",
      "llama_print_timings:        eval time = 76006.55 ms /    71 runs   ( 1070.51 ms per token,     0.93 tokens per second)\n",
      "llama_print_timings:       total time = 229426.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.10 ms /   128 runs   (    0.89 ms per token,  1121.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20494.37 ms /   118 tokens (  173.68 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:        eval time = 106990.47 ms /   127 runs   (  842.44 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time = 127947.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    21.21 ms /    24 runs   (    0.88 ms per token,  1131.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17037.58 ms /   135 tokens (  126.20 ms per token,     7.92 tokens per second)\n",
      "llama_print_timings:        eval time = 19139.17 ms /    23 runs   (  832.14 ms per token,     1.20 tokens per second)\n",
      "llama_print_timings:       total time = 36260.99 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    91.31 ms /   105 runs   (    0.87 ms per token,  1149.98 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17918.70 ms /    24 tokens (  746.61 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:        eval time = 80109.43 ms /   104 runs   (  770.28 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 98373.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     1.78 ms /     2 runs   (    0.89 ms per token,  1124.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 509676.73 ms /  1955 tokens (  260.70 ms per token,     3.84 tokens per second)\n",
      "llama_print_timings:        eval time =  4356.74 ms /     1 runs   ( 4356.74 ms per token,     0.23 tokens per second)\n",
      "llama_print_timings:       total time = 514055.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    73.90 ms /    81 runs   (    0.91 ms per token,  1096.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10170.20 ms /    70 tokens (  145.29 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:        eval time = 65559.93 ms /    80 runs   (  819.50 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 76023.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     6.15 ms /     7 runs   (    0.88 ms per token,  1138.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 366886.95 ms /  1538 tokens (  238.55 ms per token,     4.19 tokens per second)\n",
      "llama_print_timings:        eval time =  5702.44 ms /     6 runs   (  950.41 ms per token,     1.05 tokens per second)\n",
      "llama_print_timings:       total time = 372623.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.86 ms /     1 runs   (    0.86 ms per token,  1156.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 287030.92 ms /  1302 tokens (  220.45 ms per token,     4.54 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 287042.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.28 ms /   128 runs   (    0.90 ms per token,  1110.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20051.36 ms /   126 tokens (  159.14 ms per token,     6.28 tokens per second)\n",
      "llama_print_timings:        eval time = 103417.02 ms /   127 runs   (  814.31 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 123933.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.87 ms /     1 runs   (    0.87 ms per token,  1149.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 522572.53 ms /  1960 tokens (  266.62 ms per token,     3.75 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 522588.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    85.76 ms /    95 runs   (    0.90 ms per token,  1107.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23063.64 ms /   155 tokens (  148.80 ms per token,     6.72 tokens per second)\n",
      "llama_print_timings:        eval time = 92970.93 ms /    94 runs   (  989.05 ms per token,     1.01 tokens per second)\n",
      "llama_print_timings:       total time = 116392.99 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.62 ms /   128 runs   (    0.88 ms per token,  1136.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18281.19 ms /    24 tokens (  761.72 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:        eval time = 103585.67 ms /   127 runs   (  815.64 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 122325.10 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.20 ms /   128 runs   (    0.87 ms per token,  1151.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9045.41 ms /    12 tokens (  753.78 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:        eval time = 101183.97 ms /   127 runs   (  796.72 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 110665.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.11 ms /   128 runs   (    0.90 ms per token,  1112.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 234437.32 ms /  1142 tokens (  205.29 ms per token,     4.87 tokens per second)\n",
      "llama_print_timings:        eval time = 112016.42 ms /   127 runs   (  882.02 ms per token,     1.13 tokens per second)\n",
      "llama_print_timings:       total time = 346927.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    39.05 ms /    45 runs   (    0.87 ms per token,  1152.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17039.49 ms /   138 tokens (  123.47 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:        eval time = 36162.36 ms /    44 runs   (  821.87 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 53358.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    52.12 ms /    60 runs   (    0.87 ms per token,  1151.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15677.21 ms /   129 tokens (  121.53 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:        eval time = 47621.57 ms /    59 runs   (  807.15 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 63509.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     1 runs   (    0.94 ms per token,  1061.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 103843.80 ms /   630 tokens (  164.83 ms per token,     6.07 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 103852.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    89.51 ms /   100 runs   (    0.90 ms per token,  1117.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 293602.26 ms /  1325 tokens (  221.59 ms per token,     4.51 tokens per second)\n",
      "llama_print_timings:        eval time = 88216.56 ms /    99 runs   (  891.08 ms per token,     1.12 tokens per second)\n",
      "llama_print_timings:       total time = 382183.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    47.33 ms /    54 runs   (    0.88 ms per token,  1140.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14443.35 ms /   115 tokens (  125.59 ms per token,     7.96 tokens per second)\n",
      "llama_print_timings:        eval time = 51683.26 ms /    53 runs   (  975.16 ms per token,     1.03 tokens per second)\n",
      "llama_print_timings:       total time = 66327.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    59.90 ms /    68 runs   (    0.88 ms per token,  1135.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11434.29 ms /    89 tokens (  128.48 ms per token,     7.78 tokens per second)\n",
      "llama_print_timings:        eval time = 74089.19 ms /    67 runs   ( 1105.81 ms per token,     0.90 tokens per second)\n",
      "llama_print_timings:       total time = 85802.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    49.05 ms /    53 runs   (    0.93 ms per token,  1080.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51645.95 ms /   350 tokens (  147.56 ms per token,     6.78 tokens per second)\n",
      "llama_print_timings:        eval time = 44056.78 ms /    52 runs   (  847.25 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:       total time = 95906.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    14.96 ms /    17 runs   (    0.88 ms per token,  1136.67 tokens per second)\n",
      "llama_print_timings: prompt eval time = 41814.44 ms /   307 tokens (  136.20 ms per token,     7.34 tokens per second)\n",
      "llama_print_timings:        eval time = 12858.17 ms /    16 runs   (  803.64 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 54735.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.11 ms /   128 runs   (    0.88 ms per token,  1141.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12116.55 ms /    88 tokens (  137.69 ms per token,     7.26 tokens per second)\n",
      "llama_print_timings:        eval time = 103977.76 ms /   127 runs   (  818.72 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 116538.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.08 ms /   128 runs   (    0.87 ms per token,  1152.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11145.28 ms /    86 tokens (  129.60 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 100108.34 ms /   127 runs   (  788.25 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 111693.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    26.54 ms /    30 runs   (    0.88 ms per token,  1130.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19965.81 ms /   163 tokens (  122.49 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:        eval time = 23675.11 ms /    29 runs   (  816.38 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 43762.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.56 ms /   126 runs   (    0.87 ms per token,  1150.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11439.72 ms /    87 tokens (  131.49 ms per token,     7.61 tokens per second)\n",
      "llama_print_timings:        eval time = 96715.65 ms /   125 runs   (  773.73 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 108575.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    43.21 ms /    47 runs   (    0.92 ms per token,  1087.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 235545.06 ms /  1153 tokens (  204.29 ms per token,     4.90 tokens per second)\n",
      "llama_print_timings:        eval time = 41318.57 ms /    46 runs   (  898.23 ms per token,     1.11 tokens per second)\n",
      "llama_print_timings:       total time = 277043.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    43.15 ms /    48 runs   (    0.90 ms per token,  1112.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30119.46 ms /   231 tokens (  130.39 ms per token,     7.67 tokens per second)\n",
      "llama_print_timings:        eval time = 37100.75 ms /    47 runs   (  789.38 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 67389.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.93 ms /   126 runs   (    0.87 ms per token,  1146.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11167.88 ms /    87 tokens (  128.37 ms per token,     7.79 tokens per second)\n",
      "llama_print_timings:        eval time = 99641.34 ms /   125 runs   (  797.13 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 111247.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    11.32 ms /    13 runs   (    0.87 ms per token,  1148.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10813.44 ms /    87 tokens (  124.29 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:        eval time =  9115.10 ms /    12 runs   (  759.59 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 19971.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.72 ms /   128 runs   (    0.87 ms per token,  1145.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10952.38 ms /    86 tokens (  127.35 ms per token,     7.85 tokens per second)\n",
      "llama_print_timings:        eval time = 99860.27 ms /   127 runs   (  786.30 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 111271.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    60.84 ms /    70 runs   (    0.87 ms per token,  1150.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34045.78 ms /   264 tokens (  128.96 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 56056.39 ms /    69 runs   (  812.41 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 90348.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.43 ms /   128 runs   (    0.87 ms per token,  1148.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11264.50 ms /    87 tokens (  129.48 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 99641.46 ms /   127 runs   (  784.58 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 111348.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    48.73 ms /    56 runs   (    0.87 ms per token,  1149.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11397.41 ms /    89 tokens (  128.06 ms per token,     7.81 tokens per second)\n",
      "llama_print_timings:        eval time = 42350.23 ms /    55 runs   (  770.00 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 53933.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    67.81 ms /    78 runs   (    0.87 ms per token,  1150.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29575.67 ms /   231 tokens (  128.03 ms per token,     7.81 tokens per second)\n",
      "llama_print_timings:        eval time = 61290.44 ms /    77 runs   (  795.98 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 91129.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    24.44 ms /    28 runs   (    0.87 ms per token,  1145.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15605.51 ms /   126 tokens (  123.85 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:        eval time = 21423.40 ms /    27 runs   (  793.46 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 37124.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    33.22 ms /    38 runs   (    0.87 ms per token,  1143.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11702.90 ms /    93 tokens (  125.84 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:        eval time = 29307.43 ms /    37 runs   (  792.09 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 41141.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   106.19 ms /   122 runs   (    0.87 ms per token,  1148.89 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11662.91 ms /    89 tokens (  131.04 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:        eval time = 93582.46 ms /   121 runs   (  773.41 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 105650.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.68 ms /   128 runs   (    0.88 ms per token,  1135.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11594.91 ms /    87 tokens (  133.27 ms per token,     7.50 tokens per second)\n",
      "llama_print_timings:        eval time = 124778.11 ms /   127 runs   (  982.50 ms per token,     1.02 tokens per second)\n",
      "llama_print_timings:       total time = 136856.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    55.55 ms /    60 runs   (    0.93 ms per token,  1080.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 38449.71 ms /   253 tokens (  151.98 ms per token,     6.58 tokens per second)\n",
      "llama_print_timings:        eval time = 47993.72 ms /    59 runs   (  813.45 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 86656.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    34.89 ms /    40 runs   (    0.87 ms per token,  1146.39 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11186.85 ms /    87 tokens (  128.58 ms per token,     7.78 tokens per second)\n",
      "llama_print_timings:        eval time = 31207.32 ms /    39 runs   (  800.19 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 42531.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    20.21 ms /    23 runs   (    0.88 ms per token,  1138.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28175.30 ms /   226 tokens (  124.67 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:        eval time = 19668.94 ms /    22 runs   (  894.04 ms per token,     1.12 tokens per second)\n",
      "llama_print_timings:       total time = 47927.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.78 ms /   128 runs   (    0.87 ms per token,  1145.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12646.81 ms /    87 tokens (  145.37 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:        eval time = 100409.42 ms /   127 runs   (  790.63 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 113497.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.61 ms /   125 runs   (    0.88 ms per token,  1140.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11087.81 ms /    86 tokens (  128.93 ms per token,     7.76 tokens per second)\n",
      "llama_print_timings:        eval time = 100368.71 ms /   124 runs   (  809.43 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 111898.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    83.95 ms /    96 runs   (    0.87 ms per token,  1143.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11603.86 ms /    90 tokens (  128.93 ms per token,     7.76 tokens per second)\n",
      "llama_print_timings:        eval time = 76398.51 ms /    95 runs   (  804.19 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 88340.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    48.92 ms /    56 runs   (    0.87 ms per token,  1144.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11315.39 ms /    88 tokens (  128.58 ms per token,     7.78 tokens per second)\n",
      "llama_print_timings:        eval time = 44493.28 ms /    55 runs   (  808.97 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 56008.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    38.48 ms /    44 runs   (    0.87 ms per token,  1143.39 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20548.07 ms /   167 tokens (  123.04 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:        eval time = 33823.17 ms /    43 runs   (  786.59 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 54527.53 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    64.50 ms /    74 runs   (    0.87 ms per token,  1147.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14849.41 ms /    19 tokens (  781.55 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:        eval time = 56218.99 ms /    73 runs   (  770.12 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 71322.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.41 ms /   128 runs   (    0.87 ms per token,  1148.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12807.88 ms /    98 tokens (  130.69 ms per token,     7.65 tokens per second)\n",
      "llama_print_timings:        eval time = 100358.00 ms /   127 runs   (  790.22 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 113609.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.92 ms /   128 runs   (    0.88 ms per token,  1133.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 112067.80 ms /   661 tokens (  169.54 ms per token,     5.90 tokens per second)\n",
      "llama_print_timings:        eval time = 105036.45 ms /   127 runs   (  827.06 ms per token,     1.21 tokens per second)\n",
      "llama_print_timings:       total time = 217566.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.87 ms /     1 runs   (    0.87 ms per token,  1152.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 353589.29 ms /  1515 tokens (  233.39 ms per token,     4.28 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 353604.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.43 ms /   128 runs   (    0.89 ms per token,  1128.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 364038.07 ms /  1518 tokens (  239.81 ms per token,     4.17 tokens per second)\n",
      "llama_print_timings:        eval time = 125725.73 ms /   127 runs   (  989.97 ms per token,     1.01 tokens per second)\n",
      "llama_print_timings:       total time = 490266.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.00 ms /   128 runs   (    0.90 ms per token,  1113.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 220995.99 ms /  1075 tokens (  205.58 ms per token,     4.86 tokens per second)\n",
      "llama_print_timings:        eval time = 108459.26 ms /   127 runs   (  854.01 ms per token,     1.17 tokens per second)\n",
      "llama_print_timings:       total time = 329929.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.85 ms /   128 runs   (    0.87 ms per token,  1144.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19778.66 ms /   160 tokens (  123.62 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:        eval time = 101363.73 ms /   127 runs   (  798.14 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 121590.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   107.36 ms /   124 runs   (    0.87 ms per token,  1154.98 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22186.41 ms /   177 tokens (  125.35 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:        eval time = 96125.69 ms /   123 runs   (  781.51 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 118731.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.90 ms /     1 runs   (    0.90 ms per token,  1112.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 186056.07 ms /   973 tokens (  191.22 ms per token,     5.23 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 186064.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.91 ms /     1 runs   (    0.91 ms per token,  1100.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35669.11 ms /   258 tokens (  138.25 ms per token,     7.23 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 35676.95 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.48 ms /   128 runs   (    0.89 ms per token,  1127.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20077.44 ms /    25 tokens (  803.10 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:        eval time = 98770.61 ms /   127 runs   (  777.72 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 119291.43 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.48 ms /   128 runs   (    0.87 ms per token,  1148.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12205.77 ms /    16 tokens (  762.86 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:        eval time = 100209.99 ms /   127 runs   (  789.06 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 112857.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.92 ms /     1 runs   (    0.92 ms per token,  1091.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 491737.04 ms /  1902 tokens (  258.54 ms per token,     3.87 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 491753.00 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.09 ms /   128 runs   (    0.89 ms per token,  1121.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18389.87 ms /    20 tokens (  919.49 ms per token,     1.09 tokens per second)\n",
      "llama_print_timings:        eval time = 106398.89 ms /   127 runs   (  837.79 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time = 125263.46 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.41 ms /   128 runs   (    0.88 ms per token,  1138.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16330.19 ms /    17 tokens (  960.60 ms per token,     1.04 tokens per second)\n",
      "llama_print_timings:        eval time = 132132.02 ms /   127 runs   ( 1040.41 ms per token,     0.96 tokens per second)\n",
      "llama_print_timings:       total time = 148942.05 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.24 ms /   128 runs   (    0.88 ms per token,  1140.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13134.48 ms /    17 tokens (  772.62 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:        eval time = 98550.29 ms /   127 runs   (  775.99 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 112123.82 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.83 ms /   128 runs   (    0.87 ms per token,  1144.61 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20664.85 ms /    25 tokens (  826.59 ms per token,     1.21 tokens per second)\n",
      "llama_print_timings:        eval time = 100687.32 ms /   127 runs   (  792.81 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 121808.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    63.77 ms /    71 runs   (    0.90 ms per token,  1113.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 261415.00 ms /  1225 tokens (  213.40 ms per token,     4.69 tokens per second)\n",
      "llama_print_timings:        eval time = 62708.28 ms /    70 runs   (  895.83 ms per token,     1.12 tokens per second)\n",
      "llama_print_timings:       total time = 324391.78 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.46 ms /   128 runs   (    0.87 ms per token,  1148.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13729.14 ms /    18 tokens (  762.73 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:        eval time = 101434.59 ms /   127 runs   (  798.70 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 115597.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    68.80 ms /    79 runs   (    0.87 ms per token,  1148.21 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16900.76 ms /   133 tokens (  127.07 ms per token,     7.87 tokens per second)\n",
      "llama_print_timings:        eval time = 62789.60 ms /    78 runs   (  804.99 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 79962.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   108.65 ms /   125 runs   (    0.87 ms per token,  1150.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17851.16 ms /   143 tokens (  124.83 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:        eval time = 98213.49 ms /   124 runs   (  792.04 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 116485.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   108.41 ms /   122 runs   (    0.89 ms per token,  1125.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 393148.13 ms /  1628 tokens (  241.49 ms per token,     4.14 tokens per second)\n",
      "llama_print_timings:        eval time = 105514.96 ms /   121 runs   (  872.02 ms per token,     1.15 tokens per second)\n",
      "llama_print_timings:       total time = 499100.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.72 ms /   128 runs   (    0.87 ms per token,  1145.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17306.14 ms /   142 tokens (  121.87 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:        eval time = 101169.73 ms /   127 runs   (  796.61 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 118904.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.49 ms /   128 runs   (    0.89 ms per token,  1127.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 111090.80 ms /   651 tokens (  170.65 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:        eval time = 115093.66 ms /   127 runs   (  906.25 ms per token,     1.10 tokens per second)\n",
      "llama_print_timings:       total time = 226646.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.91 ms /     1 runs   (    0.91 ms per token,  1094.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 438111.60 ms /  1752 tokens (  250.06 ms per token,     4.00 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 438128.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.42 ms /   128 runs   (    0.89 ms per token,  1118.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 123658.40 ms /   689 tokens (  179.48 ms per token,     5.57 tokens per second)\n",
      "llama_print_timings:        eval time = 109791.02 ms /   127 runs   (  864.50 ms per token,     1.16 tokens per second)\n",
      "llama_print_timings:       total time = 233913.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    45.65 ms /    52 runs   (    0.88 ms per token,  1139.08 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65211.08 ms /   434 tokens (  150.26 ms per token,     6.66 tokens per second)\n",
      "llama_print_timings:        eval time = 42206.87 ms /    51 runs   (  827.59 ms per token,     1.21 tokens per second)\n",
      "llama_print_timings:       total time = 107591.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     1.03 ms /     1 runs   (    1.03 ms per token,   968.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 370591.58 ms /  1544 tokens (  240.02 ms per token,     4.17 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 370615.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   116.09 ms /   128 runs   (    0.91 ms per token,  1102.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 127657.94 ms /   718 tokens (  177.80 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:        eval time = 106219.15 ms /   127 runs   (  836.37 ms per token,     1.20 tokens per second)\n",
      "llama_print_timings:       total time = 234338.81 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.14 ms /   128 runs   (    0.88 ms per token,  1141.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6538.94 ms /    38 tokens (  172.08 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:        eval time = 99965.64 ms /   127 runs   (  787.13 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 106937.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.69 ms /   128 runs   (    0.90 ms per token,  1106.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 261852.68 ms /  1238 tokens (  211.51 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:        eval time = 149517.16 ms /   127 runs   ( 1177.30 ms per token,     0.85 tokens per second)\n",
      "llama_print_timings:       total time = 411897.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     2.65 ms /     3 runs   (    0.88 ms per token,  1131.65 tokens per second)\n",
      "llama_print_timings: prompt eval time = 393603.18 ms /  1608 tokens (  244.78 ms per token,     4.09 tokens per second)\n",
      "llama_print_timings:        eval time =  3990.37 ms /     2 runs   ( 1995.18 ms per token,     0.50 tokens per second)\n",
      "llama_print_timings:       total time = 397617.24 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.52 ms /   128 runs   (    0.89 ms per token,  1117.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6674.21 ms /    36 tokens (  185.39 ms per token,     5.39 tokens per second)\n",
      "llama_print_timings:        eval time = 106036.74 ms /   127 runs   (  834.93 ms per token,     1.20 tokens per second)\n",
      "llama_print_timings:       total time = 113179.92 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.21 ms /   128 runs   (    0.88 ms per token,  1140.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24054.80 ms /    31 tokens (  775.96 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:        eval time = 103764.17 ms /   127 runs   (  817.04 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 128264.15 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.73 ms /   128 runs   (    0.87 ms per token,  1145.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5901.96 ms /    32 tokens (  184.44 ms per token,     5.42 tokens per second)\n",
      "llama_print_timings:        eval time = 101184.24 ms /   127 runs   (  796.73 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 107528.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.36 ms /   128 runs   (    0.90 ms per token,  1109.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 121272.69 ms /   701 tokens (  173.00 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:        eval time = 107094.28 ms /   127 runs   (  843.26 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time = 228843.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.18 ms /   128 runs   (    0.89 ms per token,  1121.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 326141.87 ms /  1409 tokens (  231.47 ms per token,     4.32 tokens per second)\n",
      "llama_print_timings:        eval time = 636073.88 ms /   127 runs   ( 5008.46 ms per token,     0.20 tokens per second)\n",
      "llama_print_timings:       total time = 962847.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.58 ms /   128 runs   (    0.90 ms per token,  1117.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 285303.08 ms /  1290 tokens (  221.17 ms per token,     4.52 tokens per second)\n",
      "llama_print_timings:        eval time = 117352.74 ms /   127 runs   (  924.04 ms per token,     1.08 tokens per second)\n",
      "llama_print_timings:       total time = 403134.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.92 ms /     1 runs   (    0.92 ms per token,  1085.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 473092.25 ms /  1840 tokens (  257.12 ms per token,     3.89 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 473108.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   116.85 ms /   128 runs   (    0.91 ms per token,  1095.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27772.88 ms /   192 tokens (  144.65 ms per token,     6.91 tokens per second)\n",
      "llama_print_timings:        eval time = 128230.33 ms /   127 runs   ( 1009.69 ms per token,     0.99 tokens per second)\n",
      "llama_print_timings:       total time = 156523.05 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.00 ms /   128 runs   (    0.87 ms per token,  1142.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9614.84 ms /    12 tokens (  801.24 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:        eval time = 102795.89 ms /   127 runs   (  809.42 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 112859.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.38 ms /   128 runs   (    0.89 ms per token,  1128.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 146081.81 ms /   797 tokens (  183.29 ms per token,     5.46 tokens per second)\n",
      "llama_print_timings:        eval time = 116497.35 ms /   127 runs   (  917.30 ms per token,     1.09 tokens per second)\n",
      "llama_print_timings:       total time = 263061.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.59 ms /   128 runs   (    0.90 ms per token,  1107.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 199578.57 ms /  1005 tokens (  198.59 ms per token,     5.04 tokens per second)\n",
      "llama_print_timings:        eval time = 111782.13 ms /   127 runs   (  880.17 ms per token,     1.14 tokens per second)\n",
      "llama_print_timings:       total time = 311839.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.54 ms /   128 runs   (    0.88 ms per token,  1137.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 60887.90 ms /   414 tokens (  147.07 ms per token,     6.80 tokens per second)\n",
      "llama_print_timings:        eval time = 102334.54 ms /   127 runs   (  805.78 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 163655.97 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.61 ms /   128 runs   (    0.87 ms per token,  1146.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16206.76 ms /    21 tokens (  771.75 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:        eval time = 107300.95 ms /   127 runs   (  844.89 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:       total time = 123956.40 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.08 ms /   128 runs   (    0.88 ms per token,  1142.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22059.26 ms /    28 tokens (  787.83 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:        eval time = 102602.73 ms /   127 runs   (  807.90 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 125097.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.92 ms /   128 runs   (    0.89 ms per token,  1123.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 133821.26 ms /   768 tokens (  174.25 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:        eval time = 110804.03 ms /   127 runs   (  872.47 ms per token,     1.15 tokens per second)\n",
      "llama_print_timings:       total time = 245078.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.88 ms /     1 runs   (    0.88 ms per token,  1141.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 299693.73 ms /  1358 tokens (  220.69 ms per token,     4.53 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 299705.89 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.14 ms /   128 runs   (    0.88 ms per token,  1131.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22793.83 ms /    27 tokens (  844.22 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:        eval time = 103944.44 ms /   127 runs   (  818.46 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 127191.27 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.97 ms /   128 runs   (    0.87 ms per token,  1143.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18058.24 ms /    24 tokens (  752.43 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:        eval time = 101423.35 ms /   127 runs   (  798.61 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 119930.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     2.79 ms /     3 runs   (    0.93 ms per token,  1075.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 416702.75 ms /  1681 tokens (  247.89 ms per token,     4.03 tokens per second)\n",
      "llama_print_timings:        eval time =  6183.65 ms /     2 runs   ( 3091.82 ms per token,     0.32 tokens per second)\n",
      "llama_print_timings:       total time = 422911.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.97 ms /   128 runs   (    0.91 ms per token,  1103.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 257049.03 ms /  1182 tokens (  217.47 ms per token,     4.60 tokens per second)\n",
      "llama_print_timings:        eval time = 114278.06 ms /   127 runs   (  899.83 ms per token,     1.11 tokens per second)\n",
      "llama_print_timings:       total time = 371806.65 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.11 ms /   128 runs   (    0.88 ms per token,  1141.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19323.24 ms /    26 tokens (  743.20 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 102756.19 ms /   127 runs   (  809.10 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 122531.80 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    71.72 ms /    82 runs   (    0.87 ms per token,  1143.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20134.35 ms /    27 tokens (  745.72 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:        eval time = 65843.39 ms /    81 runs   (  812.88 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 86256.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.49 ms /   128 runs   (    0.89 ms per token,  1118.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 287931.64 ms /  1329 tokens (  216.65 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:        eval time = 112356.36 ms /   127 runs   (  884.70 ms per token,     1.13 tokens per second)\n",
      "llama_print_timings:       total time = 400759.04 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.24 ms /   128 runs   (    0.88 ms per token,  1140.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21211.34 ms /    27 tokens (  785.61 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:        eval time = 99339.74 ms /   127 runs   (  782.20 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 120986.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.60 ms /   128 runs   (    0.89 ms per token,  1126.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 107434.30 ms /   625 tokens (  171.89 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:        eval time = 103928.51 ms /   127 runs   (  818.33 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 211808.81 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    43.05 ms /    49 runs   (    0.88 ms per token,  1138.08 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19535.96 ms /    25 tokens (  781.44 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:        eval time = 38454.81 ms /    48 runs   (  801.14 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 58156.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.67 ms /   128 runs   (    0.90 ms per token,  1116.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 228508.43 ms /  1126 tokens (  202.94 ms per token,     4.93 tokens per second)\n",
      "llama_print_timings:        eval time = 107459.90 ms /   127 runs   (  846.14 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:       total time = 336421.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.07 ms /   128 runs   (    0.89 ms per token,  1122.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 264533.31 ms /  1208 tokens (  218.98 ms per token,     4.57 tokens per second)\n",
      "llama_print_timings:        eval time = 112127.82 ms /   127 runs   (  882.90 ms per token,     1.13 tokens per second)\n",
      "llama_print_timings:       total time = 377117.25 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.72 ms /   128 runs   (    0.88 ms per token,  1135.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18860.90 ms /    25 tokens (  754.44 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:        eval time = 114181.07 ms /   127 runs   (  899.06 ms per token,     1.11 tokens per second)\n",
      "llama_print_timings:       total time = 133507.10 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.01 ms /   128 runs   (    0.88 ms per token,  1132.65 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23308.53 ms /    30 tokens (  776.95 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:        eval time = 103874.42 ms /   127 runs   (  817.91 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 127637.97 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.01 ms /   128 runs   (    0.88 ms per token,  1142.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6247.35 ms /    37 tokens (  168.85 ms per token,     5.92 tokens per second)\n",
      "llama_print_timings:        eval time = 103449.11 ms /   127 runs   (  814.56 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 110143.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.68 ms /   128 runs   (    0.90 ms per token,  1106.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 455754.45 ms /  1780 tokens (  256.04 ms per token,     3.91 tokens per second)\n",
      "llama_print_timings:        eval time = 127486.68 ms /   127 runs   ( 1003.83 ms per token,     1.00 tokens per second)\n",
      "llama_print_timings:       total time = 583779.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     2.63 ms /     3 runs   (    0.88 ms per token,  1140.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 416755.85 ms /  1654 tokens (  251.97 ms per token,     3.97 tokens per second)\n",
      "llama_print_timings:        eval time =  4570.52 ms /     2 runs   ( 2285.26 ms per token,     0.44 tokens per second)\n",
      "llama_print_timings:       total time = 421353.25 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.37 ms /   128 runs   (    0.90 ms per token,  1109.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5438.78 ms /    33 tokens (  164.81 ms per token,     6.07 tokens per second)\n",
      "llama_print_timings:        eval time = 107004.48 ms /   127 runs   (  842.55 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time = 112903.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.87 ms /     1 runs   (    0.87 ms per token,  1145.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 438264.39 ms /  1755 tokens (  249.72 ms per token,     4.00 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 438279.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.55 ms /   128 runs   (    0.90 ms per token,  1107.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 485347.60 ms /  1830 tokens (  265.22 ms per token,     3.77 tokens per second)\n",
      "llama_print_timings:        eval time = 133036.11 ms /   127 runs   ( 1047.53 ms per token,     0.95 tokens per second)\n",
      "llama_print_timings:       total time = 618894.01 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.32 ms /   128 runs   (    0.89 ms per token,  1129.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10148.35 ms /    63 tokens (  161.08 ms per token,     6.21 tokens per second)\n",
      "llama_print_timings:        eval time = 123069.54 ms /   127 runs   (  969.05 ms per token,     1.03 tokens per second)\n",
      "llama_print_timings:       total time = 133694.05 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.61 ms /   128 runs   (    0.86 ms per token,  1157.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6011.63 ms /    42 tokens (  143.13 ms per token,     6.99 tokens per second)\n",
      "llama_print_timings:        eval time = 94838.29 ms /   127 runs   (  746.76 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 101258.13 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.36 ms /   128 runs   (    0.87 ms per token,  1149.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4958.98 ms /    32 tokens (  154.97 ms per token,     6.45 tokens per second)\n",
      "llama_print_timings:        eval time = 95219.80 ms /   127 runs   (  749.76 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 100589.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.88 ms /     1 runs   (    0.88 ms per token,  1136.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 377944.66 ms /  1626 tokens (  232.44 ms per token,     4.30 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 377960.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   118.44 ms /   128 runs   (    0.93 ms per token,  1080.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 127531.34 ms /   735 tokens (  173.51 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:        eval time = 101107.04 ms /   127 runs   (  796.12 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 229083.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     2.64 ms /     3 runs   (    0.88 ms per token,  1135.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 414449.13 ms /  1723 tokens (  240.54 ms per token,     4.16 tokens per second)\n",
      "llama_print_timings:        eval time =  1946.48 ms /     2 runs   (  973.24 ms per token,     1.03 tokens per second)\n",
      "llama_print_timings:       total time = 416417.08 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.06 ms /   128 runs   (    0.90 ms per token,  1112.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18532.61 ms /    25 tokens (  741.30 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 96212.63 ms /   127 runs   (  757.58 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 115171.85 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.58 ms /   128 runs   (    0.86 ms per token,  1157.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14641.58 ms /    20 tokens (  732.08 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 94766.16 ms /   127 runs   (  746.19 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 109816.74 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.82 ms /   128 runs   (    0.87 ms per token,  1155.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13903.75 ms /    19 tokens (  731.78 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 94910.13 ms /   127 runs   (  747.32 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 109222.58 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.58 ms /   128 runs   (    0.86 ms per token,  1157.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18009.63 ms /    24 tokens (  750.40 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:        eval time = 95415.55 ms /   127 runs   (  751.30 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 113839.15 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.07 ms /   128 runs   (    0.87 ms per token,  1152.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17366.58 ms /    23 tokens (  755.07 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:        eval time = 94720.69 ms /   127 runs   (  745.83 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 112498.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.87 ms /     1 runs   (    0.87 ms per token,  1144.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 297521.46 ms /  1376 tokens (  216.22 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 297533.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.88 ms /     1 runs   (    0.88 ms per token,  1132.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 300211.19 ms /  1367 tokens (  219.61 ms per token,     4.55 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 300225.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.87 ms /     1 runs   (    0.87 ms per token,  1144.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 171472.57 ms /   895 tokens (  191.59 ms per token,     5.22 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 171481.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.89 ms /     1 runs   (    0.89 ms per token,  1126.13 tokens per second)\n",
      "llama_print_timings: prompt eval time = 453310.84 ms /  1824 tokens (  248.53 ms per token,     4.02 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 453328.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.88 ms /     1 runs   (    0.88 ms per token,  1131.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 252677.30 ms /  1216 tokens (  207.79 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 252688.98 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   117.42 ms /   128 runs   (    0.92 ms per token,  1090.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9923.16 ms /    13 tokens (  763.32 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:        eval time = 98644.75 ms /   127 runs   (  776.73 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 109006.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.88 ms /     1 runs   (    0.88 ms per token,  1138.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 217889.43 ms /  1108 tokens (  196.65 ms per token,     5.09 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 217900.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.88 ms /     1 runs   (    0.88 ms per token,  1136.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 329764.37 ms /  1469 tokens (  224.48 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 329775.56 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   116.69 ms /   128 runs   (    0.91 ms per token,  1096.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14791.08 ms /    19 tokens (  778.48 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:        eval time = 97718.46 ms /   127 runs   (  769.44 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 112936.72 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.58 ms /   128 runs   (    0.87 ms per token,  1147.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23002.88 ms /    31 tokens (  742.03 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 95470.66 ms /   127 runs   (  751.74 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 118884.28 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.78 ms /   128 runs   (    0.87 ms per token,  1145.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5161.21 ms /    32 tokens (  161.29 ms per token,     6.20 tokens per second)\n",
      "llama_print_timings:        eval time = 98813.65 ms /   127 runs   (  778.06 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 104394.97 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.41 ms /   128 runs   (    0.87 ms per token,  1148.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6232.25 ms /    35 tokens (  178.06 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:        eval time = 96004.46 ms /   127 runs   (  755.94 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 102651.39 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.13 ms /   128 runs   (    0.87 ms per token,  1151.80 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17620.32 ms /    24 tokens (  734.18 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 95512.80 ms /   127 runs   (  752.07 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 113543.68 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.21 ms /   128 runs   (    0.87 ms per token,  1151.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11104.81 ms /    15 tokens (  740.32 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 95023.83 ms /   127 runs   (  748.22 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 106533.40 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.76 ms /   128 runs   (    0.88 ms per token,  1135.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16195.12 ms /    22 tokens (  736.14 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 96726.38 ms /   127 runs   (  761.63 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 113369.44 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.52 ms /   128 runs   (    0.87 ms per token,  1147.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12530.27 ms /    17 tokens (  737.07 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 95566.40 ms /   127 runs   (  752.49 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 108506.96 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.86 ms /   128 runs   (    0.87 ms per token,  1154.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19053.32 ms /    26 tokens (  732.82 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 95820.12 ms /   127 runs   (  754.49 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 115296.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.30 ms /   128 runs   (    0.87 ms per token,  1150.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43028.77 ms /   326 tokens (  131.99 ms per token,     7.58 tokens per second)\n",
      "llama_print_timings:        eval time = 97389.20 ms /   127 runs   (  766.84 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 140849.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.97 ms /   128 runs   (    0.88 ms per token,  1133.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 109636.31 ms /   669 tokens (  163.88 ms per token,     6.10 tokens per second)\n",
      "llama_print_timings:        eval time = 98805.31 ms /   127 runs   (  777.99 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 208873.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    71.01 ms /    82 runs   (    0.87 ms per token,  1154.80 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12569.77 ms /   103 tokens (  122.04 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:        eval time = 61531.53 ms /    81 runs   (  759.65 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 74370.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.88 ms /     1 runs   (    0.88 ms per token,  1137.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 246723.04 ms /  1215 tokens (  203.06 ms per token,     4.92 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 246735.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   116.89 ms /   128 runs   (    0.91 ms per token,  1095.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 109683.89 ms /   656 tokens (  167.20 ms per token,     5.98 tokens per second)\n",
      "llama_print_timings:        eval time = 101779.37 ms /   127 runs   (  801.41 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 211911.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   115.96 ms /   128 runs   (    0.91 ms per token,  1103.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 225990.70 ms /  1123 tokens (  201.24 ms per token,     4.97 tokens per second)\n",
      "llama_print_timings:        eval time = 105951.50 ms /   127 runs   (  834.26 ms per token,     1.20 tokens per second)\n",
      "llama_print_timings:       total time = 332404.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     4.38 ms /     5 runs   (    0.88 ms per token,  1142.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 542563.15 ms /  2029 tokens (  267.40 ms per token,     3.74 tokens per second)\n",
      "llama_print_timings:        eval time =  5643.92 ms /     4 runs   ( 1410.98 ms per token,     0.71 tokens per second)\n",
      "llama_print_timings:       total time = 548234.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     2.79 ms /     3 runs   (    0.93 ms per token,  1074.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 355312.85 ms /  1502 tokens (  236.56 ms per token,     4.23 tokens per second)\n",
      "llama_print_timings:        eval time =  4760.44 ms /     2 runs   ( 2380.22 ms per token,     0.42 tokens per second)\n",
      "llama_print_timings:       total time = 360094.70 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   117.78 ms /   128 runs   (    0.92 ms per token,  1086.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12956.38 ms /    17 tokens (  762.14 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:        eval time = 98588.16 ms /   127 runs   (  776.28 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 111998.61 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.74 ms /   128 runs   (    0.87 ms per token,  1155.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5886.15 ms /     8 tokens (  735.77 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 95265.55 ms /   127 runs   (  750.12 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 101572.35 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.20 ms /   128 runs   (    0.87 ms per token,  1151.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10438.82 ms /    14 tokens (  745.63 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:        eval time = 95231.90 ms /   127 runs   (  749.86 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 106092.64 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.61 ms /   128 runs   (    0.86 ms per token,  1157.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17536.67 ms /    23 tokens (  762.46 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:        eval time = 95256.52 ms /   127 runs   (  750.05 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 113223.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    86.67 ms /   100 runs   (    0.87 ms per token,  1153.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24116.97 ms /   196 tokens (  123.05 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:        eval time = 75200.99 ms /    99 runs   (  759.61 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 99648.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.86 ms /     1 runs   (    0.86 ms per token,  1156.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 384862.32 ms /  1613 tokens (  238.60 ms per token,     4.19 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 384876.55 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.97 ms /   128 runs   (    0.89 ms per token,  1123.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21259.85 ms /    28 tokens (  759.28 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:        eval time = 96424.99 ms /   127 runs   (  759.25 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 118123.41 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.96 ms /   128 runs   (    0.87 ms per token,  1153.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17824.30 ms /    24 tokens (  742.68 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 95320.46 ms /   127 runs   (  750.55 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 113566.69 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.91 ms /   128 runs   (    0.87 ms per token,  1154.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10930.95 ms /    15 tokens (  728.73 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 96400.34 ms /   127 runs   (  759.06 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 107760.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.88 ms /     1 runs   (    0.88 ms per token,  1137.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 444143.45 ms /  1811 tokens (  245.25 ms per token,     4.08 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 444159.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     2.68 ms /     3 runs   (    0.89 ms per token,  1119.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 459648.34 ms /  1827 tokens (  251.59 ms per token,     3.97 tokens per second)\n",
      "llama_print_timings:        eval time =  2951.03 ms /     2 runs   ( 1475.52 ms per token,     0.68 tokens per second)\n",
      "llama_print_timings:       total time = 462621.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    98.80 ms /   108 runs   (    0.91 ms per token,  1093.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 252029.74 ms /  1211 tokens (  208.12 ms per token,     4.80 tokens per second)\n",
      "llama_print_timings:        eval time = 89611.60 ms /   107 runs   (  837.49 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time = 342033.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     1 runs   (    0.94 ms per token,  1059.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 223040.96 ms /  1045 tokens (  213.44 ms per token,     4.69 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 223059.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     1.76 ms /     2 runs   (    0.88 ms per token,  1139.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 480742.93 ms /  1881 tokens (  255.58 ms per token,     3.91 tokens per second)\n",
      "llama_print_timings:        eval time =  2209.21 ms /     1 runs   ( 2209.21 ms per token,     0.45 tokens per second)\n",
      "llama_print_timings:       total time = 482972.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    98.52 ms /   107 runs   (    0.92 ms per token,  1086.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30252.06 ms /   239 tokens (  126.58 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time = 84236.33 ms /   106 runs   (  794.68 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 114871.23 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.66 ms /   128 runs   (    0.86 ms per token,  1156.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8228.66 ms /    11 tokens (  748.06 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:        eval time = 95257.45 ms /   127 runs   (  750.06 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 103909.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    75.25 ms /    87 runs   (    0.86 ms per token,  1156.21 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11732.83 ms /    98 tokens (  119.72 ms per token,     8.35 tokens per second)\n",
      "llama_print_timings:        eval time = 65187.47 ms /    86 runs   (  757.99 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 77208.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.26 ms /   128 runs   (    0.87 ms per token,  1150.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12868.05 ms /   109 tokens (  118.06 ms per token,     8.47 tokens per second)\n",
      "llama_print_timings:        eval time = 98029.16 ms /   127 runs   (  771.88 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 111341.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.93 ms /   128 runs   (    0.87 ms per token,  1143.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67340.00 ms /   452 tokens (  148.98 ms per token,     6.71 tokens per second)\n",
      "llama_print_timings:        eval time = 98353.05 ms /   127 runs   (  774.43 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 166126.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.79 ms /   128 runs   (    0.87 ms per token,  1155.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22103.75 ms /   182 tokens (  121.45 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:        eval time = 96806.32 ms /   127 runs   (  762.25 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 119349.41 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    89.22 ms /   101 runs   (    0.88 ms per token,  1132.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6768.41 ms /     9 tokens (  752.05 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:        eval time = 76087.64 ms /   100 runs   (  760.88 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 83204.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.87 ms /     1 runs   (    0.87 ms per token,  1149.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 339723.27 ms /  1506 tokens (  225.58 ms per token,     4.43 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 339735.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.86 ms /     1 runs   (    0.86 ms per token,  1160.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 400441.18 ms /  1600 tokens (  250.28 ms per token,     4.00 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 400461.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     0.85 ms /     1 runs   (    0.85 ms per token,  1169.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 496665.13 ms /  1931 tokens (  257.21 ms per token,     3.89 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 496674.47 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   119.19 ms /   128 runs   (    0.93 ms per token,  1073.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9121.43 ms /    12 tokens (  760.12 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:        eval time = 101160.92 ms /   127 runs   (  796.54 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 110726.35 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    41.39 ms /    48 runs   (    0.86 ms per token,  1159.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9467.75 ms /    13 tokens (  728.29 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 34525.97 ms /    47 runs   (  734.60 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 44140.57 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.03 ms /   128 runs   (    0.86 ms per token,  1163.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10911.89 ms /    15 tokens (  727.46 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93671.31 ms /   127 runs   (  737.57 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104973.33 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.20 ms /   128 runs   (    0.86 ms per token,  1161.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7268.22 ms /    10 tokens (  726.82 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 94074.03 ms /   127 runs   (  740.74 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 101735.59 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.33 ms /   128 runs   (    0.86 ms per token,  1160.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6541.20 ms /     9 tokens (  726.80 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93502.56 ms /   127 runs   (  736.24 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 100435.58 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.99 ms /   128 runs   (    0.86 ms per token,  1163.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8003.77 ms /    11 tokens (  727.62 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93524.28 ms /   127 runs   (  736.41 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 101921.53 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12734.84 ms /    17 tokens (  749.11 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:        eval time = 93577.34 ms /   127 runs   (  736.83 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106705.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.23 ms /   128 runs   (    0.86 ms per token,  1161.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23206.34 ms /   198 tokens (  117.20 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:        eval time = 94841.37 ms /   127 runs   (  746.78 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 118452.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    93.81 ms /   109 runs   (    0.86 ms per token,  1161.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29194.58 ms /   244 tokens (  119.65 ms per token,     8.36 tokens per second)\n",
      "llama_print_timings:        eval time = 80953.95 ms /   108 runs   (  749.57 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 110486.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.49 ms /   128 runs   (    0.86 ms per token,  1158.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52417.21 ms /   383 tokens (  136.86 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 95689.07 ms /   127 runs   (  753.46 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 148510.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    27.64 ms /    32 runs   (    0.86 ms per token,  1157.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40575.42 ms /   315 tokens (  128.81 ms per token,     7.76 tokens per second)\n",
      "llama_print_timings:        eval time = 23270.60 ms /    31 runs   (  750.66 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 63944.12 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.44 ms /   128 runs   (    0.86 ms per token,  1158.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5223.38 ms /    38 tokens (  137.46 ms per token,     7.27 tokens per second)\n",
      "llama_print_timings:        eval time = 93798.11 ms /   127 runs   (  738.57 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 99416.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   116.90 ms /   128 runs   (    0.91 ms per token,  1094.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 241933.24 ms /  1223 tokens (  197.82 ms per token,     5.06 tokens per second)\n",
      "llama_print_timings:        eval time = 101570.44 ms /   127 runs   (  799.77 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 343936.88 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.52 ms /   128 runs   (    0.86 ms per token,  1158.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10188.41 ms /    14 tokens (  727.74 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93603.12 ms /   127 runs   (  737.03 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104187.98 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.81 ms /   128 runs   (    0.87 ms per token,  1155.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17616.44 ms /    24 tokens (  734.02 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93661.26 ms /   127 runs   (  737.49 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111673.48 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.59 ms /   128 runs   (    0.86 ms per token,  1157.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13900.74 ms /    19 tokens (  731.62 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93631.26 ms /   127 runs   (  737.25 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107927.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =     2.68 ms /     3 runs   (    0.89 ms per token,  1118.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 350254.61 ms /  1547 tokens (  226.41 ms per token,     4.42 tokens per second)\n",
      "llama_print_timings:        eval time =  1642.63 ms /     2 runs   (  821.31 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 351913.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   117.52 ms /   128 runs   (    0.92 ms per token,  1089.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 205626.81 ms /  1064 tokens (  193.26 ms per token,     5.17 tokens per second)\n",
      "llama_print_timings:        eval time = 101654.91 ms /   127 runs   (  800.43 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 307721.28 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.45 ms /   128 runs   (    0.86 ms per token,  1158.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16124.75 ms /    22 tokens (  732.94 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93572.35 ms /   127 runs   (  736.79 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 110092.73 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.70 ms /   128 runs   (    0.86 ms per token,  1156.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16877.62 ms /    23 tokens (  733.81 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93668.20 ms /   127 runs   (  737.54 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 110941.81 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    60.19 ms /    70 runs   (    0.86 ms per token,  1163.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11640.87 ms /    16 tokens (  727.55 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 50716.69 ms /    69 runs   (  735.02 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 62569.52 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.29 ms /   128 runs   (    0.86 ms per token,  1160.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17623.87 ms /    24 tokens (  734.33 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93663.06 ms /   127 runs   (  737.50 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111678.38 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.13 ms /   128 runs   (    0.86 ms per token,  1162.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20378.25 ms /    28 tokens (  727.79 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93782.84 ms /   127 runs   (  738.45 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 114552.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   114.28 ms /   128 runs   (    0.89 ms per token,  1120.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 117405.31 ms /   727 tokens (  161.49 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:        eval time = 98242.83 ms /   127 runs   (  773.57 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 216067.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   116.87 ms /   128 runs   (    0.91 ms per token,  1095.26 tokens per second)\n",
      "llama_print_timings: prompt eval time = 279466.87 ms /  1348 tokens (  207.32 ms per token,     4.82 tokens per second)\n",
      "llama_print_timings:        eval time = 102309.14 ms /   127 runs   (  805.58 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 382206.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.14 ms /   128 runs   (    0.86 ms per token,  1162.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32321.22 ms /   262 tokens (  123.36 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:        eval time = 95173.54 ms /   127 runs   (  749.40 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 127893.24 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.25 ms /   128 runs   (    0.87 ms per token,  1150.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15381.12 ms /    21 tokens (  732.43 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 97066.36 ms /   127 runs   (  764.30 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 112848.17 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.43 ms /   128 runs   (    0.86 ms per token,  1159.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4935.11 ms /    33 tokens (  149.55 ms per token,     6.69 tokens per second)\n",
      "llama_print_timings:        eval time = 93749.48 ms /   127 runs   (  738.18 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 99075.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.64 ms /   128 runs   (    0.88 ms per token,  1136.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 69149.20 ms /   470 tokens (  147.13 ms per token,     6.80 tokens per second)\n",
      "llama_print_timings:        eval time = 96421.37 ms /   127 runs   (  759.22 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 165982.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    50.36 ms /    57 runs   (    0.88 ms per token,  1131.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34550.70 ms /   277 tokens (  124.73 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:        eval time = 48205.26 ms /    56 runs   (  860.81 ms per token,     1.16 tokens per second)\n",
      "llama_print_timings:       total time = 82939.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    93.59 ms /   102 runs   (    0.92 ms per token,  1089.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53909.94 ms /   372 tokens (  144.92 ms per token,     6.90 tokens per second)\n",
      "llama_print_timings:        eval time = 84352.40 ms /   101 runs   (  835.17 ms per token,     1.20 tokens per second)\n",
      "llama_print_timings:       total time = 138605.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    87.26 ms /   100 runs   (    0.87 ms per token,  1145.97 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44444.41 ms /   325 tokens (  136.75 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 79582.91 ms /    99 runs   (  803.87 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 124349.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   104.78 ms /   121 runs   (    0.87 ms per token,  1154.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42972.78 ms /   322 tokens (  133.46 ms per token,     7.49 tokens per second)\n",
      "llama_print_timings:        eval time = 90671.62 ms /   120 runs   (  755.60 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 134025.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   118.60 ms /   128 runs   (    0.93 ms per token,  1079.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 112484.72 ms /   688 tokens (  163.50 ms per token,     6.12 tokens per second)\n",
      "llama_print_timings:        eval time = 100840.61 ms /   127 runs   (  794.02 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 213768.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    20.84 ms /    24 runs   (    0.87 ms per token,  1151.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17014.30 ms /   145 tokens (  117.34 ms per token,     8.52 tokens per second)\n",
      "llama_print_timings:        eval time = 17185.22 ms /    23 runs   (  747.18 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 34275.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    76.78 ms /    89 runs   (    0.86 ms per token,  1159.13 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27728.41 ms /   226 tokens (  122.69 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:        eval time = 65942.02 ms /    88 runs   (  749.34 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 93952.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.51 ms /   128 runs   (    0.86 ms per token,  1158.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31629.71 ms /   255 tokens (  124.04 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:        eval time = 95133.62 ms /   127 runs   (  749.08 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 127162.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   100.02 ms /   116 runs   (    0.86 ms per token,  1159.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32329.35 ms /   260 tokens (  124.34 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:        eval time = 86192.95 ms /   115 runs   (  749.50 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 118883.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.76 ms /   128 runs   (    0.87 ms per token,  1145.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68901.53 ms /   465 tokens (  148.18 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time = 96410.30 ms /   127 runs   (  759.14 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 165725.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    71.46 ms /    83 runs   (    0.86 ms per token,  1161.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30666.49 ms /   249 tokens (  123.16 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:        eval time = 61375.43 ms /    82 runs   (  748.48 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 92298.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.00 ms /   128 runs   (    0.87 ms per token,  1142.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68962.35 ms /   458 tokens (  150.57 ms per token,     6.64 tokens per second)\n",
      "llama_print_timings:        eval time = 98405.97 ms /   127 runs   (  774.85 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 167788.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    76.53 ms /    89 runs   (    0.86 ms per token,  1162.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16827.09 ms /   149 tokens (  112.93 ms per token,     8.85 tokens per second)\n",
      "llama_print_timings:        eval time = 65545.06 ms /    88 runs   (  744.83 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 82655.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    77.42 ms /    90 runs   (    0.86 ms per token,  1162.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 39537.99 ms /   307 tokens (  128.79 ms per token,     7.76 tokens per second)\n",
      "llama_print_timings:        eval time = 66772.65 ms /    89 runs   (  750.25 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 106596.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    73.93 ms /    86 runs   (    0.86 ms per token,  1163.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30831.56 ms /   251 tokens (  122.83 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:        eval time = 63590.37 ms /    85 runs   (  748.12 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 94693.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    98.05 ms /   114 runs   (    0.86 ms per token,  1162.65 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21751.47 ms /   188 tokens (  115.70 ms per token,     8.64 tokens per second)\n",
      "llama_print_timings:        eval time = 84321.77 ms /   113 runs   (  746.21 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 106429.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   112.48 ms /   128 runs   (    0.88 ms per token,  1137.98 tokens per second)\n",
      "llama_print_timings: prompt eval time = 77779.45 ms /   510 tokens (  152.51 ms per token,     6.56 tokens per second)\n",
      "llama_print_timings:        eval time = 96822.91 ms /   127 runs   (  762.39 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 175015.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.18 ms /   128 runs   (    0.86 ms per token,  1161.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26611.87 ms /   223 tokens (  119.34 ms per token,     8.38 tokens per second)\n",
      "llama_print_timings:        eval time = 96208.51 ms /   127 runs   (  757.55 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 123222.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.22 ms /   128 runs   (    0.88 ms per token,  1130.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 63882.90 ms /   433 tokens (  147.54 ms per token,     6.78 tokens per second)\n",
      "llama_print_timings:        eval time = 97290.15 ms /   127 runs   (  766.06 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 161589.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    14.73 ms /    17 runs   (    0.87 ms per token,  1153.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44663.40 ms /   334 tokens (  133.72 ms per token,     7.48 tokens per second)\n",
      "llama_print_timings:        eval time = 12082.54 ms /    16 runs   (  755.16 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 56799.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   108.49 ms /   125 runs   (    0.87 ms per token,  1152.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11916.12 ms /   102 tokens (  116.82 ms per token,     8.56 tokens per second)\n",
      "llama_print_timings:        eval time = 92513.97 ms /   124 runs   (  746.08 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 104829.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   102.03 ms /   118 runs   (    0.86 ms per token,  1156.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16482.11 ms /   142 tokens (  116.07 ms per token,     8.62 tokens per second)\n",
      "llama_print_timings:        eval time = 87410.03 ms /   117 runs   (  747.09 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 104265.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    52.59 ms /    61 runs   (    0.86 ms per token,  1159.85 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23738.15 ms /   197 tokens (  120.50 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:        eval time = 44964.78 ms /    60 runs   (  749.41 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 68894.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.22 ms /   128 runs   (    0.87 ms per token,  1150.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51015.79 ms /   369 tokens (  138.25 ms per token,     7.23 tokens per second)\n",
      "llama_print_timings:        eval time = 103990.36 ms /   127 runs   (  818.82 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time = 155422.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   102.03 ms /   117 runs   (    0.87 ms per token,  1146.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30763.71 ms /   251 tokens (  122.56 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:        eval time = 89802.57 ms /   116 runs   (  774.16 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 120950.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    44.75 ms /    52 runs   (    0.86 ms per token,  1161.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16750.48 ms /   145 tokens (  115.52 ms per token,     8.66 tokens per second)\n",
      "llama_print_timings:        eval time = 37967.00 ms /    51 runs   (  744.45 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 54883.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.35 ms /   128 runs   (    0.86 ms per token,  1159.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27177.94 ms /   223 tokens (  121.87 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:        eval time = 95192.96 ms /   127 runs   (  749.55 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 122782.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.34 ms /   128 runs   (    0.86 ms per token,  1160.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35798.36 ms /   282 tokens (  126.94 ms per token,     7.88 tokens per second)\n",
      "llama_print_timings:        eval time = 95353.53 ms /   127 runs   (  750.82 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 131557.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34030.79 ms /   270 tokens (  126.04 ms per token,     7.93 tokens per second)\n",
      "llama_print_timings:        eval time = 95239.83 ms /   127 runs   (  749.92 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 129673.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   117.03 ms /   128 runs   (    0.91 ms per token,  1093.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 108025.32 ms /   670 tokens (  161.23 ms per token,     6.20 tokens per second)\n",
      "llama_print_timings:        eval time = 102049.53 ms /   127 runs   (  803.54 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 210515.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12143.95 ms /   106 tokens (  114.57 ms per token,     8.73 tokens per second)\n",
      "llama_print_timings:        eval time = 94631.38 ms /   127 runs   (  745.13 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 107180.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.10 ms /   128 runs   (    0.86 ms per token,  1162.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14566.53 ms /   129 tokens (  112.92 ms per token,     8.86 tokens per second)\n",
      "llama_print_timings:        eval time = 94689.95 ms /   127 runs   (  745.59 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 109658.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    74.80 ms /    87 runs   (    0.86 ms per token,  1163.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12896.64 ms /   114 tokens (  113.13 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:        eval time = 64192.64 ms /    86 runs   (  746.43 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 77360.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.48 ms /   128 runs   (    0.86 ms per token,  1158.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27784.87 ms /   232 tokens (  119.76 ms per token,     8.35 tokens per second)\n",
      "llama_print_timings:        eval time = 95045.44 ms /   127 runs   (  748.39 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 123233.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.99 ms /   128 runs   (    0.86 ms per token,  1163.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32699.63 ms /   264 tokens (  123.86 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:        eval time = 95176.23 ms /   127 runs   (  749.42 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 128278.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    26.73 ms /    31 runs   (    0.86 ms per token,  1159.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25889.23 ms /   219 tokens (  118.22 ms per token,     8.46 tokens per second)\n",
      "llama_print_timings:        eval time = 22391.28 ms /    30 runs   (  746.38 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 48377.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.14 ms /   128 runs   (    0.86 ms per token,  1162.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44850.92 ms /   341 tokens (  131.53 ms per token,     7.60 tokens per second)\n",
      "llama_print_timings:        eval time = 95605.35 ms /   127 runs   (  752.80 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 140863.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   113.08 ms /   128 runs   (    0.88 ms per token,  1131.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 85615.37 ms /   522 tokens (  164.01 ms per token,     6.10 tokens per second)\n",
      "llama_print_timings:        eval time = 101009.80 ms /   127 runs   (  795.35 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 187054.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.17 ms /   128 runs   (    0.86 ms per token,  1161.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19172.62 ms /   156 tokens (  122.90 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:        eval time = 94690.60 ms /   127 runs   (  745.60 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 114276.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    68.08 ms /    79 runs   (    0.86 ms per token,  1160.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 33169.98 ms /   269 tokens (  123.31 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:        eval time = 58520.65 ms /    78 runs   (  750.26 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 91941.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.04 ms /   128 runs   (    0.86 ms per token,  1163.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18841.13 ms /   165 tokens (  114.19 ms per token,     8.76 tokens per second)\n",
      "llama_print_timings:        eval time = 94761.14 ms /   127 runs   (  746.15 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 114002.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20002.17 ms /   173 tokens (  115.62 ms per token,     8.65 tokens per second)\n",
      "llama_print_timings:        eval time = 95586.46 ms /   127 runs   (  752.65 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 115992.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.18 ms /   128 runs   (    0.86 ms per token,  1161.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40651.21 ms /   313 tokens (  129.88 ms per token,     7.70 tokens per second)\n",
      "llama_print_timings:        eval time = 95410.37 ms /   127 runs   (  751.26 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 136469.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.95 ms /   128 runs   (    0.87 ms per token,  1153.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15032.38 ms /   128 tokens (  117.44 ms per token,     8.51 tokens per second)\n",
      "llama_print_timings:        eval time = 96129.46 ms /   127 runs   (  756.92 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 111572.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   104.01 ms /   121 runs   (    0.86 ms per token,  1163.33 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19979.66 ms /   174 tokens (  114.83 ms per token,     8.71 tokens per second)\n",
      "llama_print_timings:        eval time = 89576.58 ms /   120 runs   (  746.47 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 109936.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31433.07 ms /   257 tokens (  122.31 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:        eval time = 95118.53 ms /   127 runs   (  748.96 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 126955.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.06 ms /   128 runs   (    0.86 ms per token,  1163.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16236.68 ms /   144 tokens (  112.75 ms per token,     8.87 tokens per second)\n",
      "llama_print_timings:        eval time = 94602.27 ms /   127 runs   (  744.90 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 111241.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.90 ms /   128 runs   (    0.86 ms per token,  1164.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27237.34 ms /   229 tokens (  118.94 ms per token,     8.41 tokens per second)\n",
      "llama_print_timings:        eval time = 95019.68 ms /   127 runs   (  748.19 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 122659.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.33 ms /   128 runs   (    0.86 ms per token,  1160.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16114.20 ms /   142 tokens (  113.48 ms per token,     8.81 tokens per second)\n",
      "llama_print_timings:        eval time = 94726.01 ms /   127 runs   (  745.87 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 111243.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   101.37 ms /   118 runs   (    0.86 ms per token,  1164.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26025.74 ms /   201 tokens (  129.48 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 88491.93 ms /   117 runs   (  756.34 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 114898.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.99 ms /   128 runs   (    0.86 ms per token,  1163.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 39027.82 ms /   303 tokens (  128.80 ms per token,     7.76 tokens per second)\n",
      "llama_print_timings:        eval time = 95364.54 ms /   127 runs   (  750.90 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 134802.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    76.50 ms /    89 runs   (    0.86 ms per token,  1163.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24310.56 ms /   207 tokens (  117.44 ms per token,     8.51 tokens per second)\n",
      "llama_print_timings:        eval time = 66164.01 ms /    88 runs   (  751.86 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 90753.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.00 ms /   128 runs   (    0.86 ms per token,  1163.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49183.03 ms /   365 tokens (  134.75 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:        eval time = 95696.87 ms /   127 runs   (  753.52 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 145287.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.85 ms /   128 runs   (    0.86 ms per token,  1165.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17444.45 ms /   154 tokens (  113.28 ms per token,     8.83 tokens per second)\n",
      "llama_print_timings:        eval time = 94607.87 ms /   127 runs   (  744.94 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 112458.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.96 ms /   128 runs   (    0.86 ms per token,  1164.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 36939.14 ms /   294 tokens (  125.64 ms per token,     7.96 tokens per second)\n",
      "llama_print_timings:        eval time = 95300.49 ms /   127 runs   (  750.40 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 132642.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.96 ms /   128 runs   (    0.86 ms per token,  1164.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14048.27 ms /   125 tokens (  112.39 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:        eval time = 94451.94 ms /   127 runs   (  743.72 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 108903.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.44 ms /   128 runs   (    0.86 ms per token,  1159.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18564.98 ms /   163 tokens (  113.90 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:        eval time = 94726.06 ms /   127 runs   (  745.87 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 113694.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.93 ms /   128 runs   (    0.86 ms per token,  1164.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16445.64 ms /   145 tokens (  113.42 ms per token,     8.82 tokens per second)\n",
      "llama_print_timings:        eval time = 94606.37 ms /   127 runs   (  744.93 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 111455.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.20 ms /   128 runs   (    0.87 ms per token,  1151.08 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14797.72 ms /   131 tokens (  112.96 ms per token,     8.85 tokens per second)\n",
      "llama_print_timings:        eval time = 101860.72 ms /   127 runs   (  802.05 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 117070.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   111.13 ms /   128 runs   (    0.87 ms per token,  1151.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 37077.09 ms /   281 tokens (  131.95 ms per token,     7.58 tokens per second)\n",
      "llama_print_timings:        eval time = 100627.49 ms /   127 runs   (  792.34 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 138127.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.70 ms /   128 runs   (    0.86 ms per token,  1156.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15948.70 ms /   139 tokens (  114.74 ms per token,     8.72 tokens per second)\n",
      "llama_print_timings:        eval time = 97107.10 ms /   127 runs   (  764.62 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 113472.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.91 ms /   128 runs   (    0.87 ms per token,  1154.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48878.16 ms /   347 tokens (  140.86 ms per token,     7.10 tokens per second)\n",
      "llama_print_timings:        eval time = 99673.43 ms /   127 runs   (  784.83 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 148968.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.16 ms /   128 runs   (    0.86 ms per token,  1161.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14999.91 ms /   130 tokens (  115.38 ms per token,     8.67 tokens per second)\n",
      "llama_print_timings:        eval time = 94515.83 ms /   127 runs   (  744.22 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 109921.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    52.45 ms /    61 runs   (    0.86 ms per token,  1163.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26212.33 ms /   217 tokens (  120.79 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:        eval time = 44807.84 ms /    60 runs   (  746.80 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 71211.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.15 ms /   128 runs   (    0.86 ms per token,  1162.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23329.50 ms /   198 tokens (  117.83 ms per token,     8.49 tokens per second)\n",
      "llama_print_timings:        eval time = 94896.51 ms /   127 runs   (  747.22 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 118634.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.06 ms /   128 runs   (    0.86 ms per token,  1163.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28529.03 ms /   236 tokens (  120.89 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:        eval time = 95601.30 ms /   127 runs   (  752.77 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 124537.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.96 ms /   128 runs   (    0.86 ms per token,  1164.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42609.90 ms /   323 tokens (  131.92 ms per token,     7.58 tokens per second)\n",
      "llama_print_timings:        eval time = 95416.90 ms /   127 runs   (  751.31 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 138430.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    21.40 ms /    25 runs   (    0.86 ms per token,  1168.33 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23439.01 ms /   200 tokens (  117.20 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:        eval time = 17877.31 ms /    24 runs   (  744.89 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 41394.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   110.00 ms /   128 runs   (    0.86 ms per token,  1163.62 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17564.08 ms /   153 tokens (  114.80 ms per token,     8.71 tokens per second)\n",
      "llama_print_timings:        eval time = 94747.35 ms /   127 runs   (  746.04 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 112715.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.83 ms /   128 runs   (    0.86 ms per token,  1165.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17586.01 ms /   154 tokens (  114.19 ms per token,     8.76 tokens per second)\n",
      "llama_print_timings:        eval time = 94610.23 ms /   127 runs   (  744.96 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 112599.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.98 ms /   128 runs   (    0.86 ms per token,  1163.85 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20641.93 ms /   178 tokens (  115.97 ms per token,     8.62 tokens per second)\n",
      "llama_print_timings:        eval time = 94734.84 ms /   127 runs   (  745.94 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 115779.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =    94.34 ms /   109 runs   (    0.87 ms per token,  1155.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15420.68 ms /   135 tokens (  114.23 ms per token,     8.75 tokens per second)\n",
      "llama_print_timings:        eval time = 83806.45 ms /   108 runs   (  775.99 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 99574.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 47539.25 ms\n",
      "llama_print_timings:      sample time =   109.89 ms /   128 runs   (    0.86 ms per token,  1164.80 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15047.70 ms /   129 tokens (  116.65 ms per token,     8.57 tokens per second)\n",
      "llama_print_timings:        eval time = 94669.65 ms /   127 runs   (  745.43 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 110126.45 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "responses = []\n",
    "i = 0\n",
    "for question in df_test['Question Text'].values.tolist():\n",
    "    df_search_results = search_content(query=question, df_sentances=df_booklet, index=fastIndex, embedder=embedding_model, k=5)\n",
    "    response = get_response(text=question, llm=llm_model, df_matches=df_search_results)\n",
    "    response[\"keywords\"] = extract_keyword(str(df_search_results['text'].values), top_n=5)\n",
    "    responses.append(response)\n",
    "    i += 1\n",
    "\n",
    "df_responses = pd.DataFrame(responses)\n",
    "df_responses['ID'] = df_test['ID']\n",
    "df_responses['Question'] = df_test['Question Text']\n",
    "# df_responses[[\"Question\", \"question_answer\", \"reference_document\", \"paragraph(s)_number\", \"keywords\", \"ID\"]].to_csv(pwd + \"/data/data/answers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submissoion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.columns = ['question_answer', 'reference_document', 'paragraph(s)_number', 'keywords', 'ID', 'Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses[\"question_answer\"] = [\"Could not retrieve answer\" if answer == \"\" else answer for answer in df_responses[\"question_answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.melt(df_responses, id_vars=['ID'], value_vars=['question_answer', 'reference_document', 'paragraph(s)_number', \"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission['ID'] = df_submission['ID'] + '_' + df_submission['variable']\n",
    "df_submission.columns = [\"ID\", \"variable\", \"Target\"]\n",
    "df_submission = df_submission[['ID', \"Target\"]].set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(pwd + \"/data/submissions/submission_v2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.1 ('zindi_llm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1b94373ed21143aa54ae29a501b4c41cca272fcc00b21ffb9f53282b803de8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
