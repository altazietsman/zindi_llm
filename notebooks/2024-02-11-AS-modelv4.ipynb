{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/zindi_llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import torch\n",
    "from utils.embeddings import Embedder\n",
    "from utils.preprocess import create_sentance_booklet, create_faise_index\n",
    "import faiss\n",
    "from utils.utils import search_content, read_booklets, retrieve_booklet_text, clean_text\n",
    "from models.llama import llama_cpp\n",
    "from utils.response_generator import get_response, extract_keyword, get_paragraph_info\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silly Mac that forces me to change the environmental variable to prevent issues running transformers\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd  = str(pathlib.Path().cwd().parent.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet = read_booklets((pwd + \"/data/data/booklets/\"))\n",
    "df_train = pd.read_csv(pwd + \"/data/data/Train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Clean some of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet['cleanText'] = df_booklet['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet['numWords'] = [len(text.split(\" \")) for text in df_booklet['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = \" \".join(df_booklet['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_text(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"text\":all_splits}).to_csv(pwd + \"/data/data/resources/docs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There paragraphs are long, so we might need to consider spliting text on sentences to make them shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem like the following steps will have to be taken:\n",
    "\n",
    "- embed booklet\n",
    "- embed search phrase\n",
    "- use search phrase embedding to search for relevant text in booklet\n",
    "- retrive all relevant text from booklet\n",
    "- format search phrase and into prompt for LLM\n",
    "- Send promt to LLM and return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try simple model\n",
    "I will first use all-mpnet-base-v2  as the sentance embedder and then I will use Llama as the LLM .\n",
    "\n",
    "- Download: `wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_1.bin`\n",
    "- Then run: pip install llama-cpp-python==0.1.78\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Embed all sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Embedder(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not create embeddings uncomment below\n",
    "# doc_embeddings = embedding_model.embed(all_splits)\n",
    "# np.save(file=(pwd + \"/data/data/resources/doc_embeddings\" ), arr=doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index train answers\n",
    "# train_embeddings = embedding_model.embed(df_train['Question Answer'].values)\n",
    "# np.save(file=(pwd + \"/data/data/resources/embeddings_train\" ), arr=train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not create embeddings uncomment below\n",
    "# booklet_embeddings = embedding_model.embed(df_booklet_clean['cleanText'].values)\n",
    "# np.save(file=(pwd + \"/data/data/resources/embeddings_cleanv3\" ), arr=booklet_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creat faiss index for search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not create the index uncomment below\n",
    "# fastIndex = create_faise_index(doc_embeddings)\n",
    "\n",
    "# # # Save the index\n",
    "# faiss.write_index(fastIndex, pwd + \"/data/data/resources/doc_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not create the index uncomment below\n",
    "# create index fro train\n",
    "# fastIndex_train = create_faise_index(train_embeddings)\n",
    "\n",
    "# # Save the index\n",
    "# faiss.write_index(fastIndex_train, pwd + \"/data/data/resources/train_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not create the index uncomment below\n",
    "# fastIndex = create_faise_index(booklet_embeddings)\n",
    "\n",
    "# # Save the index\n",
    "# faiss.write_index(fastIndex, pwd + \"/data/data/resources/paragraph_index_cleanv3.faiss\")\n",
    "# df_booklet_clean.to_csv(pwd + \"/data/data/resources/bookletv3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in index\n",
    "fastIndex_docs = faiss.read_index( pwd + \"/data/data/resources/doc_index.faiss\")\n",
    "fastIndex_train = faiss.read_index( pwd + \"/data/data/resources/train_index.faiss\")\n",
    "fastIndex_book = faiss.read_index( pwd + \"/data/data/resources/paragraph_index_cleanv3.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs = pd.read_csv(pwd + \"/data/data/resources/docs.csv\")\n",
    "df_booklet = pd.read_csv(pwd + \"/data/data/resources/bookletv3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Search embeddings and get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/altasaunders/Alta_projects/zindi_llm/llama-2-7b-chat.ggmlv3.q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 3 (mostly Q4_1)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4017.35 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   71.84 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm_model = llama_cpp(pwd + \"/llama-2-7b-chat.ggmlv3.q4_1.bin\", gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(pwd +  \"/data/data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(columns=['answer', 'book', 'paragraphs', 'keywords', 'ID', 'Question'])\n",
    "df_submission.to_csv(pwd + \"/data/submissions/submission_v4_temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    67.87 ms /    78 runs   (    0.87 ms per token,  1149.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 59224.04 ms /    78 runs   (  759.28 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 59460.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   112.65 ms /   128 runs   (    0.88 ms per token,  1136.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49738.27 ms /   361 tokens (  137.78 ms per token,     7.26 tokens per second)\n",
      "llama_print_timings:        eval time = 96344.98 ms /   127 runs   (  758.62 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 146480.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   111.10 ms /   128 runs   (    0.87 ms per token,  1152.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47168.26 ms /   352 tokens (  134.00 ms per token,     7.46 tokens per second)\n",
      "llama_print_timings:        eval time = 95820.34 ms /   127 runs   (  754.49 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 143378.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    85.82 ms /    99 runs   (    0.87 ms per token,  1153.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56351.74 ms /   401 tokens (  140.53 ms per token,     7.12 tokens per second)\n",
      "llama_print_timings:        eval time = 74634.74 ms /    98 runs   (  761.58 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 131285.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   102.89 ms /   118 runs   (    0.87 ms per token,  1146.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49601.58 ms /   363 tokens (  136.64 ms per token,     7.32 tokens per second)\n",
      "llama_print_timings:        eval time = 91170.48 ms /   117 runs   (  779.23 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 141134.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    37.73 ms /    43 runs   (    0.88 ms per token,  1139.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68156.79 ms /   457 tokens (  149.14 ms per token,     6.71 tokens per second)\n",
      "llama_print_timings:        eval time = 32801.41 ms /    42 runs   (  780.99 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 101089.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   114.14 ms /   128 runs   (    0.89 ms per token,  1121.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50456.16 ms /   362 tokens (  139.38 ms per token,     7.17 tokens per second)\n",
      "llama_print_timings:        eval time = 99646.14 ms /   127 runs   (  784.62 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 150505.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    85.64 ms /    99 runs   (    0.87 ms per token,  1155.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48880.21 ms /   362 tokens (  135.03 ms per token,     7.41 tokens per second)\n",
      "llama_print_timings:        eval time = 73211.56 ms /    98 runs   (  747.06 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 122391.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.53 ms /   128 runs   (    0.86 ms per token,  1158.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50888.10 ms /   371 tokens (  137.16 ms per token,     7.29 tokens per second)\n",
      "llama_print_timings:        eval time = 95118.05 ms /   127 runs   (  748.96 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 146395.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.55 ms /   128 runs   (    0.86 ms per token,  1157.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46149.92 ms /   348 tokens (  132.61 ms per token,     7.54 tokens per second)\n",
      "llama_print_timings:        eval time = 94757.32 ms /   127 runs   (  746.12 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141297.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.59 ms /   128 runs   (    0.86 ms per token,  1157.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46087.69 ms /   348 tokens (  132.44 ms per token,     7.55 tokens per second)\n",
      "llama_print_timings:        eval time = 94783.92 ms /   127 runs   (  746.33 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141262.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    94.98 ms /   110 runs   (    0.86 ms per token,  1158.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53754.15 ms /   391 tokens (  137.48 ms per token,     7.27 tokens per second)\n",
      "llama_print_timings:        eval time = 81627.18 ms /   109 runs   (  748.87 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 135714.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    88.00 ms /   101 runs   (    0.87 ms per token,  1147.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 55426.06 ms /   400 tokens (  138.57 ms per token,     7.22 tokens per second)\n",
      "llama_print_timings:        eval time = 77301.31 ms /   100 runs   (  773.01 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 133041.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   108.86 ms /   126 runs   (    0.86 ms per token,  1157.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52142.76 ms /   375 tokens (  139.05 ms per token,     7.19 tokens per second)\n",
      "llama_print_timings:        eval time = 93789.00 ms /   125 runs   (  750.31 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 146317.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    56.84 ms /    66 runs   (    0.86 ms per token,  1161.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61551.12 ms /   435 tokens (  141.50 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:        eval time = 48790.16 ms /    65 runs   (  750.62 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 110540.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   106.89 ms /   123 runs   (    0.87 ms per token,  1150.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50054.08 ms /   375 tokens (  133.48 ms per token,     7.49 tokens per second)\n",
      "llama_print_timings:        eval time = 91270.78 ms /   122 runs   (  748.12 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141699.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.34 ms /   128 runs   (    0.86 ms per token,  1160.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47806.31 ms /   363 tokens (  131.70 ms per token,     7.59 tokens per second)\n",
      "llama_print_timings:        eval time = 94896.54 ms /   127 runs   (  747.22 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143092.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44037.34 ms /   342 tokens (  128.76 ms per token,     7.77 tokens per second)\n",
      "llama_print_timings:        eval time = 96197.18 ms /   127 runs   (  757.46 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 140628.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    86.92 ms /   100 runs   (    0.87 ms per token,  1150.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54773.01 ms /   401 tokens (  136.59 ms per token,     7.32 tokens per second)\n",
      "llama_print_timings:        eval time = 74243.20 ms /    99 runs   (  749.93 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 129322.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   106.65 ms /   124 runs   (    0.86 ms per token,  1162.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50669.55 ms /   377 tokens (  134.40 ms per token,     7.44 tokens per second)\n",
      "llama_print_timings:        eval time = 91749.38 ms /   123 runs   (  745.93 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142796.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    97.33 ms /   113 runs   (    0.86 ms per token,  1160.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52162.58 ms /   387 tokens (  134.79 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:        eval time = 83906.82 ms /   112 runs   (  749.17 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 136413.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.80 ms /   128 runs   (    0.86 ms per token,  1165.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44098.11 ms /   342 tokens (  128.94 ms per token,     7.76 tokens per second)\n",
      "llama_print_timings:        eval time = 94672.29 ms /   127 runs   (  745.45 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139163.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.66 ms /   128 runs   (    0.86 ms per token,  1156.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46694.03 ms /   356 tokens (  131.16 ms per token,     7.62 tokens per second)\n",
      "llama_print_timings:        eval time = 96438.04 ms /   127 runs   (  759.35 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 143527.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    97.55 ms /   113 runs   (    0.86 ms per token,  1158.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46500.14 ms /   344 tokens (  135.17 ms per token,     7.40 tokens per second)\n",
      "llama_print_timings:        eval time = 85223.08 ms /   112 runs   (  760.92 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 132071.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.73 ms /   128 runs   (    0.86 ms per token,  1166.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49687.22 ms /   373 tokens (  133.21 ms per token,     7.51 tokens per second)\n",
      "llama_print_timings:        eval time = 95339.18 ms /   127 runs   (  750.70 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 145415.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.47 ms /   128 runs   (    0.86 ms per token,  1158.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 41619.76 ms /   328 tokens (  126.89 ms per token,     7.88 tokens per second)\n",
      "llama_print_timings:        eval time = 94780.60 ms /   127 runs   (  746.30 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 136796.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    34.44 ms /    40 runs   (    0.86 ms per token,  1161.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 66614.09 ms /   461 tokens (  144.50 ms per token,     6.92 tokens per second)\n",
      "llama_print_timings:        eval time = 29154.37 ms /    39 runs   (  747.55 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 95888.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    75.38 ms /    85 runs   (    0.89 ms per token,  1127.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 57709.96 ms /   416 tokens (  138.73 ms per token,     7.21 tokens per second)\n",
      "llama_print_timings:        eval time = 63604.04 ms /    84 runs   (  757.19 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 121581.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    60.26 ms /    70 runs   (    0.86 ms per token,  1161.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62218.52 ms /   431 tokens (  144.36 ms per token,     6.93 tokens per second)\n",
      "llama_print_timings:        eval time = 52669.83 ms /    69 runs   (  763.33 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 115102.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   111.57 ms /   128 runs   (    0.87 ms per token,  1147.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48361.55 ms /   359 tokens (  134.71 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:        eval time = 95434.76 ms /   127 runs   (  751.45 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 144196.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   104.41 ms /   121 runs   (    0.86 ms per token,  1158.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51764.07 ms /   380 tokens (  136.22 ms per token,     7.34 tokens per second)\n",
      "llama_print_timings:        eval time = 89565.26 ms /   120 runs   (  746.38 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141701.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    85.14 ms /    97 runs   (    0.88 ms per token,  1139.26 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56309.58 ms /   404 tokens (  139.38 ms per token,     7.17 tokens per second)\n",
      "llama_print_timings:        eval time = 72106.76 ms /    96 runs   (  751.11 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 128717.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    96.91 ms /   112 runs   (    0.87 ms per token,  1155.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47191.94 ms /   353 tokens (  133.69 ms per token,     7.48 tokens per second)\n",
      "llama_print_timings:        eval time = 82775.30 ms /   111 runs   (  745.72 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 130311.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.42 ms /   128 runs   (    0.86 ms per token,  1159.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49377.79 ms /   360 tokens (  137.16 ms per token,     7.29 tokens per second)\n",
      "llama_print_timings:        eval time = 95067.39 ms /   127 runs   (  748.56 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 144843.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   102.24 ms /   119 runs   (    0.86 ms per token,  1163.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48875.83 ms /   369 tokens (  132.45 ms per token,     7.55 tokens per second)\n",
      "llama_print_timings:        eval time = 88155.12 ms /   118 runs   (  747.08 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 137395.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.26 ms /   128 runs   (    0.86 ms per token,  1160.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42450.44 ms /   333 tokens (  127.48 ms per token,     7.84 tokens per second)\n",
      "llama_print_timings:        eval time = 95035.26 ms /   127 runs   (  748.31 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 137882.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   108.42 ms /   126 runs   (    0.86 ms per token,  1162.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49437.75 ms /   374 tokens (  132.19 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:        eval time = 93713.90 ms /   125 runs   (  749.71 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 143538.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   112.14 ms /   128 runs   (    0.88 ms per token,  1141.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47871.73 ms /   352 tokens (  136.00 ms per token,     7.35 tokens per second)\n",
      "llama_print_timings:        eval time = 99307.62 ms /   127 runs   (  781.95 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 147583.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   111.00 ms /   128 runs   (    0.87 ms per token,  1153.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47137.75 ms /   348 tokens (  135.45 ms per token,     7.38 tokens per second)\n",
      "llama_print_timings:        eval time = 98959.22 ms /   127 runs   (  779.21 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 146498.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.33 ms /   128 runs   (    0.86 ms per token,  1160.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49851.26 ms /   371 tokens (  134.37 ms per token,     7.44 tokens per second)\n",
      "llama_print_timings:        eval time = 94936.94 ms /   127 runs   (  747.53 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 145181.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.20 ms /   128 runs   (    0.86 ms per token,  1161.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44242.09 ms /   344 tokens (  128.61 ms per token,     7.78 tokens per second)\n",
      "llama_print_timings:        eval time = 94690.75 ms /   127 runs   (  745.60 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139328.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.95 ms /   128 runs   (    0.86 ms per token,  1164.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46344.53 ms /   356 tokens (  130.18 ms per token,     7.68 tokens per second)\n",
      "llama_print_timings:        eval time = 95423.59 ms /   127 runs   (  751.37 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 142161.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.86 ms /   128 runs   (    0.86 ms per token,  1165.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 41446.61 ms /   326 tokens (  127.14 ms per token,     7.87 tokens per second)\n",
      "llama_print_timings:        eval time = 94588.04 ms /   127 runs   (  744.79 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 136430.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    87.90 ms /   102 runs   (    0.86 ms per token,  1160.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54279.62 ms /   399 tokens (  136.04 ms per token,     7.35 tokens per second)\n",
      "llama_print_timings:        eval time = 75403.17 ms /   101 runs   (  746.57 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 129995.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.99 ms /   128 runs   (    0.86 ms per token,  1163.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46464.41 ms /   355 tokens (  130.89 ms per token,     7.64 tokens per second)\n",
      "llama_print_timings:        eval time = 94732.02 ms /   127 runs   (  745.92 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141590.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   111.03 ms /   128 runs   (    0.87 ms per token,  1152.85 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47894.43 ms /   350 tokens (  136.84 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 95734.69 ms /   127 runs   (  753.82 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 144028.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.04 ms /   128 runs   (    0.86 ms per token,  1163.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45958.02 ms /   353 tokens (  130.19 ms per token,     7.68 tokens per second)\n",
      "llama_print_timings:        eval time = 95192.45 ms /   127 runs   (  749.55 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 141546.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.00 ms /   128 runs   (    0.86 ms per token,  1163.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45387.23 ms /   350 tokens (  129.68 ms per token,     7.71 tokens per second)\n",
      "llama_print_timings:        eval time = 94728.89 ms /   127 runs   (  745.90 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140511.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.10 ms /   128 runs   (    0.86 ms per token,  1162.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43020.33 ms /   336 tokens (  128.04 ms per token,     7.81 tokens per second)\n",
      "llama_print_timings:        eval time = 94705.71 ms /   127 runs   (  745.71 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 138123.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.57 ms /   128 runs   (    0.86 ms per token,  1157.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44310.56 ms /   341 tokens (  129.94 ms per token,     7.70 tokens per second)\n",
      "llama_print_timings:        eval time = 96644.31 ms /   127 runs   (  760.98 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 141356.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    77.44 ms /    90 runs   (    0.86 ms per token,  1162.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56759.33 ms /   411 tokens (  138.10 ms per token,     7.24 tokens per second)\n",
      "llama_print_timings:        eval time = 66555.41 ms /    89 runs   (  747.81 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 123590.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    88.98 ms /   103 runs   (    0.86 ms per token,  1157.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53938.05 ms /   397 tokens (  135.86 ms per token,     7.36 tokens per second)\n",
      "llama_print_timings:        eval time = 77029.77 ms /   102 runs   (  755.19 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 131284.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    97.06 ms /   113 runs   (    0.86 ms per token,  1164.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52299.55 ms /   388 tokens (  134.79 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:        eval time = 83889.17 ms /   112 runs   (  749.01 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 136535.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.17 ms /   128 runs   (    0.86 ms per token,  1161.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44931.94 ms /   348 tokens (  129.11 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 94678.78 ms /   127 runs   (  745.50 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140010.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.89 ms /   128 runs   (    0.86 ms per token,  1164.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48426.25 ms /   367 tokens (  131.95 ms per token,     7.58 tokens per second)\n",
      "llama_print_timings:        eval time = 94767.09 ms /   127 runs   (  746.20 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143587.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.26 ms /   128 runs   (    0.86 ms per token,  1160.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43717.31 ms /   328 tokens (  133.28 ms per token,     7.50 tokens per second)\n",
      "llama_print_timings:        eval time = 94678.76 ms /   127 runs   (  745.50 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 138792.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.17 ms /   128 runs   (    0.86 ms per token,  1161.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45230.98 ms /   340 tokens (  133.03 ms per token,     7.52 tokens per second)\n",
      "llama_print_timings:        eval time = 94735.96 ms /   127 runs   (  745.95 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140363.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    30.98 ms /    36 runs   (    0.86 ms per token,  1161.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68529.04 ms /   465 tokens (  147.37 ms per token,     6.79 tokens per second)\n",
      "llama_print_timings:        eval time = 26172.19 ms /    35 runs   (  747.78 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 94810.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   115.98 ms /   128 runs   (    0.91 ms per token,  1103.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50851.79 ms /   370 tokens (  137.44 ms per token,     7.28 tokens per second)\n",
      "llama_print_timings:        eval time = 96978.57 ms /   127 runs   (  763.61 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 148246.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    27.57 ms /    32 runs   (    0.86 ms per token,  1160.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 63003.62 ms /   437 tokens (  144.17 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:        eval time = 23117.45 ms /    31 runs   (  745.72 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 86217.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    46.44 ms /    51 runs   (    0.91 ms per token,  1098.29 tokens per second)\n",
      "llama_print_timings: prompt eval time = 66601.74 ms /   449 tokens (  148.33 ms per token,     6.74 tokens per second)\n",
      "llama_print_timings:        eval time = 39377.31 ms /    50 runs   (  787.55 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 106150.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    54.64 ms /    61 runs   (    0.90 ms per token,  1116.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62067.08 ms /   439 tokens (  141.38 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:        eval time = 45574.72 ms /    60 runs   (  759.58 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 107835.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    70.43 ms /    82 runs   (    0.86 ms per token,  1164.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 57757.44 ms /   418 tokens (  138.18 ms per token,     7.24 tokens per second)\n",
      "llama_print_timings:        eval time = 61038.02 ms /    81 runs   (  753.56 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 119046.56 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.14 ms /   128 runs   (    0.86 ms per token,  1162.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10904.05 ms /    15 tokens (  726.94 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93485.52 ms /   127 runs   (  736.11 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104790.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     1.72 ms /     2 runs   (    0.86 ms per token,  1166.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 75290.33 ms /   509 tokens (  147.92 ms per token,     6.76 tokens per second)\n",
      "llama_print_timings:        eval time =   745.83 ms /     1 runs   (  745.83 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 76042.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   113.08 ms /   128 runs   (    0.88 ms per token,  1131.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48250.38 ms /   363 tokens (  132.92 ms per token,     7.52 tokens per second)\n",
      "llama_print_timings:        eval time = 96137.38 ms /   127 runs   (  756.99 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 144800.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    66.91 ms /    78 runs   (    0.86 ms per token,  1165.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58353.23 ms /   422 tokens (  138.28 ms per token,     7.23 tokens per second)\n",
      "llama_print_timings:        eval time = 57502.61 ms /    77 runs   (  746.79 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 116093.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.84 ms /   128 runs   (    0.86 ms per token,  1165.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45438.19 ms /   352 tokens (  129.09 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 94662.42 ms /   127 runs   (  745.37 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140496.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.05 ms /   128 runs   (    0.86 ms per token,  1163.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46350.55 ms /   355 tokens (  130.56 ms per token,     7.66 tokens per second)\n",
      "llama_print_timings:        eval time = 94730.46 ms /   127 runs   (  745.91 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141477.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.97 ms /   128 runs   (    0.86 ms per token,  1163.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45870.90 ms /   354 tokens (  129.58 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 94730.99 ms /   127 runs   (  745.91 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140998.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    91.85 ms /   107 runs   (    0.86 ms per token,  1164.97 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52785.51 ms /   392 tokens (  134.66 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:        eval time = 79545.99 ms /   106 runs   (  750.43 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 132660.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    99.70 ms /   112 runs   (    0.89 ms per token,  1123.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52614.08 ms /   389 tokens (  135.25 ms per token,     7.39 tokens per second)\n",
      "llama_print_timings:        eval time = 126438.76 ms /   111 runs   ( 1139.09 ms per token,     0.88 tokens per second)\n",
      "llama_print_timings:       total time = 179416.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   111.49 ms /   128 runs   (    0.87 ms per token,  1148.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45082.84 ms /   345 tokens (  130.67 ms per token,     7.65 tokens per second)\n",
      "llama_print_timings:        eval time = 95187.24 ms /   127 runs   (  749.51 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 140684.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    77.42 ms /    90 runs   (    0.86 ms per token,  1162.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56232.14 ms /   411 tokens (  136.82 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 66595.53 ms /    89 runs   (  748.26 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 123105.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    67.13 ms /    78 runs   (    0.86 ms per token,  1161.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58590.01 ms /   422 tokens (  138.84 ms per token,     7.20 tokens per second)\n",
      "llama_print_timings:        eval time = 57599.30 ms /    77 runs   (  748.04 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 116429.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    46.27 ms /    52 runs   (    0.89 ms per token,  1123.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 63550.04 ms /   449 tokens (  141.54 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:        eval time = 40607.82 ms /    51 runs   (  796.23 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 104326.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    63.24 ms /    69 runs   (    0.92 ms per token,  1091.08 tokens per second)\n",
      "llama_print_timings: prompt eval time = 63715.16 ms /   432 tokens (  147.49 ms per token,     6.78 tokens per second)\n",
      "llama_print_timings:        eval time = 56264.38 ms /    68 runs   (  827.42 ms per token,     1.21 tokens per second)\n",
      "llama_print_timings:       total time = 120212.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    96.94 ms /   110 runs   (    0.88 ms per token,  1134.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56171.64 ms /   391 tokens (  143.66 ms per token,     6.96 tokens per second)\n",
      "llama_print_timings:        eval time = 85620.00 ms /   109 runs   (  785.50 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 142154.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    27.53 ms /    32 runs   (    0.86 ms per token,  1162.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68870.59 ms /   468 tokens (  147.16 ms per token,     6.80 tokens per second)\n",
      "llama_print_timings:        eval time = 23203.24 ms /    31 runs   (  748.49 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 92171.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    74.20 ms /    83 runs   (    0.89 ms per token,  1118.61 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58492.38 ms /   417 tokens (  140.27 ms per token,     7.13 tokens per second)\n",
      "llama_print_timings:        eval time = 62826.85 ms /    82 runs   (  766.18 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 121585.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   108.21 ms /   126 runs   (    0.86 ms per token,  1164.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49712.41 ms /   375 tokens (  132.57 ms per token,     7.54 tokens per second)\n",
      "llama_print_timings:        eval time = 93565.58 ms /   125 runs   (  748.52 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143670.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47136.23 ms /   360 tokens (  130.93 ms per token,     7.64 tokens per second)\n",
      "llama_print_timings:        eval time = 95141.96 ms /   127 runs   (  749.15 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 142677.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   107.23 ms /   125 runs   (    0.86 ms per token,  1165.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49794.77 ms /   376 tokens (  132.43 ms per token,     7.55 tokens per second)\n",
      "llama_print_timings:        eval time = 92906.02 ms /   124 runs   (  749.24 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 143089.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    70.49 ms /    82 runs   (    0.86 ms per token,  1163.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 57663.67 ms /   419 tokens (  137.62 ms per token,     7.27 tokens per second)\n",
      "llama_print_timings:        eval time = 60574.18 ms /    81 runs   (  747.83 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 118489.86 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.22 ms /   128 runs   (    0.86 ms per token,  1161.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12556.30 ms /    17 tokens (  738.61 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 93207.36 ms /   127 runs   (  733.92 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106171.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    96.20 ms /   112 runs   (    0.86 ms per token,  1164.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53702.92 ms /   399 tokens (  134.59 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:        eval time = 82988.40 ms /   111 runs   (  747.64 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 137038.72 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.02 ms /   128 runs   (    0.86 ms per token,  1163.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5120.53 ms /     7 tokens (  731.50 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93190.35 ms /   127 runs   (  733.78 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98712.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    71.78 ms /    83 runs   (    0.86 ms per token,  1156.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58983.99 ms /   428 tokens (  137.81 ms per token,     7.26 tokens per second)\n",
      "llama_print_timings:        eval time = 61437.84 ms /    82 runs   (  749.24 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 120680.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.88 ms /   128 runs   (    0.86 ms per token,  1164.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46060.50 ms /   353 tokens (  130.48 ms per token,     7.66 tokens per second)\n",
      "llama_print_timings:        eval time = 94793.90 ms /   127 runs   (  746.41 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141253.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    31.69 ms /    37 runs   (    0.86 ms per token,  1167.67 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62220.78 ms /   442 tokens (  140.77 ms per token,     7.10 tokens per second)\n",
      "llama_print_timings:        eval time = 26899.53 ms /    36 runs   (  747.21 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 89233.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    94.80 ms /   107 runs   (    0.89 ms per token,  1128.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53025.76 ms /   390 tokens (  135.96 ms per token,     7.35 tokens per second)\n",
      "llama_print_timings:        eval time = 80546.34 ms /   106 runs   (  759.87 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 133917.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    95.22 ms /   111 runs   (    0.86 ms per token,  1165.67 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48116.52 ms /   366 tokens (  131.47 ms per token,     7.61 tokens per second)\n",
      "llama_print_timings:        eval time = 82015.09 ms /   110 runs   (  745.59 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 130476.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47133.71 ms /   357 tokens (  132.03 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:        eval time = 95179.38 ms /   127 runs   (  749.44 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 142713.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.01 ms /   128 runs   (    0.86 ms per token,  1163.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46936.49 ms /   359 tokens (  130.74 ms per token,     7.65 tokens per second)\n",
      "llama_print_timings:        eval time = 94887.23 ms /   127 runs   (  747.14 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142223.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.07 ms /   128 runs   (    0.86 ms per token,  1162.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47535.30 ms /   362 tokens (  131.31 ms per token,     7.62 tokens per second)\n",
      "llama_print_timings:        eval time = 94819.70 ms /   127 runs   (  746.61 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142753.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    44.90 ms /    52 runs   (    0.86 ms per token,  1158.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62838.72 ms /   443 tokens (  141.85 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:        eval time = 38242.15 ms /    51 runs   (  749.85 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 101241.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    89.57 ms /   102 runs   (    0.88 ms per token,  1138.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54953.03 ms /   399 tokens (  137.73 ms per token,     7.26 tokens per second)\n",
      "llama_print_timings:        eval time = 75993.12 ms /   101 runs   (  752.41 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 131269.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.22 ms /   128 runs   (    0.86 ms per token,  1161.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46937.80 ms /   358 tokens (  131.11 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:        eval time = 95098.52 ms /   127 runs   (  748.81 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142436.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.45 ms /   128 runs   (    0.86 ms per token,  1158.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45839.26 ms /   353 tokens (  129.86 ms per token,     7.70 tokens per second)\n",
      "llama_print_timings:        eval time = 94928.67 ms /   127 runs   (  747.47 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141170.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.89 ms /   128 runs   (    0.86 ms per token,  1164.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45519.22 ms /   352 tokens (  129.32 ms per token,     7.73 tokens per second)\n",
      "llama_print_timings:        eval time = 94749.99 ms /   127 runs   (  746.06 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140670.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.93 ms /   128 runs   (    0.86 ms per token,  1164.39 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43640.03 ms /   338 tokens (  129.11 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 94678.67 ms /   127 runs   (  745.50 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 138722.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    79.81 ms /    93 runs   (    0.86 ms per token,  1165.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 55554.50 ms /   406 tokens (  136.83 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 68715.80 ms /    92 runs   (  746.91 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 124558.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.28 ms /   128 runs   (    0.86 ms per token,  1160.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46464.96 ms /   355 tokens (  130.89 ms per token,     7.64 tokens per second)\n",
      "llama_print_timings:        eval time = 95184.83 ms /   127 runs   (  749.49 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 142052.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.38 ms /   128 runs   (    0.86 ms per token,  1159.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42581.24 ms /   331 tokens (  128.64 ms per token,     7.77 tokens per second)\n",
      "llama_print_timings:        eval time = 94552.73 ms /   127 runs   (  744.51 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 137539.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   108.29 ms /   126 runs   (    0.86 ms per token,  1163.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44604.60 ms /   345 tokens (  129.29 ms per token,     7.73 tokens per second)\n",
      "llama_print_timings:        eval time = 93195.00 ms /   125 runs   (  745.56 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 138194.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    99.20 ms /   115 runs   (    0.86 ms per token,  1159.33 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51955.12 ms /   386 tokens (  134.60 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:        eval time = 85191.96 ms /   114 runs   (  747.30 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 137506.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.76 ms /   128 runs   (    0.86 ms per token,  1166.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47578.44 ms /   364 tokens (  130.71 ms per token,     7.65 tokens per second)\n",
      "llama_print_timings:        eval time = 94767.23 ms /   127 runs   (  746.20 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142746.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.26 ms /   128 runs   (    0.86 ms per token,  1160.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49125.44 ms /   371 tokens (  132.41 ms per token,     7.55 tokens per second)\n",
      "llama_print_timings:        eval time = 95406.53 ms /   127 runs   (  751.23 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 144935.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.26 ms /   128 runs   (    0.86 ms per token,  1160.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43509.47 ms /   337 tokens (  129.11 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 94876.96 ms /   127 runs   (  747.06 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 138790.14 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.93 ms /   128 runs   (    0.86 ms per token,  1164.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14594.54 ms /    20 tokens (  729.73 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93390.63 ms /   127 runs   (  735.36 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108385.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     5.14 ms /     6 runs   (    0.86 ms per token,  1167.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74394.16 ms /   505 tokens (  147.32 ms per token,     6.79 tokens per second)\n",
      "llama_print_timings:        eval time =  3743.66 ms /     5 runs   (  748.73 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 78156.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   114.70 ms /   128 runs   (    0.90 ms per token,  1115.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47813.90 ms /   359 tokens (  133.19 ms per token,     7.51 tokens per second)\n",
      "llama_print_timings:        eval time = 96291.52 ms /   127 runs   (  758.20 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 144524.19 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     1.73 ms /     2 runs   (    0.86 ms per token,  1159.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5078.95 ms /     7 tokens (  725.56 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time =   727.07 ms /     1 runs   (  727.07 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:       total time =  5812.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.06 ms /   128 runs   (    0.86 ms per token,  1163.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44582.38 ms /   348 tokens (  128.11 ms per token,     7.81 tokens per second)\n",
      "llama_print_timings:        eval time = 94641.58 ms /   127 runs   (  745.21 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139628.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    90.57 ms /   105 runs   (    0.86 ms per token,  1159.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53434.25 ms /   396 tokens (  134.93 ms per token,     7.41 tokens per second)\n",
      "llama_print_timings:        eval time = 77677.86 ms /   104 runs   (  746.90 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 131441.71 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.03 ms /   128 runs   (    0.86 ms per token,  1163.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13921.11 ms /    19 tokens (  732.69 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93237.30 ms /   127 runs   (  734.15 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107560.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.83 ms /   128 runs   (    0.86 ms per token,  1165.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44245.87 ms /   348 tokens (  127.14 ms per token,     7.87 tokens per second)\n",
      "llama_print_timings:        eval time = 94697.84 ms /   127 runs   (  745.65 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139348.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    41.20 ms /    48 runs   (    0.86 ms per token,  1165.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49737.65 ms /   373 tokens (  133.34 ms per token,     7.50 tokens per second)\n",
      "llama_print_timings:        eval time = 35012.27 ms /    47 runs   (  744.94 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 84899.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    79.60 ms /    91 runs   (    0.87 ms per token,  1143.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43440.95 ms /   337 tokens (  128.90 ms per token,     7.76 tokens per second)\n",
      "llama_print_timings:        eval time = 67574.67 ms /    90 runs   (  750.83 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 111309.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    37.77 ms /    44 runs   (    0.86 ms per token,  1165.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 59433.86 ms /   428 tokens (  138.86 ms per token,     7.20 tokens per second)\n",
      "llama_print_timings:        eval time = 32205.84 ms /    43 runs   (  748.97 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 91776.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    99.75 ms /   116 runs   (    0.86 ms per token,  1162.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44012.84 ms /   341 tokens (  129.07 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 85765.27 ms /   115 runs   (  745.78 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 130143.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46548.59 ms /   355 tokens (  131.12 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:        eval time = 94883.80 ms /   127 runs   (  747.12 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141835.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.03 ms /   128 runs   (    0.86 ms per token,  1163.33 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45673.34 ms /   352 tokens (  129.75 ms per token,     7.71 tokens per second)\n",
      "llama_print_timings:        eval time = 94888.42 ms /   127 runs   (  747.15 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140964.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.11 ms /   128 runs   (    0.86 ms per token,  1162.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43503.16 ms /   338 tokens (  128.71 ms per token,     7.77 tokens per second)\n",
      "llama_print_timings:        eval time = 94711.17 ms /   127 runs   (  745.76 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 138618.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.96 ms /   128 runs   (    0.86 ms per token,  1164.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45363.22 ms /   350 tokens (  129.61 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 94698.46 ms /   127 runs   (  745.66 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140464.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    77.63 ms /    90 runs   (    0.86 ms per token,  1159.39 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56033.76 ms /   410 tokens (  136.67 ms per token,     7.32 tokens per second)\n",
      "llama_print_timings:        eval time = 66524.88 ms /    89 runs   (  747.47 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 122840.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.94 ms /   128 runs   (    0.86 ms per token,  1164.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46548.25 ms /   356 tokens (  130.75 ms per token,     7.65 tokens per second)\n",
      "llama_print_timings:        eval time = 95071.60 ms /   127 runs   (  748.60 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142022.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.81 ms /   128 runs   (    0.86 ms per token,  1165.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43529.34 ms /   339 tokens (  128.41 ms per token,     7.79 tokens per second)\n",
      "llama_print_timings:        eval time = 94775.90 ms /   127 runs   (  746.27 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 138710.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.01 ms /   128 runs   (    0.86 ms per token,  1163.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48355.89 ms /   367 tokens (  131.76 ms per token,     7.59 tokens per second)\n",
      "llama_print_timings:        eval time = 94848.99 ms /   127 runs   (  746.84 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143607.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    69.47 ms /    81 runs   (    0.86 ms per token,  1165.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 55525.64 ms /   407 tokens (  136.43 ms per token,     7.33 tokens per second)\n",
      "llama_print_timings:        eval time = 59806.36 ms /    80 runs   (  747.58 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 115584.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    36.58 ms /    42 runs   (    0.87 ms per token,  1148.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65666.95 ms /   459 tokens (  143.07 ms per token,     6.99 tokens per second)\n",
      "llama_print_timings:        eval time = 30879.57 ms /    41 runs   (  753.16 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 96677.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   112.09 ms /   128 runs   (    0.88 ms per token,  1141.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42431.15 ms /   329 tokens (  128.97 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 95682.75 ms /   127 runs   (  753.41 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 138528.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   103.99 ms /   121 runs   (    0.86 ms per token,  1163.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50554.42 ms /   380 tokens (  133.04 ms per token,     7.52 tokens per second)\n",
      "llama_print_timings:        eval time = 89589.42 ms /   120 runs   (  746.58 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140524.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    80.96 ms /    94 runs   (    0.86 ms per token,  1161.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54156.50 ms /   400 tokens (  135.39 ms per token,     7.39 tokens per second)\n",
      "llama_print_timings:        eval time = 69611.98 ms /    93 runs   (  748.52 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 124063.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.06 ms /   128 runs   (    0.86 ms per token,  1162.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40924.56 ms /   324 tokens (  126.31 ms per token,     7.92 tokens per second)\n",
      "llama_print_timings:        eval time = 94584.72 ms /   127 runs   (  744.76 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 135916.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.78 ms /   128 runs   (    0.86 ms per token,  1166.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47704.34 ms /   363 tokens (  131.42 ms per token,     7.61 tokens per second)\n",
      "llama_print_timings:        eval time = 94774.87 ms /   127 runs   (  746.26 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142881.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.81 ms /   128 runs   (    0.86 ms per token,  1165.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48552.85 ms /   368 tokens (  131.94 ms per token,     7.58 tokens per second)\n",
      "llama_print_timings:        eval time = 94858.70 ms /   127 runs   (  746.92 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143814.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    67.15 ms /    78 runs   (    0.86 ms per token,  1161.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58186.86 ms /   422 tokens (  137.88 ms per token,     7.25 tokens per second)\n",
      "llama_print_timings:        eval time = 57660.90 ms /    77 runs   (  748.84 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 116092.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.15 ms /   128 runs   (    0.86 ms per token,  1162.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46166.03 ms /   354 tokens (  130.41 ms per token,     7.67 tokens per second)\n",
      "llama_print_timings:        eval time = 94788.97 ms /   127 runs   (  746.37 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141360.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.80 ms /   128 runs   (    0.86 ms per token,  1165.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45711.23 ms /   352 tokens (  129.86 ms per token,     7.70 tokens per second)\n",
      "llama_print_timings:        eval time = 94744.31 ms /   127 runs   (  746.02 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140858.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    29.28 ms /    34 runs   (    0.86 ms per token,  1161.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67274.69 ms /   467 tokens (  144.06 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:        eval time = 24749.76 ms /    33 runs   (  749.99 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 92130.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   100.95 ms /   113 runs   (    0.89 ms per token,  1119.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52421.91 ms /   388 tokens (  135.11 ms per token,     7.40 tokens per second)\n",
      "llama_print_timings:        eval time = 85042.07 ms /   112 runs   (  759.30 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 137832.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    23.15 ms /    27 runs   (    0.86 ms per token,  1166.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68860.92 ms /   474 tokens (  145.28 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:        eval time = 19446.74 ms /    26 runs   (  747.95 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 88391.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    94.51 ms /   106 runs   (    0.89 ms per token,  1121.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53449.42 ms /   392 tokens (  136.35 ms per token,     7.33 tokens per second)\n",
      "llama_print_timings:        eval time = 79826.97 ms /   105 runs   (  760.26 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 133621.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   108.99 ms /   127 runs   (    0.86 ms per token,  1165.29 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49400.72 ms /   374 tokens (  132.09 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:        eval time = 94166.86 ms /   126 runs   (  747.36 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143967.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    72.75 ms /    85 runs   (    0.86 ms per token,  1168.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48340.21 ms /   368 tokens (  131.36 ms per token,     7.61 tokens per second)\n",
      "llama_print_timings:        eval time = 62676.83 ms /    84 runs   (  746.15 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 111282.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.92 ms /   128 runs   (    0.86 ms per token,  1164.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48535.00 ms /   366 tokens (  132.61 ms per token,     7.54 tokens per second)\n",
      "llama_print_timings:        eval time = 94789.25 ms /   127 runs   (  746.37 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143727.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.94 ms /   128 runs   (    0.86 ms per token,  1164.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44530.69 ms /   345 tokens (  129.07 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 94804.23 ms /   127 runs   (  746.49 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139740.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    96.04 ms /   112 runs   (    0.86 ms per token,  1166.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52748.15 ms /   389 tokens (  135.60 ms per token,     7.37 tokens per second)\n",
      "llama_print_timings:        eval time = 83100.78 ms /   111 runs   (  748.66 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 136202.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.96 ms /   128 runs   (    0.86 ms per token,  1164.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45891.93 ms /   353 tokens (  130.01 ms per token,     7.69 tokens per second)\n",
      "llama_print_timings:        eval time = 94659.37 ms /   127 runs   (  745.35 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140957.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.24 ms /   128 runs   (    0.86 ms per token,  1161.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44148.38 ms /   341 tokens (  129.47 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 94677.59 ms /   127 runs   (  745.49 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139231.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   108.15 ms /   126 runs   (    0.86 ms per token,  1165.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49610.58 ms /   375 tokens (  132.29 ms per token,     7.56 tokens per second)\n",
      "llama_print_timings:        eval time = 93345.68 ms /   125 runs   (  746.77 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143353.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.87 ms /   128 runs   (    0.86 ms per token,  1165.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47906.33 ms /   366 tokens (  130.89 ms per token,     7.64 tokens per second)\n",
      "llama_print_timings:        eval time = 94825.68 ms /   127 runs   (  746.66 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143135.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    66.48 ms /    77 runs   (    0.86 ms per token,  1158.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58503.97 ms /   424 tokens (  137.98 ms per token,     7.25 tokens per second)\n",
      "llama_print_timings:        eval time = 56859.43 ms /    76 runs   (  748.15 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 115605.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    10.27 ms /    12 runs   (    0.86 ms per token,  1168.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71915.78 ms /   489 tokens (  147.07 ms per token,     6.80 tokens per second)\n",
      "llama_print_timings:        eval time =  8278.72 ms /    11 runs   (  752.61 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 80232.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    75.25 ms /    83 runs   (    0.91 ms per token,  1103.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58717.23 ms /   418 tokens (  140.47 ms per token,     7.12 tokens per second)\n",
      "llama_print_timings:        eval time = 63185.12 ms /    82 runs   (  770.55 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 122176.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    91.81 ms /   107 runs   (    0.86 ms per token,  1165.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53524.01 ms /   394 tokens (  135.85 ms per token,     7.36 tokens per second)\n",
      "llama_print_timings:        eval time = 79255.74 ms /   106 runs   (  747.70 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133116.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.94 ms /   128 runs   (    0.86 ms per token,  1164.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49120.32 ms /   372 tokens (  132.04 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:        eval time = 94909.20 ms /   127 runs   (  747.32 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 144434.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    70.02 ms /    81 runs   (    0.86 ms per token,  1156.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58146.75 ms /   420 tokens (  138.44 ms per token,     7.22 tokens per second)\n",
      "llama_print_timings:        eval time = 59911.84 ms /    80 runs   (  748.90 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 118314.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    77.95 ms /    89 runs   (    0.88 ms per token,  1141.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56587.48 ms /   412 tokens (  137.35 ms per token,     7.28 tokens per second)\n",
      "llama_print_timings:        eval time = 66119.07 ms /    88 runs   (  751.35 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 122990.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    84.06 ms /    98 runs   (    0.86 ms per token,  1165.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54983.03 ms /   403 tokens (  136.43 ms per token,     7.33 tokens per second)\n",
      "llama_print_timings:        eval time = 72645.61 ms /    97 runs   (  748.92 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 127936.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   106.62 ms /   124 runs   (    0.86 ms per token,  1163.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49952.92 ms /   377 tokens (  132.50 ms per token,     7.55 tokens per second)\n",
      "llama_print_timings:        eval time = 91860.67 ms /   123 runs   (  746.83 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142204.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.06 ms /   128 runs   (    0.86 ms per token,  1163.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44076.18 ms /   344 tokens (  128.13 ms per token,     7.80 tokens per second)\n",
      "llama_print_timings:        eval time = 94752.35 ms /   127 runs   (  746.08 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139234.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    93.53 ms /   109 runs   (    0.86 ms per token,  1165.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52637.40 ms /   390 tokens (  134.97 ms per token,     7.41 tokens per second)\n",
      "llama_print_timings:        eval time = 80562.12 ms /   108 runs   (  745.95 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133541.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    59.20 ms /    68 runs   (    0.87 ms per token,  1148.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 60821.56 ms /   433 tokens (  140.47 ms per token,     7.12 tokens per second)\n",
      "llama_print_timings:        eval time = 50208.99 ms /    67 runs   (  749.39 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 111245.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   101.43 ms /   118 runs   (    0.86 ms per token,  1163.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51469.58 ms /   383 tokens (  134.39 ms per token,     7.44 tokens per second)\n",
      "llama_print_timings:        eval time = 87605.48 ms /   117 runs   (  748.76 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139449.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.03 ms /   128 runs   (    0.86 ms per token,  1163.33 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45691.05 ms /   352 tokens (  129.80 ms per token,     7.70 tokens per second)\n",
      "llama_print_timings:        eval time = 94747.79 ms /   127 runs   (  746.05 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140844.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    21.51 ms /    25 runs   (    0.86 ms per token,  1162.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 69012.06 ms /   476 tokens (  144.98 ms per token,     6.90 tokens per second)\n",
      "llama_print_timings:        eval time = 17987.05 ms /    24 runs   (  749.46 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 87077.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   113.36 ms /   128 runs   (    0.89 ms per token,  1129.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46390.52 ms /   351 tokens (  132.17 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:        eval time = 95896.18 ms /   127 runs   (  755.09 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 142706.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.95 ms /   128 runs   (    0.86 ms per token,  1164.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43924.96 ms /   343 tokens (  128.06 ms per token,     7.81 tokens per second)\n",
      "llama_print_timings:        eval time = 94732.11 ms /   127 runs   (  745.92 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139062.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.99 ms /   128 runs   (    0.86 ms per token,  1163.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45455.46 ms /   351 tokens (  129.50 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 94726.51 ms /   127 runs   (  745.88 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140587.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   102.91 ms /   120 runs   (    0.86 ms per token,  1166.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51244.58 ms /   381 tokens (  134.50 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:        eval time = 88938.42 ms /   119 runs   (  747.38 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140560.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   101.58 ms /   118 runs   (    0.86 ms per token,  1161.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51276.26 ms /   383 tokens (  133.88 ms per token,     7.47 tokens per second)\n",
      "llama_print_timings:        eval time = 87430.75 ms /   117 runs   (  747.27 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139079.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.08 ms /   128 runs   (    0.86 ms per token,  1162.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47717.03 ms /   363 tokens (  131.45 ms per token,     7.61 tokens per second)\n",
      "llama_print_timings:        eval time = 94868.23 ms /   127 runs   (  746.99 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142991.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.39 ms /   128 runs   (    0.86 ms per token,  1159.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45344.43 ms /   348 tokens (  130.30 ms per token,     7.67 tokens per second)\n",
      "llama_print_timings:        eval time = 94711.48 ms /   127 runs   (  745.76 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140463.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.21 ms /   128 runs   (    0.86 ms per token,  1161.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42677.83 ms /   334 tokens (  127.78 ms per token,     7.83 tokens per second)\n",
      "llama_print_timings:        eval time = 94824.16 ms /   127 runs   (  746.65 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 137911.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.14 ms /   128 runs   (    0.86 ms per token,  1162.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43973.54 ms /   340 tokens (  129.33 ms per token,     7.73 tokens per second)\n",
      "llama_print_timings:        eval time = 94820.35 ms /   127 runs   (  746.62 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139200.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    37.79 ms /    44 runs   (    0.86 ms per token,  1164.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65090.52 ms /   457 tokens (  142.43 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:        eval time = 32267.71 ms /    43 runs   (  750.41 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 97495.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    87.85 ms /   100 runs   (    0.88 ms per token,  1138.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54909.10 ms /   401 tokens (  136.93 ms per token,     7.30 tokens per second)\n",
      "llama_print_timings:        eval time = 74407.13 ms /    99 runs   (  751.59 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 129636.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    85.81 ms /   100 runs   (    0.86 ms per token,  1165.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54606.03 ms /   401 tokens (  136.17 ms per token,     7.34 tokens per second)\n",
      "llama_print_timings:        eval time = 74456.29 ms /    99 runs   (  752.08 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 129377.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    82.87 ms /    96 runs   (    0.86 ms per token,  1158.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 55423.61 ms /   405 tokens (  136.85 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 71140.64 ms /    95 runs   (  748.85 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 126867.90 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.88 ms /   128 runs   (    0.86 ms per token,  1164.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21087.21 ms /    29 tokens (  727.15 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93264.98 ms /   127 runs   (  734.37 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 114758.82 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.98 ms /   128 runs   (    0.86 ms per token,  1163.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16876.72 ms /    23 tokens (  733.77 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93268.25 ms /   127 runs   (  734.40 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 110549.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    15.52 ms /    18 runs   (    0.86 ms per token,  1159.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72223.31 ms /   493 tokens (  146.50 ms per token,     6.83 tokens per second)\n",
      "llama_print_timings:        eval time = 12782.92 ms /    17 runs   (  751.94 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 85062.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    49.35 ms /    55 runs   (    0.90 ms per token,  1114.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64067.04 ms /   446 tokens (  143.65 ms per token,     6.96 tokens per second)\n",
      "llama_print_timings:        eval time = 41399.35 ms /    54 runs   (  766.65 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 105646.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    99.60 ms /   115 runs   (    0.87 ms per token,  1154.65 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52375.85 ms /   386 tokens (  135.69 ms per token,     7.37 tokens per second)\n",
      "llama_print_timings:        eval time = 85466.23 ms /   114 runs   (  749.70 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 138208.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    94.00 ms /   109 runs   (    0.86 ms per token,  1159.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52780.32 ms /   392 tokens (  134.64 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:        eval time = 80636.96 ms /   108 runs   (  746.64 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133762.06 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     1.72 ms /     2 runs   (    0.86 ms per token,  1163.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16856.72 ms /    23 tokens (  732.90 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time =   727.58 ms /     1 runs   (  727.58 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 17590.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    31.71 ms /    37 runs   (    0.86 ms per token,  1167.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 57309.38 ms /   419 tokens (  136.78 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 26881.35 ms /    36 runs   (  746.70 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 84306.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   112.31 ms /   128 runs   (    0.88 ms per token,  1139.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49149.82 ms /   367 tokens (  133.92 ms per token,     7.47 tokens per second)\n",
      "llama_print_timings:        eval time = 95987.09 ms /   127 runs   (  755.80 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 145551.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    89.25 ms /   104 runs   (    0.86 ms per token,  1165.21 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54016.18 ms /   397 tokens (  136.06 ms per token,     7.35 tokens per second)\n",
      "llama_print_timings:        eval time = 76936.54 ms /   103 runs   (  746.96 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 131280.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.67 ms /   128 runs   (    0.86 ms per token,  1167.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47651.45 ms /   364 tokens (  130.91 ms per token,     7.64 tokens per second)\n",
      "llama_print_timings:        eval time = 94805.20 ms /   127 runs   (  746.50 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142861.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.94 ms /   128 runs   (    0.86 ms per token,  1164.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44152.67 ms /   342 tokens (  129.10 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 94741.53 ms /   127 runs   (  746.00 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 139300.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    92.95 ms /   108 runs   (    0.86 ms per token,  1161.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52806.72 ms /   392 tokens (  134.71 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:        eval time = 79989.40 ms /   107 runs   (  747.56 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133137.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    56.89 ms /    66 runs   (    0.86 ms per token,  1160.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61089.95 ms /   434 tokens (  140.76 ms per token,     7.10 tokens per second)\n",
      "llama_print_timings:        eval time = 48650.41 ms /    65 runs   (  748.47 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 109947.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    91.84 ms /   107 runs   (    0.86 ms per token,  1165.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53348.30 ms /   393 tokens (  135.75 ms per token,     7.37 tokens per second)\n",
      "llama_print_timings:        eval time = 79365.89 ms /   106 runs   (  748.73 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133052.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.79 ms /   128 runs   (    0.86 ms per token,  1165.85 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47192.47 ms /   360 tokens (  131.09 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:        eval time = 94825.73 ms /   127 runs   (  746.66 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142424.31 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    61.10 ms /    71 runs   (    0.86 ms per token,  1162.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10169.08 ms /    14 tokens (  726.36 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 53287.04 ms /    70 runs   (  761.24 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 63681.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.74 ms /   128 runs   (    0.86 ms per token,  1166.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47635.34 ms /   367 tokens (  129.80 ms per token,     7.70 tokens per second)\n",
      "llama_print_timings:        eval time = 94864.12 ms /   127 runs   (  746.96 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142909.05 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.99 ms /   128 runs   (    0.86 ms per token,  1163.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10160.46 ms /    14 tokens (  725.75 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93332.70 ms /   127 runs   (  734.90 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 103899.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.03 ms /   128 runs   (    0.86 ms per token,  1163.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48396.93 ms /   371 tokens (  130.45 ms per token,     7.67 tokens per second)\n",
      "llama_print_timings:        eval time = 94833.26 ms /   127 runs   (  746.72 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143635.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.70 ms /   128 runs   (    0.86 ms per token,  1166.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44949.83 ms /   347 tokens (  129.54 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 94796.11 ms /   127 runs   (  746.43 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140152.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.80 ms /   128 runs   (    0.86 ms per token,  1165.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47617.57 ms /   363 tokens (  131.18 ms per token,     7.62 tokens per second)\n",
      "llama_print_timings:        eval time = 94806.39 ms /   127 runs   (  746.51 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142828.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.90 ms /   128 runs   (    0.86 ms per token,  1164.67 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45605.42 ms /   352 tokens (  129.56 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 94705.60 ms /   127 runs   (  745.71 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140717.40 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.93 ms /   128 runs   (    0.86 ms per token,  1164.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13315.00 ms /    18 tokens (  739.72 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 93374.45 ms /   127 runs   (  735.23 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107094.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    21.47 ms /    25 runs   (    0.86 ms per token,  1164.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 70461.13 ms /   486 tokens (  144.98 ms per token,     6.90 tokens per second)\n",
      "llama_print_timings:        eval time = 17996.44 ms /    24 runs   (  749.85 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 88535.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   114.93 ms /   128 runs   (    0.90 ms per token,  1113.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49155.47 ms /   365 tokens (  134.67 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:        eval time = 96570.17 ms /   127 runs   (  760.40 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 146150.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    27.46 ms /    32 runs   (    0.86 ms per token,  1165.29 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68219.12 ms /   469 tokens (  145.46 ms per token,     6.87 tokens per second)\n",
      "llama_print_timings:        eval time = 23223.02 ms /    31 runs   (  749.13 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 91542.76 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.92 ms /   128 runs   (    0.87 ms per token,  1153.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5658.70 ms /    43 tokens (  131.60 ms per token,     7.60 tokens per second)\n",
      "llama_print_timings:        eval time = 93644.88 ms /   127 runs   (  737.36 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 99713.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    68.02 ms /    79 runs   (    0.86 ms per token,  1161.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 59710.49 ms /   432 tokens (  138.22 ms per token,     7.23 tokens per second)\n",
      "llama_print_timings:        eval time = 58391.34 ms /    78 runs   (  748.61 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 118350.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     4.39 ms /     5 runs   (    0.88 ms per token,  1137.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73380.36 ms /   496 tokens (  147.94 ms per token,     6.76 tokens per second)\n",
      "llama_print_timings:        eval time =  3030.55 ms /     4 runs   (  757.64 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 76426.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    39.89 ms /    44 runs   (    0.91 ms per token,  1103.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 66478.47 ms /   457 tokens (  145.47 ms per token,     6.87 tokens per second)\n",
      "llama_print_timings:        eval time = 33196.04 ms /    43 runs   (  772.00 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 99819.28 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   111.57 ms /   128 runs   (    0.87 ms per token,  1147.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14710.34 ms /    20 tokens (  735.52 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93524.24 ms /   127 runs   (  736.41 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108645.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    44.56 ms /    52 runs   (    0.86 ms per token,  1167.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65082.57 ms /   459 tokens (  141.79 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:        eval time = 38151.80 ms /    51 runs   (  748.07 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 103397.52 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.99 ms /   128 runs   (    0.86 ms per token,  1163.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10204.07 ms /    14 tokens (  728.86 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93321.10 ms /   127 runs   (  734.81 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 103932.14 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.93 ms /   128 runs   (    0.86 ms per token,  1164.33 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13207.37 ms /    18 tokens (  733.74 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93358.96 ms /   127 runs   (  735.11 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106972.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    22.40 ms /    26 runs   (    0.86 ms per token,  1160.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 70453.45 ms /   485 tokens (  145.26 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:        eval time = 18829.98 ms /    25 runs   (  753.20 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 89365.86 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.65 ms /   128 runs   (    0.86 ms per token,  1156.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8720.47 ms /    12 tokens (  726.71 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93325.57 ms /   127 runs   (  734.85 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 102453.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    10.29 ms /    12 runs   (    0.86 ms per token,  1165.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73362.03 ms /   499 tokens (  147.02 ms per token,     6.80 tokens per second)\n",
      "llama_print_timings:        eval time =  8233.28 ms /    11 runs   (  748.48 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 81632.47 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.63 ms /   128 runs   (    0.86 ms per token,  1167.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13957.77 ms /    19 tokens (  734.62 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93336.63 ms /   127 runs   (  734.93 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107699.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    17.11 ms /    20 runs   (    0.86 ms per token,  1169.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71788.33 ms /   491 tokens (  146.21 ms per token,     6.84 tokens per second)\n",
      "llama_print_timings:        eval time = 14216.69 ms /    19 runs   (  748.25 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 86067.04 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.95 ms /   128 runs   (    0.86 ms per token,  1164.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13179.11 ms /    18 tokens (  732.17 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93360.31 ms /   127 runs   (  735.12 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106944.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    67.06 ms /    78 runs   (    0.86 ms per token,  1163.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 59790.72 ms /   433 tokens (  138.08 ms per token,     7.24 tokens per second)\n",
      "llama_print_timings:        eval time = 57760.85 ms /    77 runs   (  750.14 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 117798.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    51.86 ms /    60 runs   (    0.86 ms per token,  1156.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61025.63 ms /   433 tokens (  140.94 ms per token,     7.10 tokens per second)\n",
      "llama_print_timings:        eval time = 44166.62 ms /    59 runs   (  748.59 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 105381.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    51.96 ms /    60 runs   (    0.87 ms per token,  1154.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61449.91 ms /   433 tokens (  141.92 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:        eval time = 44203.63 ms /    59 runs   (  749.21 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 105844.34 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.18 ms /   128 runs   (    0.86 ms per token,  1161.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17599.06 ms /    24 tokens (  733.29 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93384.45 ms /   127 runs   (  735.31 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111388.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    56.71 ms /    66 runs   (    0.86 ms per token,  1163.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62589.75 ms /   445 tokens (  140.65 ms per token,     7.11 tokens per second)\n",
      "llama_print_timings:        eval time = 48684.18 ms /    65 runs   (  748.99 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 111481.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    53.77 ms /    61 runs   (    0.88 ms per token,  1134.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61401.24 ms /   432 tokens (  142.13 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:        eval time = 45809.03 ms /    60 runs   (  763.48 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 107408.09 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10200.99 ms /    14 tokens (  728.64 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93329.13 ms /   127 runs   (  734.88 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 103937.19 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.14 ms /   128 runs   (    0.86 ms per token,  1162.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4374.61 ms /     6 tokens (  729.10 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93273.77 ms /   127 runs   (  734.44 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98054.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    11.14 ms /    13 runs   (    0.86 ms per token,  1166.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72680.15 ms /   498 tokens (  145.94 ms per token,     6.85 tokens per second)\n",
      "llama_print_timings:        eval time =  8991.13 ms /    12 runs   (  749.26 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 81712.13 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.88 ms /   128 runs   (    0.87 ms per token,  1154.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10153.53 ms /    14 tokens (  725.25 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93551.42 ms /   127 runs   (  736.63 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104113.75 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.82 ms /   128 runs   (    0.86 ms per token,  1165.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11655.83 ms /    16 tokens (  728.49 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93350.65 ms /   127 runs   (  735.04 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105413.01 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.17 ms /   128 runs   (    0.86 ms per token,  1161.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5814.08 ms /     8 tokens (  726.76 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93310.32 ms /   127 runs   (  734.73 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 99530.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    24.40 ms /    28 runs   (    0.87 ms per token,  1147.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 70055.03 ms /   483 tokens (  145.04 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:        eval time = 20197.07 ms /    27 runs   (  748.04 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 90340.25 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.81 ms /   128 runs   (    0.87 ms per token,  1155.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11621.23 ms /    16 tokens (  726.33 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93471.57 ms /   127 runs   (  736.00 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105503.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    25.74 ms /    30 runs   (    0.86 ms per token,  1165.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 69355.04 ms /   481 tokens (  144.19 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:        eval time = 21700.95 ms /    29 runs   (  748.31 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 91149.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.56 ms /   128 runs   (    0.86 ms per token,  1157.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12514.52 ms /    17 tokens (  736.15 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93389.02 ms /   127 runs   (  735.35 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106312.22 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.11 ms /   128 runs   (    0.86 ms per token,  1162.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10893.68 ms /    15 tokens (  726.25 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93352.18 ms /   127 runs   (  735.06 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104652.40 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.19 ms /   128 runs   (    0.86 ms per token,  1161.62 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16084.78 ms /    22 tokens (  731.13 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93408.38 ms /   127 runs   (  735.50 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 109898.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    27.05 ms /    31 runs   (    0.87 ms per token,  1145.98 tokens per second)\n",
      "llama_print_timings: prompt eval time = 69314.80 ms /   480 tokens (  144.41 ms per token,     6.92 tokens per second)\n",
      "llama_print_timings:        eval time = 22423.14 ms /    30 runs   (  747.44 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 91836.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   105.03 ms /   114 runs   (    0.92 ms per token,  1085.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49014.15 ms /   365 tokens (  134.29 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:        eval time = 85854.79 ms /   113 runs   (  759.78 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 135252.69 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    55.89 ms /    65 runs   (    0.86 ms per token,  1163.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13868.80 ms /    19 tokens (  729.94 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 46984.37 ms /    64 runs   (  734.13 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 61056.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.24 ms /   127 runs   (    0.86 ms per token,  1162.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50748.67 ms /   384 tokens (  132.16 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:        eval time = 94121.83 ms /   126 runs   (  747.00 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 145273.75 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    37.88 ms /    44 runs   (    0.86 ms per token,  1161.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14595.91 ms /    20 tokens (  729.80 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 31580.81 ms /    43 runs   (  734.44 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 46315.36 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.11 ms /   128 runs   (    0.86 ms per token,  1162.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13182.24 ms /    18 tokens (  732.35 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93839.59 ms /   127 runs   (  738.89 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 107429.45 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.77 ms /   128 runs   (    0.86 ms per token,  1166.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11625.72 ms /    16 tokens (  726.61 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93263.59 ms /   127 runs   (  734.36 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105295.05 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    19.83 ms /    23 runs   (    0.86 ms per token,  1159.80 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13933.51 ms /    19 tokens (  733.34 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 16081.88 ms /    22 runs   (  730.99 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 30087.50 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.95 ms /   128 runs   (    0.86 ms per token,  1164.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11646.35 ms /    16 tokens (  727.90 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93386.43 ms /   127 runs   (  735.33 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105438.57 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.80 ms /   128 runs   (    0.86 ms per token,  1165.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11631.89 ms /    16 tokens (  726.99 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93326.58 ms /   127 runs   (  734.85 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105364.12 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.94 ms /   128 runs   (    0.86 ms per token,  1164.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13881.33 ms /    19 tokens (  730.60 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93418.68 ms /   127 runs   (  735.58 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107705.81 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.64 ms /   128 runs   (    0.86 ms per token,  1167.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13948.11 ms /    19 tokens (  734.11 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93235.71 ms /   127 runs   (  734.14 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107589.30 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    35.27 ms /    41 runs   (    0.86 ms per token,  1162.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13209.20 ms /    18 tokens (  733.84 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 29381.62 ms /    40 runs   (  734.54 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 42719.39 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.03 ms /   128 runs   (    0.86 ms per token,  1163.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14595.14 ms /    20 tokens (  729.76 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93396.68 ms /   127 runs   (  735.41 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108398.50 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.94 ms /   128 runs   (    0.86 ms per token,  1164.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13878.90 ms /    19 tokens (  730.47 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93271.85 ms /   127 runs   (  734.42 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107554.87 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    42.85 ms /    50 runs   (    0.86 ms per token,  1166.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13931.85 ms /    19 tokens (  733.26 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 35859.63 ms /    49 runs   (  731.83 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 49948.62 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.67 ms /   128 runs   (    0.86 ms per token,  1167.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11617.64 ms /    16 tokens (  726.10 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93337.85 ms /   127 runs   (  734.94 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105361.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    10.31 ms /    12 runs   (    0.86 ms per token,  1164.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73449.54 ms /   499 tokens (  147.19 ms per token,     6.79 tokens per second)\n",
      "llama_print_timings:        eval time =  8231.12 ms /    11 runs   (  748.28 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 81717.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    80.46 ms /    90 runs   (    0.89 ms per token,  1118.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 57531.84 ms /   411 tokens (  139.98 ms per token,     7.14 tokens per second)\n",
      "llama_print_timings:        eval time = 68000.13 ms /    89 runs   (  764.05 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 125827.69 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.12 ms /   128 runs   (    0.86 ms per token,  1162.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15599.58 ms /    21 tokens (  742.84 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 93300.61 ms /   127 runs   (  734.65 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 109305.58 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.79 ms /   128 runs   (    0.86 ms per token,  1165.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13906.38 ms /    19 tokens (  731.91 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93137.92 ms /   127 runs   (  733.37 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107448.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    30.93 ms /    36 runs   (    0.86 ms per token,  1163.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68655.01 ms /   475 tokens (  144.54 ms per token,     6.92 tokens per second)\n",
      "llama_print_timings:        eval time = 26307.57 ms /    35 runs   (  751.64 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 95074.78 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     1.72 ms /     2 runs   (    0.86 ms per token,  1165.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13881.29 ms /    19 tokens (  730.59 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time =   728.00 ms /     1 runs   (  728.00 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 14614.96 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.89 ms /   128 runs   (    0.86 ms per token,  1164.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11644.53 ms /    16 tokens (  727.78 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93435.67 ms /   127 runs   (  735.71 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105487.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     2.60 ms /     3 runs   (    0.87 ms per token,  1152.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 75282.62 ms /   508 tokens (  148.19 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time =  1491.14 ms /     2 runs   (  745.57 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 76782.86 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   103.10 ms /   118 runs   (    0.87 ms per token,  1144.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13886.30 ms /    19 tokens (  730.86 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 86570.48 ms /   117 runs   (  739.92 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 100837.03 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.02 ms /   128 runs   (    0.86 ms per token,  1163.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16102.79 ms /    22 tokens (  731.95 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93334.82 ms /   127 runs   (  734.92 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 109842.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    26.65 ms /    31 runs   (    0.86 ms per token,  1163.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 60366.30 ms /   437 tokens (  138.14 ms per token,     7.24 tokens per second)\n",
      "llama_print_timings:        eval time = 22371.55 ms /    30 runs   (  745.72 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 82834.61 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.18 ms /   128 runs   (    0.86 ms per token,  1161.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11615.31 ms /    16 tokens (  725.96 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93330.37 ms /   127 runs   (  734.88 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105352.86 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    48.11 ms /    56 runs   (    0.86 ms per token,  1164.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12516.09 ms /    17 tokens (  736.24 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 40429.54 ms /    55 runs   (  735.08 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 53122.13 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14586.13 ms /    20 tokens (  729.31 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93397.99 ms /   127 runs   (  735.42 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108389.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    23.14 ms /    27 runs   (    0.86 ms per token,  1167.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 70084.27 ms /   484 tokens (  144.80 ms per token,     6.91 tokens per second)\n",
      "llama_print_timings:        eval time = 19442.32 ms /    26 runs   (  747.78 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 89611.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    41.90 ms /    47 runs   (    0.89 ms per token,  1121.80 tokens per second)\n",
      "llama_print_timings: prompt eval time = 60727.54 ms /   428 tokens (  141.89 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:        eval time = 35182.02 ms /    46 runs   (  764.83 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 96062.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    44.85 ms /    51 runs   (    0.88 ms per token,  1137.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64667.45 ms /   449 tokens (  144.03 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:        eval time = 37877.37 ms /    50 runs   (  757.55 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 102709.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    47.33 ms /    54 runs   (    0.88 ms per token,  1140.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 63707.96 ms /   446 tokens (  142.84 ms per token,     7.00 tokens per second)\n",
      "llama_print_timings:        eval time = 39932.58 ms /    53 runs   (  753.44 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 103814.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    81.68 ms /    95 runs   (    0.86 ms per token,  1163.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 55415.76 ms /   405 tokens (  136.83 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 70429.73 ms /    94 runs   (  749.25 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 126146.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    97.84 ms /   114 runs   (    0.86 ms per token,  1165.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51782.82 ms /   387 tokens (  133.81 ms per token,     7.47 tokens per second)\n",
      "llama_print_timings:        eval time = 84498.93 ms /   113 runs   (  747.78 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 136643.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    38.60 ms /    45 runs   (    0.86 ms per token,  1165.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65143.07 ms /   456 tokens (  142.86 ms per token,     7.00 tokens per second)\n",
      "llama_print_timings:        eval time = 32968.58 ms /    44 runs   (  749.29 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 98252.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   111.41 ms /   128 runs   (    0.87 ms per token,  1148.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47407.26 ms /   358 tokens (  132.42 ms per token,     7.55 tokens per second)\n",
      "llama_print_timings:        eval time = 95480.58 ms /   127 runs   (  751.82 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 143300.89 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.95 ms /   128 runs   (    0.86 ms per token,  1164.13 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18182.90 ms /    25 tokens (  727.32 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93773.48 ms /   127 runs   (  738.37 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 112361.64 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    47.31 ms /    55 runs   (    0.86 ms per token,  1162.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11752.22 ms /    16 tokens (  734.51 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 39557.48 ms /    54 runs   (  732.55 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 51482.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     1.73 ms /     2 runs   (    0.87 ms per token,  1154.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12536.09 ms /    17 tokens (  737.42 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time =   741.71 ms /     1 runs   (  741.71 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 13283.68 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     1.71 ms /     2 runs   (    0.86 ms per token,  1166.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14733.72 ms /    20 tokens (  736.69 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time =   728.70 ms /     1 runs   (  728.70 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 15468.14 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.05 ms /   128 runs   (    0.86 ms per token,  1163.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12513.44 ms /    17 tokens (  736.08 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93326.02 ms /   127 runs   (  734.85 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106246.52 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.49 ms /   128 runs   (    0.86 ms per token,  1158.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12606.42 ms /    17 tokens (  741.55 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 93126.27 ms /   127 runs   (  733.28 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106139.48 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     1.72 ms /     2 runs   (    0.86 ms per token,  1165.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18203.91 ms /    25 tokens (  728.16 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time =   728.39 ms /     1 runs   (  728.39 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 18938.54 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.96 ms /   128 runs   (    0.86 ms per token,  1164.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18264.72 ms /    25 tokens (  730.59 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93225.49 ms /   127 runs   (  734.06 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111895.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    70.32 ms /    82 runs   (    0.86 ms per token,  1166.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 59295.11 ms /   429 tokens (  138.22 ms per token,     7.23 tokens per second)\n",
      "llama_print_timings:        eval time = 60476.90 ms /    81 runs   (  746.63 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 120030.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    52.01 ms /    60 runs   (    0.87 ms per token,  1153.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62217.73 ms /   441 tokens (  141.08 ms per token,     7.09 tokens per second)\n",
      "llama_print_timings:        eval time = 44297.79 ms /    59 runs   (  750.81 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 106706.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    50.34 ms /    58 runs   (    0.87 ms per token,  1152.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62904.62 ms /   443 tokens (  142.00 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:        eval time = 42966.59 ms /    57 runs   (  753.80 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 106056.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    53.29 ms /    62 runs   (    0.86 ms per token,  1163.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62259.21 ms /   439 tokens (  141.82 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:        eval time = 45659.96 ms /    61 runs   (  748.52 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 108114.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.93 ms /   128 runs   (    0.86 ms per token,  1164.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47735.43 ms /   364 tokens (  131.14 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:        eval time = 94807.61 ms /   127 runs   (  746.52 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142950.53 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    43.86 ms /    51 runs   (    0.86 ms per token,  1162.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7254.36 ms /    10 tokens (  725.44 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 36792.61 ms /    50 runs   (  735.85 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 44209.43 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.93 ms /   128 runs   (    0.86 ms per token,  1164.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15319.52 ms /    21 tokens (  729.50 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93705.40 ms /   127 runs   (  737.84 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 109431.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    70.34 ms /    82 runs   (    0.86 ms per token,  1165.85 tokens per second)\n",
      "llama_print_timings: prompt eval time = 59141.29 ms /   429 tokens (  137.86 ms per token,     7.25 tokens per second)\n",
      "llama_print_timings:        eval time = 60523.36 ms /    81 runs   (  747.20 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 119922.93 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.98 ms /   128 runs   (    0.86 ms per token,  1163.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9499.98 ms /    13 tokens (  730.77 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93198.26 ms /   127 runs   (  733.84 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 103106.10 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.92 ms /   128 runs   (    0.86 ms per token,  1164.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14632.49 ms /    20 tokens (  731.62 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93331.35 ms /   127 runs   (  734.89 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108370.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    83.40 ms /    97 runs   (    0.86 ms per token,  1163.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56517.39 ms /   414 tokens (  136.52 ms per token,     7.33 tokens per second)\n",
      "llama_print_timings:        eval time = 71798.39 ms /    96 runs   (  747.90 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 128622.17 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.81 ms /   128 runs   (    0.86 ms per token,  1165.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5238.19 ms /    38 tokens (  137.85 ms per token,     7.25 tokens per second)\n",
      "llama_print_timings:        eval time = 93527.76 ms /   127 runs   (  736.44 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 99172.02 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.04 ms /   128 runs   (    0.86 ms per token,  1163.21 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13874.14 ms /    19 tokens (  730.22 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93506.58 ms /   127 runs   (  736.27 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107787.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    32.82 ms /    38 runs   (    0.86 ms per token,  1157.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68217.34 ms /   473 tokens (  144.22 ms per token,     6.93 tokens per second)\n",
      "llama_print_timings:        eval time = 27708.74 ms /    37 runs   (  748.88 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 96045.20 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.12 ms /   128 runs   (    0.86 ms per token,  1162.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4944.82 ms /    36 tokens (  137.36 ms per token,     7.28 tokens per second)\n",
      "llama_print_timings:        eval time = 93554.96 ms /   127 runs   (  736.65 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98907.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    13.69 ms /    16 runs   (    0.86 ms per token,  1169.08 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72629.88 ms /   495 tokens (  146.73 ms per token,     6.82 tokens per second)\n",
      "llama_print_timings:        eval time = 11264.87 ms /    15 runs   (  750.99 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 83945.20 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.86 ms /   128 runs   (    0.87 ms per token,  1154.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4622.99 ms /    32 tokens (  144.47 ms per token,     6.92 tokens per second)\n",
      "llama_print_timings:        eval time = 93916.54 ms /   127 runs   (  739.50 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 98948.52 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.15 ms /   128 runs   (    0.86 ms per token,  1162.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17582.80 ms /    24 tokens (  732.62 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93491.84 ms /   127 runs   (  736.16 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111481.59 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.93 ms /   128 runs   (    0.86 ms per token,  1164.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19623.40 ms /    27 tokens (  726.79 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93435.70 ms /   127 runs   (  735.71 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 113464.87 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.64 ms /   128 runs   (    0.86 ms per token,  1167.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19606.32 ms /    27 tokens (  726.16 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93375.08 ms /   127 runs   (  735.24 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 113385.76 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.97 ms /   128 runs   (    0.86 ms per token,  1163.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4628.74 ms /    32 tokens (  144.65 ms per token,     6.91 tokens per second)\n",
      "llama_print_timings:        eval time = 93377.36 ms /   127 runs   (  735.25 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98411.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.12 ms /   128 runs   (    0.86 ms per token,  1162.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48625.66 ms /   372 tokens (  130.71 ms per token,     7.65 tokens per second)\n",
      "llama_print_timings:        eval time = 94833.77 ms /   127 runs   (  746.72 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143867.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    26.61 ms /    31 runs   (    0.86 ms per token,  1165.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67568.08 ms /   469 tokens (  144.07 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:        eval time = 22461.22 ms /    30 runs   (  748.71 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 90126.25 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.11 ms /   128 runs   (    0.86 ms per token,  1162.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13915.55 ms /    19 tokens (  732.40 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93282.30 ms /   127 runs   (  734.51 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107604.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    18.88 ms /    22 runs   (    0.86 ms per token,  1165.13 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71345.18 ms /   489 tokens (  145.90 ms per token,     6.85 tokens per second)\n",
      "llama_print_timings:        eval time = 15713.06 ms /    21 runs   (  748.24 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 87127.35 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.65 ms /   128 runs   (    0.86 ms per token,  1156.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4581.56 ms /    32 tokens (  143.17 ms per token,     6.98 tokens per second)\n",
      "llama_print_timings:        eval time = 93955.47 ms /   127 runs   (  739.81 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 98945.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     5.14 ms /     6 runs   (    0.86 ms per token,  1167.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74206.30 ms /   505 tokens (  146.94 ms per token,     6.81 tokens per second)\n",
      "llama_print_timings:        eval time =  3768.89 ms /     5 runs   (  753.78 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 77994.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.99 ms /   128 runs   (    0.86 ms per token,  1163.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20328.33 ms /    28 tokens (  726.01 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93498.77 ms /   127 runs   (  736.21 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 114233.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    31.91 ms /    37 runs   (    0.86 ms per token,  1159.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68072.51 ms /   474 tokens (  143.61 ms per token,     6.96 tokens per second)\n",
      "llama_print_timings:        eval time = 27001.74 ms /    36 runs   (  750.05 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 95190.53 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.07 ms /   128 runs   (    0.86 ms per token,  1162.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16833.21 ms /    23 tokens (  731.88 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93487.10 ms /   127 runs   (  736.12 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 110727.77 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   111.36 ms /   128 runs   (    0.87 ms per token,  1149.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19674.38 ms /    27 tokens (  728.68 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 96757.61 ms /   127 runs   (  761.87 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 116847.60 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.76 ms /   128 runs   (    0.86 ms per token,  1166.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17690.48 ms /    24 tokens (  737.10 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93294.97 ms /   127 runs   (  734.61 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111396.42 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.81 ms /   128 runs   (    0.87 ms per token,  1155.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17568.81 ms /    24 tokens (  732.03 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93392.74 ms /   127 runs   (  735.38 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111376.77 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.16 ms /   128 runs   (    0.86 ms per token,  1161.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21060.45 ms /    29 tokens (  726.22 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93356.91 ms /   127 runs   (  735.09 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 114800.25 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    42.05 ms /    49 runs   (    0.86 ms per token,  1165.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18900.44 ms /    26 tokens (  726.94 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 35244.56 ms /    48 runs   (  734.26 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 54290.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    15.50 ms /    18 runs   (    0.86 ms per token,  1161.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72028.10 ms /   493 tokens (  146.10 ms per token,     6.84 tokens per second)\n",
      "llama_print_timings:        eval time = 12717.71 ms /    17 runs   (  748.10 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 84799.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    41.29 ms /    46 runs   (    0.90 ms per token,  1114.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65883.25 ms /   455 tokens (  144.80 ms per token,     6.91 tokens per second)\n",
      "llama_print_timings:        eval time = 34537.88 ms /    45 runs   (  767.51 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 100565.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    39.18 ms /    44 runs   (    0.89 ms per token,  1123.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 66060.80 ms /   457 tokens (  144.55 ms per token,     6.92 tokens per second)\n",
      "llama_print_timings:        eval time = 32411.33 ms /    43 runs   (  753.75 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 98606.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    69.80 ms /    81 runs   (    0.86 ms per token,  1160.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58462.30 ms /   420 tokens (  139.20 ms per token,     7.18 tokens per second)\n",
      "llama_print_timings:        eval time = 59783.02 ms /    80 runs   (  747.29 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 118488.57 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.99 ms /   128 runs   (    0.86 ms per token,  1163.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18154.97 ms /    25 tokens (  726.20 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93350.15 ms /   127 runs   (  735.04 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111890.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    93.98 ms /   109 runs   (    0.86 ms per token,  1159.85 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54135.39 ms /   402 tokens (  134.67 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:        eval time = 80746.63 ms /   108 runs   (  747.65 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 135211.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.11 ms /   128 runs   (    0.86 ms per token,  1162.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4967.80 ms /    35 tokens (  141.94 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:        eval time = 93332.29 ms /   127 runs   (  734.90 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98686.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     6.86 ms /     8 runs   (    0.86 ms per token,  1166.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74207.16 ms /   503 tokens (  147.53 ms per token,     6.78 tokens per second)\n",
      "llama_print_timings:        eval time =  5345.80 ms /     7 runs   (  763.69 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 79576.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    14.16 ms /    16 runs   (    0.89 ms per token,  1129.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71743.15 ms /   482 tokens (  148.84 ms per token,     6.72 tokens per second)\n",
      "llama_print_timings:        eval time = 11574.04 ms /    15 runs   (  771.60 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 83366.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    59.21 ms /    64 runs   (    0.93 ms per token,  1080.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62983.60 ms /   437 tokens (  144.13 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:        eval time = 48781.47 ms /    63 runs   (  774.31 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 111970.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     9.43 ms /    11 runs   (    0.86 ms per token,  1166.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72337.54 ms /   489 tokens (  147.93 ms per token,     6.76 tokens per second)\n",
      "llama_print_timings:        eval time =  7472.44 ms /    10 runs   (  747.24 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 79842.91 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   112.26 ms /   128 runs   (    0.88 ms per token,  1140.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5551.66 ms /    42 tokens (  132.18 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:        eval time = 93959.08 ms /   127 runs   (  739.84 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 99905.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    18.12 ms /    21 runs   (    0.86 ms per token,  1158.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71281.27 ms /   490 tokens (  145.47 ms per token,     6.87 tokens per second)\n",
      "llama_print_timings:        eval time = 14957.20 ms /    20 runs   (  747.86 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 86301.19 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   111.28 ms /   128 runs   (    0.87 ms per token,  1150.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6347.47 ms /    52 tokens (  122.07 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:        eval time = 93735.94 ms /   127 runs   (  738.08 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 100476.46 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.46 ms /   128 runs   (    0.86 ms per token,  1158.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8630.42 ms /    74 tokens (  116.63 ms per token,     8.57 tokens per second)\n",
      "llama_print_timings:        eval time = 95878.65 ms /   127 runs   (  754.95 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 104901.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    11.13 ms /    13 runs   (    0.86 ms per token,  1168.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72934.93 ms /   498 tokens (  146.46 ms per token,     6.83 tokens per second)\n",
      "llama_print_timings:        eval time =  8998.97 ms /    12 runs   (  749.91 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 81972.45 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   112.00 ms /   128 runs   (    0.87 ms per token,  1142.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5590.07 ms /    42 tokens (  133.10 ms per token,     7.51 tokens per second)\n",
      "llama_print_timings:        eval time = 94231.30 ms /   127 runs   (  741.98 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 100216.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    16.33 ms /    19 runs   (    0.86 ms per token,  1163.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71555.12 ms /   492 tokens (  145.44 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:        eval time = 13467.13 ms /    18 runs   (  748.17 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 85079.44 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.29 ms /   128 runs   (    0.86 ms per token,  1160.62 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18151.34 ms /    25 tokens (  726.05 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93352.95 ms /   127 runs   (  735.06 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111894.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    26.66 ms /    31 runs   (    0.86 ms per token,  1162.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 69001.35 ms /   480 tokens (  143.75 ms per token,     6.96 tokens per second)\n",
      "llama_print_timings:        eval time = 22439.08 ms /    30 runs   (  747.97 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 91533.74 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.83 ms /   128 runs   (    0.86 ms per token,  1165.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19684.78 ms /    27 tokens (  729.07 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93348.12 ms /   127 runs   (  735.02 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 113421.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    29.21 ms /    34 runs   (    0.86 ms per token,  1164.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68849.92 ms /   477 tokens (  144.34 ms per token,     6.93 tokens per second)\n",
      "llama_print_timings:        eval time = 24768.73 ms /    33 runs   (  750.57 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 93721.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    81.42 ms /    89 runs   (    0.91 ms per token,  1093.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 57254.23 ms /   412 tokens (  138.97 ms per token,     7.20 tokens per second)\n",
      "llama_print_timings:        eval time = 67719.82 ms /    88 runs   (  769.54 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 125261.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    69.56 ms /    81 runs   (    0.86 ms per token,  1164.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58208.82 ms /   420 tokens (  138.59 ms per token,     7.22 tokens per second)\n",
      "llama_print_timings:        eval time = 59947.29 ms /    80 runs   (  749.34 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 118401.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     1.71 ms /     2 runs   (    0.85 ms per token,  1172.33 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74187.91 ms /   499 tokens (  148.67 ms per token,     6.73 tokens per second)\n",
      "llama_print_timings:        eval time =   745.60 ms /     1 runs   (  745.60 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 74939.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    79.37 ms /    88 runs   (    0.90 ms per token,  1108.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 57371.20 ms /   413 tokens (  138.91 ms per token,     7.20 tokens per second)\n",
      "llama_print_timings:        eval time = 66569.18 ms /    87 runs   (  765.16 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 124218.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    80.70 ms /    94 runs   (    0.86 ms per token,  1164.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56156.41 ms /   406 tokens (  138.32 ms per token,     7.23 tokens per second)\n",
      "llama_print_timings:        eval time = 69478.96 ms /    93 runs   (  747.09 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 125920.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    51.67 ms /    60 runs   (    0.86 ms per token,  1161.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61525.82 ms /   438 tokens (  140.47 ms per token,     7.12 tokens per second)\n",
      "llama_print_timings:        eval time = 44188.92 ms /    59 runs   (  748.96 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 105897.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   111.67 ms /   128 runs   (    0.87 ms per token,  1146.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49371.58 ms /   371 tokens (  133.08 ms per token,     7.51 tokens per second)\n",
      "llama_print_timings:        eval time = 95191.00 ms /   127 runs   (  749.54 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 144962.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    35.20 ms /    41 runs   (    0.86 ms per token,  1164.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65799.26 ms /   459 tokens (  143.35 ms per token,     6.98 tokens per second)\n",
      "llama_print_timings:        eval time = 30060.93 ms /    40 runs   (  751.52 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 95984.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    59.83 ms /    67 runs   (    0.89 ms per token,  1119.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 60623.23 ms /   433 tokens (  140.01 ms per token,     7.14 tokens per second)\n",
      "llama_print_timings:        eval time = 50529.76 ms /    66 runs   (  765.60 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 111363.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.31 ms /   128 runs   (    0.86 ms per token,  1160.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9453.04 ms /    13 tokens (  727.16 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93366.91 ms /   127 runs   (  735.17 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 103218.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    52.67 ms /    61 runs   (    0.86 ms per token,  1158.13 tokens per second)\n",
      "llama_print_timings: prompt eval time = 63257.47 ms /   450 tokens (  140.57 ms per token,     7.11 tokens per second)\n",
      "llama_print_timings:        eval time = 44857.30 ms /    60 runs   (  747.62 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 108300.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.28 ms /   128 runs   (    0.86 ms per token,  1160.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14644.55 ms /    20 tokens (  732.23 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93250.24 ms /   127 runs   (  734.25 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108287.23 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   108.60 ms /   126 runs   (    0.86 ms per token,  1160.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13905.21 ms /    19 tokens (  731.85 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 91814.46 ms /   125 runs   (  734.52 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106106.48 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.04 ms /   128 runs   (    0.86 ms per token,  1163.21 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22552.28 ms /    31 tokens (  727.49 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93359.22 ms /   127 runs   (  735.11 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 116303.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    14.56 ms /    17 runs   (    0.86 ms per token,  1167.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72214.69 ms /   494 tokens (  146.18 ms per token,     6.84 tokens per second)\n",
      "llama_print_timings:        eval time = 11965.36 ms /    16 runs   (  747.84 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 84231.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    45.64 ms /    50 runs   (    0.91 ms per token,  1095.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64855.41 ms /   451 tokens (  143.80 ms per token,     6.95 tokens per second)\n",
      "llama_print_timings:        eval time = 37809.43 ms /    49 runs   (  771.62 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 102827.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    15.11 ms /    17 runs   (    0.89 ms per token,  1124.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71613.49 ms /   484 tokens (  147.96 ms per token,     6.76 tokens per second)\n",
      "llama_print_timings:        eval time = 12096.72 ms /    16 runs   (  756.05 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 83762.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    67.71 ms /    74 runs   (    0.92 ms per token,  1092.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 60198.69 ms /   427 tokens (  140.98 ms per token,     7.09 tokens per second)\n",
      "llama_print_timings:        eval time = 56240.95 ms /    73 runs   (  770.42 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 116679.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    60.18 ms /    69 runs   (    0.87 ms per token,  1146.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 60541.34 ms /   432 tokens (  140.14 ms per token,     7.14 tokens per second)\n",
      "llama_print_timings:        eval time = 53670.39 ms /    68 runs   (  789.27 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 114430.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    11.23 ms /    13 runs   (    0.86 ms per token,  1158.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74332.98 ms /   484 tokens (  153.58 ms per token,     6.51 tokens per second)\n",
      "llama_print_timings:        eval time =  9432.23 ms /    12 runs   (  786.02 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 83805.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    65.98 ms /    71 runs   (    0.93 ms per token,  1076.08 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64497.44 ms /   429 tokens (  150.34 ms per token,     6.65 tokens per second)\n",
      "llama_print_timings:        eval time = 57093.90 ms /    70 runs   (  815.63 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 121834.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    60.50 ms /    67 runs   (    0.90 ms per token,  1107.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64009.57 ms /   432 tokens (  148.17 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time = 53035.63 ms /    66 runs   (  803.57 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time = 117263.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    99.48 ms /   112 runs   (    0.89 ms per token,  1125.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 55054.10 ms /   389 tokens (  141.53 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:        eval time = 87771.78 ms /   111 runs   (  790.74 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 143189.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    46.55 ms /    54 runs   (    0.86 ms per token,  1160.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50567.29 ms /   366 tokens (  138.16 ms per token,     7.24 tokens per second)\n",
      "llama_print_timings:        eval time = 41246.19 ms /    53 runs   (  778.23 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 91981.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    71.68 ms /    80 runs   (    0.90 ms per token,  1116.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61552.16 ms /   420 tokens (  146.55 ms per token,     6.82 tokens per second)\n",
      "llama_print_timings:        eval time = 64472.16 ms /    79 runs   (  816.10 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 126284.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    74.30 ms /    49 runs   (    1.52 ms per token,   659.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 229835.46 ms /   452 tokens (  508.49 ms per token,     1.97 tokens per second)\n",
      "llama_print_timings:        eval time = 1733921.69 ms /    48 runs   (36123.37 ms per token,     0.03 tokens per second)\n",
      "llama_print_timings:       total time = 1964028.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    40.51 ms /    47 runs   (    0.86 ms per token,  1160.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 63171.94 ms /   437 tokens (  144.56 ms per token,     6.92 tokens per second)\n",
      "llama_print_timings:        eval time = 34571.91 ms /    46 runs   (  751.56 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 97888.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    43.62 ms /    49 runs   (    0.89 ms per token,  1123.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 66007.22 ms /   450 tokens (  146.68 ms per token,     6.82 tokens per second)\n",
      "llama_print_timings:        eval time = 36825.98 ms /    48 runs   (  767.21 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 102988.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    27.91 ms /    32 runs   (    0.87 ms per token,  1146.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68427.75 ms /   469 tokens (  145.90 ms per token,     6.85 tokens per second)\n",
      "llama_print_timings:        eval time = 23418.25 ms /    31 runs   (  755.43 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 91944.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    13.08 ms /    15 runs   (    0.87 ms per token,  1146.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72028.99 ms /   486 tokens (  148.21 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time = 10632.80 ms /    14 runs   (  759.49 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 82708.20 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.97 ms /   128 runs   (    0.87 ms per token,  1153.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8071.73 ms /    11 tokens (  733.79 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93831.01 ms /   127 runs   (  738.83 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 102309.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    11.16 ms /    13 runs   (    0.86 ms per token,  1165.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72916.22 ms /   498 tokens (  146.42 ms per token,     6.83 tokens per second)\n",
      "llama_print_timings:        eval time =  8987.54 ms /    12 runs   (  748.96 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 81942.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    30.90 ms /    35 runs   (    0.88 ms per token,  1132.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67975.83 ms /   466 tokens (  145.87 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:        eval time = 26098.77 ms /    34 runs   (  767.61 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 94184.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    60.68 ms /    69 runs   (    0.88 ms per token,  1137.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61269.58 ms /   432 tokens (  141.83 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:        eval time = 51485.49 ms /    68 runs   (  757.14 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 112971.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    12.89 ms /    15 runs   (    0.86 ms per token,  1163.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71470.70 ms /   486 tokens (  147.06 ms per token,     6.80 tokens per second)\n",
      "llama_print_timings:        eval time = 10546.50 ms /    14 runs   (  753.32 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 82063.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    35.11 ms /    39 runs   (    0.90 ms per token,  1110.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67419.99 ms /   462 tokens (  145.93 ms per token,     6.85 tokens per second)\n",
      "llama_print_timings:        eval time = 29187.12 ms /    38 runs   (  768.08 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 96731.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     2.56 ms /     3 runs   (    0.85 ms per token,  1170.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74453.30 ms /   498 tokens (  149.50 ms per token,     6.69 tokens per second)\n",
      "llama_print_timings:        eval time =  1506.22 ms /     2 runs   (  753.11 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 75968.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    44.58 ms /    49 runs   (    0.91 ms per token,  1099.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65821.15 ms /   452 tokens (  145.62 ms per token,     6.87 tokens per second)\n",
      "llama_print_timings:        eval time = 37356.78 ms /    48 runs   (  778.27 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 103336.27 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    95.31 ms /   110 runs   (    0.87 ms per token,  1154.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8813.70 ms /    12 tokens (  734.48 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 80390.90 ms /   109 runs   (  737.53 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 89546.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    43.71 ms /    51 runs   (    0.86 ms per token,  1166.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65125.39 ms /   460 tokens (  141.58 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:        eval time = 37414.93 ms /    50 runs   (  748.30 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 102696.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     4.48 ms /     5 runs   (    0.90 ms per token,  1115.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73856.58 ms /   495 tokens (  149.21 ms per token,     6.70 tokens per second)\n",
      "llama_print_timings:        eval time =  3032.91 ms /     4 runs   (  758.23 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 76904.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    86.77 ms /    95 runs   (    0.91 ms per token,  1094.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56799.04 ms /   406 tokens (  139.90 ms per token,     7.15 tokens per second)\n",
      "llama_print_timings:        eval time = 72419.31 ms /    94 runs   (  770.42 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 129531.40 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.43 ms /   128 runs   (    0.86 ms per token,  1159.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15355.63 ms /    21 tokens (  731.22 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 95256.42 ms /   127 runs   (  750.05 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 111011.07 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13905.49 ms /    19 tokens (  731.87 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93478.22 ms /   127 runs   (  736.05 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107781.53 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    87.53 ms /   100 runs   (    0.88 ms per token,  1142.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8051.77 ms /    11 tokens (  731.98 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 74336.63 ms /    99 runs   (  750.88 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 82704.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    11.22 ms /    13 runs   (    0.86 ms per token,  1158.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73572.79 ms /   498 tokens (  147.74 ms per token,     6.77 tokens per second)\n",
      "llama_print_timings:        eval time =  9243.44 ms /    12 runs   (  770.29 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 82855.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   116.84 ms /   128 runs   (    0.91 ms per token,  1095.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49785.44 ms /   365 tokens (  136.40 ms per token,     7.33 tokens per second)\n",
      "llama_print_timings:        eval time = 98629.00 ms /   127 runs   (  776.61 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 148844.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    66.48 ms /    72 runs   (    0.92 ms per token,  1082.97 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62664.79 ms /   429 tokens (  146.07 ms per token,     6.85 tokens per second)\n",
      "llama_print_timings:        eval time = 65220.92 ms /    71 runs   (  918.60 ms per token,     1.09 tokens per second)\n",
      "llama_print_timings:       total time = 128129.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    73.08 ms /    82 runs   (    0.89 ms per token,  1122.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61869.86 ms /   419 tokens (  147.66 ms per token,     6.77 tokens per second)\n",
      "llama_print_timings:        eval time = 62956.31 ms /    81 runs   (  777.24 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 125093.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    17.24 ms /    20 runs   (    0.86 ms per token,  1160.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56807.11 ms /   405 tokens (  140.26 ms per token,     7.13 tokens per second)\n",
      "llama_print_timings:        eval time = 14212.68 ms /    19 runs   (  748.04 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 71082.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    80.98 ms /    91 runs   (    0.89 ms per token,  1123.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58161.48 ms /   410 tokens (  141.86 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:        eval time = 71973.23 ms /    90 runs   (  799.70 ms per token,     1.25 tokens per second)\n",
      "llama_print_timings:       total time = 130432.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    45.95 ms /    53 runs   (    0.87 ms per token,  1153.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61023.94 ms /   417 tokens (  146.34 ms per token,     6.83 tokens per second)\n",
      "llama_print_timings:        eval time = 40119.48 ms /    52 runs   (  771.53 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 101312.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    15.64 ms /    18 runs   (    0.87 ms per token,  1151.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61051.88 ms /   420 tokens (  145.36 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:        eval time = 13228.26 ms /    17 runs   (  778.13 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 74337.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    49.91 ms /    54 runs   (    0.92 ms per token,  1081.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67497.75 ms /   446 tokens (  151.34 ms per token,     6.61 tokens per second)\n",
      "llama_print_timings:        eval time = 43138.10 ms /    53 runs   (  813.93 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time = 110819.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    26.32 ms /    30 runs   (    0.88 ms per token,  1139.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 69520.55 ms /   471 tokens (  147.60 ms per token,     6.77 tokens per second)\n",
      "llama_print_timings:        eval time = 21891.22 ms /    29 runs   (  754.87 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 91505.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    34.68 ms /    39 runs   (    0.89 ms per token,  1124.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67607.84 ms /   462 tokens (  146.34 ms per token,     6.83 tokens per second)\n",
      "llama_print_timings:        eval time = 29524.31 ms /    38 runs   (  776.96 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 97256.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    15.14 ms /    17 runs   (    0.89 ms per token,  1122.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72940.61 ms /   483 tokens (  151.02 ms per token,     6.62 tokens per second)\n",
      "llama_print_timings:        eval time = 12280.67 ms /    16 runs   (  767.54 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 85275.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    49.52 ms /    54 runs   (    0.92 ms per token,  1090.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65939.43 ms /   445 tokens (  148.18 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time = 41449.47 ms /    53 runs   (  782.07 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 107569.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    24.61 ms /    28 runs   (    0.88 ms per token,  1137.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71371.69 ms /   473 tokens (  150.89 ms per token,     6.63 tokens per second)\n",
      "llama_print_timings:        eval time = 21242.64 ms /    27 runs   (  786.76 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 92703.11 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   111.17 ms /   128 runs   (    0.87 ms per token,  1151.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12945.89 ms /    17 tokens (  761.52 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:        eval time = 94713.37 ms /   127 runs   (  745.77 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 108070.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    33.58 ms /    39 runs   (    0.86 ms per token,  1161.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68435.57 ms /   472 tokens (  144.99 ms per token,     6.90 tokens per second)\n",
      "llama_print_timings:        eval time = 28586.38 ms /    38 runs   (  752.27 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 97141.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    16.95 ms /    19 runs   (    0.89 ms per token,  1120.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71289.07 ms /   481 tokens (  148.21 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time = 13793.23 ms /    18 runs   (  766.29 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 85143.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    33.34 ms /    37 runs   (    0.90 ms per token,  1109.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68957.91 ms /   463 tokens (  148.94 ms per token,     6.71 tokens per second)\n",
      "llama_print_timings:        eval time = 27954.96 ms /    36 runs   (  776.53 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 97032.42 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   113.57 ms /   128 runs   (    0.89 ms per token,  1127.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11074.76 ms /    15 tokens (  738.32 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 94336.81 ms /   127 runs   (  742.81 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 105821.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    29.38 ms /    34 runs   (    0.86 ms per token,  1157.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 69571.90 ms /   477 tokens (  145.85 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:        eval time = 25990.36 ms /    33 runs   (  787.59 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 95668.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    11.68 ms /    13 runs   (    0.90 ms per token,  1113.39 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74486.01 ms /   487 tokens (  152.95 ms per token,     6.54 tokens per second)\n",
      "llama_print_timings:        eval time =  9223.93 ms /    12 runs   (  768.66 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 83752.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     8.85 ms /    10 runs   (    0.89 ms per token,  1129.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74252.84 ms /   491 tokens (  151.23 ms per token,     6.61 tokens per second)\n",
      "llama_print_timings:        eval time =  6948.83 ms /     9 runs   (  772.09 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 81233.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    90.58 ms /    99 runs   (    0.91 ms per token,  1092.97 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56868.71 ms /   402 tokens (  141.46 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:        eval time = 76158.29 ms /    98 runs   (  777.13 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 133360.18 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.31 ms /   128 runs   (    0.86 ms per token,  1160.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16177.67 ms /    22 tokens (  735.35 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93626.03 ms /   127 runs   (  737.21 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 110210.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    16.42 ms /    19 runs   (    0.86 ms per token,  1157.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72478.47 ms /   492 tokens (  147.31 ms per token,     6.79 tokens per second)\n",
      "llama_print_timings:        eval time = 13709.16 ms /    18 runs   (  761.62 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 86246.87 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.07 ms /   128 runs   (    0.86 ms per token,  1162.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18931.63 ms /    26 tokens (  728.14 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 94568.03 ms /   127 runs   (  744.63 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 113899.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    46.54 ms /    54 runs   (    0.86 ms per token,  1160.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64624.61 ms /   457 tokens (  141.41 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:        eval time = 39669.33 ms /    53 runs   (  748.48 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 104460.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    25.02 ms /    29 runs   (    0.86 ms per token,  1159.26 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68399.97 ms /   472 tokens (  144.92 ms per token,     6.90 tokens per second)\n",
      "llama_print_timings:        eval time = 21309.58 ms /    28 runs   (  761.06 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 89799.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    14.48 ms /    16 runs   (    0.90 ms per token,  1105.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74636.03 ms /   482 tokens (  154.85 ms per token,     6.46 tokens per second)\n",
      "llama_print_timings:        eval time = 11696.15 ms /    15 runs   (  779.74 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 86385.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =     6.98 ms /     8 runs   (    0.87 ms per token,  1145.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74733.56 ms /   493 tokens (  151.59 ms per token,     6.60 tokens per second)\n",
      "llama_print_timings:        eval time =  5368.82 ms /     7 runs   (  766.97 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 80127.61 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   112.52 ms /   128 runs   (    0.88 ms per token,  1137.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20885.10 ms /    28 tokens (  745.90 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:        eval time = 98066.62 ms /   127 runs   (  772.18 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 119364.97 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.69 ms /   128 runs   (    0.86 ms per token,  1156.39 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19654.81 ms /    27 tokens (  727.96 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93845.17 ms /   127 runs   (  738.94 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 113899.16 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    27.02 ms /    31 runs   (    0.87 ms per token,  1147.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22869.54 ms /    31 tokens (  737.73 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 23241.15 ms /    30 runs   (  774.70 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 46206.85 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.95 ms /   128 runs   (    0.87 ms per token,  1153.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19281.41 ms /    26 tokens (  741.59 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 98342.19 ms /   127 runs   (  774.35 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 118028.96 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.21 ms /   128 runs   (    0.86 ms per token,  1161.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15353.48 ms /    21 tokens (  731.12 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93722.13 ms /   127 runs   (  737.97 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 109475.10 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.17 ms /   128 runs   (    0.86 ms per token,  1161.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4962.57 ms /    33 tokens (  150.38 ms per token,     6.65 tokens per second)\n",
      "llama_print_timings:        eval time = 93894.57 ms /   127 runs   (  739.33 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 99256.77 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.63 ms /   128 runs   (    0.86 ms per token,  1157.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22566.75 ms /    31 tokens (  727.96 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 94849.93 ms /   127 runs   (  746.85 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 117822.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    81.03 ms /    93 runs   (    0.87 ms per token,  1147.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58964.05 ms /   418 tokens (  141.06 ms per token,     7.09 tokens per second)\n",
      "llama_print_timings:        eval time = 69340.80 ms /    92 runs   (  753.70 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 128604.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    76.73 ms /    89 runs   (    0.86 ms per token,  1159.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47822.69 ms /   348 tokens (  137.42 ms per token,     7.28 tokens per second)\n",
      "llama_print_timings:        eval time = 66102.25 ms /    88 runs   (  751.16 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 114208.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   102.77 ms /   119 runs   (    0.86 ms per token,  1157.89 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51450.47 ms /   382 tokens (  134.69 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:        eval time = 89149.44 ms /   118 runs   (  755.50 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 140976.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    91.08 ms /   106 runs   (    0.86 ms per token,  1163.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54055.48 ms /   395 tokens (  136.85 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 79007.18 ms /   105 runs   (  752.45 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 133392.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.20 ms /   128 runs   (    0.86 ms per token,  1161.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47840.66 ms /   362 tokens (  132.16 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:        eval time = 95168.37 ms /   127 runs   (  749.36 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 143409.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.93 ms /   128 runs   (    0.86 ms per token,  1164.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45070.95 ms /   348 tokens (  129.51 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 94937.12 ms /   127 runs   (  747.54 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140408.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    41.11 ms /    48 runs   (    0.86 ms per token,  1167.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48678.96 ms /   369 tokens (  131.92 ms per token,     7.58 tokens per second)\n",
      "llama_print_timings:        eval time = 34965.70 ms /    47 runs   (  743.95 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 83793.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    99.67 ms /   115 runs   (    0.87 ms per token,  1153.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51910.81 ms /   386 tokens (  134.48 ms per token,     7.44 tokens per second)\n",
      "llama_print_timings:        eval time = 86556.13 ms /   114 runs   (  759.26 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 138830.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    90.42 ms /   105 runs   (    0.86 ms per token,  1161.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47376.72 ms /   358 tokens (  132.34 ms per token,     7.56 tokens per second)\n",
      "llama_print_timings:        eval time = 77876.98 ms /   104 runs   (  748.82 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 125582.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    34.60 ms /    40 runs   (    0.86 ms per token,  1156.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53280.55 ms /   391 tokens (  136.27 ms per token,     7.34 tokens per second)\n",
      "llama_print_timings:        eval time = 29099.80 ms /    39 runs   (  746.15 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 82504.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    48.71 ms /    55 runs   (    0.89 ms per token,  1129.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 63640.62 ms /   446 tokens (  142.69 ms per token,     7.01 tokens per second)\n",
      "llama_print_timings:        eval time = 40809.68 ms /    54 runs   (  755.73 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 104625.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   107.73 ms /   125 runs   (    0.86 ms per token,  1160.29 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50945.84 ms /   376 tokens (  135.49 ms per token,     7.38 tokens per second)\n",
      "llama_print_timings:        eval time = 92812.33 ms /   124 runs   (  748.49 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 144153.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.57 ms /   128 runs   (    0.86 ms per token,  1157.65 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47403.88 ms /   354 tokens (  133.91 ms per token,     7.47 tokens per second)\n",
      "llama_print_timings:        eval time = 95204.37 ms /   127 runs   (  749.64 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 143012.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    81.94 ms /    95 runs   (    0.86 ms per token,  1159.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56705.79 ms /   406 tokens (  139.67 ms per token,     7.16 tokens per second)\n",
      "llama_print_timings:        eval time = 70712.09 ms /    94 runs   (  752.26 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 127716.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   127 runs   (    0.87 ms per token,  1153.61 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50609.34 ms /   371 tokens (  136.41 ms per token,     7.33 tokens per second)\n",
      "llama_print_timings:        eval time = 94953.06 ms /   126 runs   (  753.60 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 145963.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    81.24 ms /    94 runs   (    0.86 ms per token,  1157.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52602.69 ms /   387 tokens (  135.92 ms per token,     7.36 tokens per second)\n",
      "llama_print_timings:        eval time = 70097.52 ms /    93 runs   (  753.74 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 122994.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    61.76 ms /    70 runs   (    0.88 ms per token,  1133.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61521.35 ms /   430 tokens (  143.07 ms per token,     6.99 tokens per second)\n",
      "llama_print_timings:        eval time = 52680.43 ms /    69 runs   (  763.48 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 114425.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   112.29 ms /   128 runs   (    0.88 ms per token,  1139.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50712.29 ms /   369 tokens (  137.43 ms per token,     7.28 tokens per second)\n",
      "llama_print_timings:        eval time = 95871.52 ms /   127 runs   (  754.89 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 146995.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.07 ms /   128 runs   (    0.86 ms per token,  1162.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50611.80 ms /   370 tokens (  136.79 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 94953.24 ms /   127 runs   (  747.66 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 145968.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    80.14 ms /    93 runs   (    0.86 ms per token,  1160.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47932.22 ms /   357 tokens (  134.26 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:        eval time = 68700.71 ms /    92 runs   (  746.75 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 116926.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.73 ms /   128 runs   (    0.86 ms per token,  1166.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45929.74 ms /   347 tokens (  132.36 ms per token,     7.56 tokens per second)\n",
      "llama_print_timings:        eval time = 94854.00 ms /   127 runs   (  746.88 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141187.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    70.89 ms /    82 runs   (    0.86 ms per token,  1156.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 59282.25 ms /   419 tokens (  141.49 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:        eval time = 60760.04 ms /    81 runs   (  750.12 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 120301.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    73.61 ms /    83 runs   (    0.89 ms per token,  1127.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 60025.51 ms /   413 tokens (  145.34 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:        eval time = 62196.17 ms /    82 runs   (  758.49 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 122488.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    73.30 ms /    84 runs   (    0.87 ms per token,  1145.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 59036.56 ms /   417 tokens (  141.57 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:        eval time = 62488.07 ms /    83 runs   (  752.87 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 121791.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    93.26 ms /   107 runs   (    0.87 ms per token,  1147.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54966.89 ms /   394 tokens (  139.51 ms per token,     7.17 tokens per second)\n",
      "llama_print_timings:        eval time = 81711.39 ms /   106 runs   (  770.86 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 137021.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    77.70 ms /    90 runs   (    0.86 ms per token,  1158.33 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53838.53 ms /   391 tokens (  137.69 ms per token,     7.26 tokens per second)\n",
      "llama_print_timings:        eval time = 66917.63 ms /    89 runs   (  751.88 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 121039.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   100.74 ms /   117 runs   (    0.86 ms per token,  1161.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52642.53 ms /   383 tokens (  137.45 ms per token,     7.28 tokens per second)\n",
      "llama_print_timings:        eval time = 87138.58 ms /   116 runs   (  751.19 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 140148.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    86.01 ms /   100 runs   (    0.86 ms per token,  1162.61 tokens per second)\n",
      "llama_print_timings: prompt eval time = 55903.51 ms /   401 tokens (  139.41 ms per token,     7.17 tokens per second)\n",
      "llama_print_timings:        eval time = 74094.71 ms /    99 runs   (  748.43 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 130312.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.45 ms /   128 runs   (    0.86 ms per token,  1158.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49257.41 ms /   365 tokens (  134.95 ms per token,     7.41 tokens per second)\n",
      "llama_print_timings:        eval time = 95465.67 ms /   127 runs   (  751.70 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 145127.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    98.06 ms /   114 runs   (    0.86 ms per token,  1162.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53308.65 ms /   387 tokens (  137.75 ms per token,     7.26 tokens per second)\n",
      "llama_print_timings:        eval time = 84566.48 ms /   113 runs   (  748.38 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 138234.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   101.58 ms /   118 runs   (    0.86 ms per token,  1161.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52980.98 ms /   383 tokens (  138.33 ms per token,     7.23 tokens per second)\n",
      "llama_print_timings:        eval time = 87538.86 ms /   117 runs   (  748.20 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140891.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.30 ms /   128 runs   (    0.86 ms per token,  1160.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47524.70 ms /   355 tokens (  133.87 ms per token,     7.47 tokens per second)\n",
      "llama_print_timings:        eval time = 94918.17 ms /   127 runs   (  747.39 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142848.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.35 ms /   128 runs   (    0.86 ms per token,  1159.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49952.73 ms /   369 tokens (  135.37 ms per token,     7.39 tokens per second)\n",
      "llama_print_timings:        eval time = 94912.76 ms /   127 runs   (  747.34 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 145269.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   103.34 ms /   120 runs   (    0.86 ms per token,  1161.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52106.13 ms /   381 tokens (  136.76 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 89085.81 ms /   119 runs   (  748.62 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141570.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.31 ms /   128 runs   (    0.86 ms per token,  1160.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49731.56 ms /   367 tokens (  135.51 ms per token,     7.38 tokens per second)\n",
      "llama_print_timings:        eval time = 95113.61 ms /   127 runs   (  748.93 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 145253.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    52.32 ms /    61 runs   (    0.86 ms per token,  1165.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61787.97 ms /   440 tokens (  140.43 ms per token,     7.12 tokens per second)\n",
      "llama_print_timings:        eval time = 44992.37 ms /    60 runs   (  749.87 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 106970.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    90.77 ms /   105 runs   (    0.86 ms per token,  1156.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53727.04 ms /   396 tokens (  135.67 ms per token,     7.37 tokens per second)\n",
      "llama_print_timings:        eval time = 77933.19 ms /   104 runs   (  749.36 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 131991.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.99 ms /   128 runs   (    0.86 ms per token,  1163.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 41507.02 ms /   326 tokens (  127.32 ms per token,     7.85 tokens per second)\n",
      "llama_print_timings:        eval time = 94891.83 ms /   127 runs   (  747.18 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 136807.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   109.98 ms /   128 runs   (    0.86 ms per token,  1163.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 49397.41 ms /   371 tokens (  133.15 ms per token,     7.51 tokens per second)\n",
      "llama_print_timings:        eval time = 95423.20 ms /   127 runs   (  751.36 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 145223.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    85.03 ms /    99 runs   (    0.86 ms per token,  1164.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54782.83 ms /   402 tokens (  136.28 ms per token,     7.34 tokens per second)\n",
      "llama_print_timings:        eval time = 73291.10 ms /    98 runs   (  747.87 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 128383.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.20 ms /   128 runs   (    0.86 ms per token,  1161.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 44648.47 ms /   345 tokens (  129.42 ms per token,     7.73 tokens per second)\n",
      "llama_print_timings:        eval time = 95424.95 ms /   127 runs   (  751.38 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 140481.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   103.14 ms /   119 runs   (    0.87 ms per token,  1153.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53836.72 ms /   382 tokens (  140.93 ms per token,     7.10 tokens per second)\n",
      "llama_print_timings:        eval time = 90268.36 ms /   118 runs   (  764.99 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 144492.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    30.43 ms /    35 runs   (    0.87 ms per token,  1150.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68718.34 ms /   466 tokens (  147.46 ms per token,     6.78 tokens per second)\n",
      "llama_print_timings:        eval time = 25615.82 ms /    34 runs   (  753.41 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 94443.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   112.30 ms /   123 runs   (    0.91 ms per token,  1095.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53379.55 ms /   378 tokens (  141.22 ms per token,     7.08 tokens per second)\n",
      "llama_print_timings:        eval time = 95136.63 ms /   122 runs   (  779.81 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 148929.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.98 ms /   128 runs   (    0.87 ms per token,  1153.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42470.00 ms /   324 tokens (  131.08 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:        eval time = 98486.44 ms /   127 runs   (  775.48 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 141373.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    37.66 ms /    43 runs   (    0.88 ms per token,  1141.67 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68734.16 ms /   458 tokens (  150.07 ms per token,     6.66 tokens per second)\n",
      "llama_print_timings:        eval time = 31776.81 ms /    42 runs   (  756.59 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 100649.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   142.34 ms /   128 runs   (    1.11 ms per token,   899.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46489.47 ms /   340 tokens (  136.73 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 120290.38 ms /   127 runs   (  947.17 ms per token,     1.06 tokens per second)\n",
      "llama_print_timings:       total time = 167307.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   295.10 ms /   128 runs   (    2.31 ms per token,   433.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 98533.78 ms /   372 tokens (  264.88 ms per token,     3.78 tokens per second)\n",
      "llama_print_timings:        eval time = 276103.42 ms /   127 runs   ( 2174.04 ms per token,     0.46 tokens per second)\n",
      "llama_print_timings:       total time = 375744.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   317.75 ms /   123 runs   (    2.58 ms per token,   387.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 142149.25 ms /   377 tokens (  377.05 ms per token,     2.65 tokens per second)\n",
      "llama_print_timings:        eval time = 311975.28 ms /   122 runs   ( 2557.17 ms per token,     0.39 tokens per second)\n",
      "llama_print_timings:       total time = 455322.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   232.69 ms /    94 runs   (    2.48 ms per token,   403.97 tokens per second)\n",
      "llama_print_timings: prompt eval time = 134572.29 ms /   365 tokens (  368.69 ms per token,     2.71 tokens per second)\n",
      "llama_print_timings:        eval time = 225658.69 ms /    93 runs   ( 2426.44 ms per token,     0.41 tokens per second)\n",
      "llama_print_timings:       total time = 361108.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   253.07 ms /   102 runs   (    2.48 ms per token,   403.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 139444.55 ms /   399 tokens (  349.49 ms per token,     2.86 tokens per second)\n",
      "llama_print_timings:        eval time = 245162.92 ms /   101 runs   ( 2427.36 ms per token,     0.41 tokens per second)\n",
      "llama_print_timings:       total time = 385555.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   323.45 ms /   128 runs   (    2.53 ms per token,   395.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 125636.92 ms /   362 tokens (  347.06 ms per token,     2.88 tokens per second)\n",
      "llama_print_timings:        eval time = 315576.16 ms /   127 runs   ( 2484.85 ms per token,     0.40 tokens per second)\n",
      "llama_print_timings:       total time = 442422.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   229.36 ms /    94 runs   (    2.44 ms per token,   409.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 142479.06 ms /   406 tokens (  350.93 ms per token,     2.85 tokens per second)\n",
      "llama_print_timings:        eval time = 217827.65 ms /    93 runs   ( 2342.23 ms per token,     0.43 tokens per second)\n",
      "llama_print_timings:       total time = 361149.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   263.88 ms /   101 runs   (    2.61 ms per token,   382.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 125372.78 ms /   373 tokens (  336.12 ms per token,     2.98 tokens per second)\n",
      "llama_print_timings:        eval time = 262199.18 ms /   100 runs   ( 2621.99 ms per token,     0.38 tokens per second)\n",
      "llama_print_timings:       total time = 388567.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   197.59 ms /    78 runs   (    2.53 ms per token,   394.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 150213.58 ms /   379 tokens (  396.34 ms per token,     2.52 tokens per second)\n",
      "llama_print_timings:        eval time = 186167.32 ms /    77 runs   ( 2417.76 ms per token,     0.41 tokens per second)\n",
      "llama_print_timings:       total time = 337108.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   117.72 ms /    51 runs   (    2.31 ms per token,   433.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 147921.05 ms /   450 tokens (  328.71 ms per token,     3.04 tokens per second)\n",
      "llama_print_timings:        eval time = 103886.99 ms /    50 runs   ( 2077.74 ms per token,     0.48 tokens per second)\n",
      "llama_print_timings:       total time = 252232.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   296.95 ms /   128 runs   (    2.32 ms per token,   431.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 107768.16 ms /   353 tokens (  305.29 ms per token,     3.28 tokens per second)\n",
      "llama_print_timings:        eval time = 266732.43 ms /   127 runs   ( 2100.26 ms per token,     0.48 tokens per second)\n",
      "llama_print_timings:       total time = 375597.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   152.03 ms /    76 runs   (    2.00 ms per token,   499.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 122887.79 ms /   425 tokens (  289.15 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:        eval time = 129906.70 ms /    75 runs   ( 1732.09 ms per token,     0.58 tokens per second)\n",
      "llama_print_timings:       total time = 253368.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.51 ms /   128 runs   (    0.86 ms per token,  1158.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 127344.80 ms /   367 tokens (  346.99 ms per token,     2.88 tokens per second)\n",
      "llama_print_timings:        eval time = 95824.55 ms /   127 runs   (  754.52 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 223586.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    81.62 ms /    94 runs   (    0.87 ms per token,  1151.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 55741.02 ms /   407 tokens (  136.96 ms per token,     7.30 tokens per second)\n",
      "llama_print_timings:        eval time = 69894.89 ms /    93 runs   (  751.56 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 125936.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   100.76 ms /   117 runs   (    0.86 ms per token,  1161.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51124.41 ms /   384 tokens (  133.14 ms per token,     7.51 tokens per second)\n",
      "llama_print_timings:        eval time = 87105.24 ms /   116 runs   (  750.91 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 138599.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.06 ms /   128 runs   (    0.86 ms per token,  1163.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48771.35 ms /   370 tokens (  131.81 ms per token,     7.59 tokens per second)\n",
      "llama_print_timings:        eval time = 94908.38 ms /   127 runs   (  747.31 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 144084.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    99.67 ms /   116 runs   (    0.86 ms per token,  1163.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51528.30 ms /   385 tokens (  133.84 ms per token,     7.47 tokens per second)\n",
      "llama_print_timings:        eval time = 86316.71 ms /   115 runs   (  750.58 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 138210.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    58.11 ms /    67 runs   (    0.87 ms per token,  1152.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 60767.99 ms /   434 tokens (  140.02 ms per token,     7.14 tokens per second)\n",
      "llama_print_timings:        eval time = 49580.83 ms /    66 runs   (  751.22 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 110561.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    25.45 ms /    29 runs   (    0.88 ms per token,  1139.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68718.27 ms /   472 tokens (  145.59 ms per token,     6.87 tokens per second)\n",
      "llama_print_timings:        eval time = 21304.54 ms /    28 runs   (  760.88 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 90116.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   112.04 ms /   128 runs   (    0.88 ms per token,  1142.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46172.69 ms /   348 tokens (  132.68 ms per token,     7.54 tokens per second)\n",
      "llama_print_timings:        eval time = 95582.60 ms /   127 runs   (  752.62 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 142171.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   106.62 ms /   124 runs   (    0.86 ms per token,  1162.97 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50149.60 ms /   377 tokens (  133.02 ms per token,     7.52 tokens per second)\n",
      "llama_print_timings:        eval time = 92016.32 ms /   123 runs   (  748.10 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142557.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.06 ms /   128 runs   (    0.86 ms per token,  1162.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47413.38 ms /   363 tokens (  130.62 ms per token,     7.66 tokens per second)\n",
      "llama_print_timings:        eval time = 94923.42 ms /   127 runs   (  747.43 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142741.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.02 ms /   128 runs   (    0.86 ms per token,  1163.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48574.83 ms /   368 tokens (  132.00 ms per token,     7.58 tokens per second)\n",
      "llama_print_timings:        eval time = 94723.12 ms /   127 runs   (  745.85 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143702.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    36.43 ms /    42 runs   (    0.87 ms per token,  1152.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52402.95 ms /   392 tokens (  133.68 ms per token,     7.48 tokens per second)\n",
      "llama_print_timings:        eval time = 30624.91 ms /    41 runs   (  746.95 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 83160.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.46 ms /   128 runs   (    0.86 ms per token,  1158.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42635.28 ms /   333 tokens (  128.03 ms per token,     7.81 tokens per second)\n",
      "llama_print_timings:        eval time = 95137.58 ms /   127 runs   (  749.11 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 138183.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =    95.53 ms /   111 runs   (    0.86 ms per token,  1162.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52281.78 ms /   390 tokens (  134.06 ms per token,     7.46 tokens per second)\n",
      "llama_print_timings:        eval time = 82131.54 ms /   110 runs   (  746.65 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 134763.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 64510.40 ms\n",
      "llama_print_timings:      sample time =   110.13 ms /   128 runs   (    0.86 ms per token,  1162.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47696.28 ms /   364 tokens (  131.03 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:        eval time = 95028.41 ms /   127 runs   (  748.26 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143130.39 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "responses = []\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    question = row['Question Text']\n",
    "    id = row['ID']\n",
    "    df_book_results = search_content(query=question, df_sentances=df_docs,\n",
    "                                       book_index=fastIndex_docs, embedder=embedding_model, k=3)\n",
    "\n",
    "    response = get_response(text=question, llm=llm_model, df_book_matches=df_book_results)\n",
    "\n",
    "    book_info = get_paragraph_info(query=question, df_booklet=df_booklet, embedder=embedding_model, fastIndex=fastIndex_book)\n",
    "\n",
    "    response.update(book_info)\n",
    "    response[\"keywords\"] = extract_keyword(str(df_book_results['text'].values), top_n=5)\n",
    "    df_responses = pd.DataFrame([response])\n",
    "    df_responses['ID'] = id\n",
    "    df_responses['Question'] = question\n",
    "    csv_file_path = pwd + \"/data/submissions/submission_v4_temp.csv\"\n",
    "    df_responses[['answer', 'book', 'paragraphs', 'keywords', 'ID', 'Question']].to_csv(csv_file_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses = pd.read_csv(pwd +  \"/data/submissions/submission_v4_temp.csv\")\n",
    "df_responses.columns = ['answer', 'book', 'paragraphs', 'keywords', 'ID', 'Question', 'None']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a view instances where no prediction was made. We will replace this with a standard response of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_default_answer(row):\n",
    "    if pd.isna(row['answer']):\n",
    "        df_book_results = search_content(query=row['Question'], df_sentances=df_docs,\n",
    "                                       book_index=fastIndex_docs, embedder=embedding_model, k=1)\n",
    "\n",
    "        paragraph_words = []\n",
    "\n",
    "        for paragraph in df_book_results['text'].values.tolist():\n",
    "            paragraph_words += paragraph.split(\" \")\n",
    "            \n",
    "        booklet_information = \" \".join(paragraph_words)\n",
    "\n",
    "        # remove unicode characters\n",
    "        unicode_pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "        cleaned_text = unicode_pattern.sub('', booklet_information)\n",
    "\n",
    "        answer = f\"The answer is in the following text: {cleaned_text}\"\n",
    "        return answer\n",
    "\n",
    "    else:\n",
    "        return row['answer']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses['final_answer'] = df_responses.apply(fill_default_answer, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submissoion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>book</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>keywords</th>\n",
       "      <th>ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>None</th>\n",
       "      <th>final_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An unusual event refers to any situation that ...</td>\n",
       "      <td>booklet1</td>\n",
       "      <td>313-423</td>\n",
       "      <td>['acute respiratory infection', 'acute respira...</td>\n",
       "      <td>Q4</td>\n",
       "      <td>What is the definition of \"unusual event\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An unusual event refers to any situation that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBS is a community-based surveillance system t...</td>\n",
       "      <td>booklet1</td>\n",
       "      <td>20-469</td>\n",
       "      <td>['Community Based Surveillance', 'subnational ...</td>\n",
       "      <td>Q5</td>\n",
       "      <td>What is Community Based Surveillance (CBS)?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CBS is a community-based surveillance system t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Training for VHCs should include the following...</td>\n",
       "      <td>booklet3</td>\n",
       "      <td>518-1687</td>\n",
       "      <td>['high absenteeism', 'absenteeism at school', ...</td>\n",
       "      <td>Q9</td>\n",
       "      <td>What kind of training should members of VHC re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Training for VHCs should include the following...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Indicator-based surveillance (IBS) is a system...</td>\n",
       "      <td>booklet1</td>\n",
       "      <td>20-400</td>\n",
       "      <td>['Indicator-based surveillance', 'Surveillance...</td>\n",
       "      <td>Q10</td>\n",
       "      <td>What is indicator based surveillance (IBS)?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indicator-based surveillance (IBS) is a system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case-based surveillance involves ongoing and r...</td>\n",
       "      <td>booklet1</td>\n",
       "      <td>20-345</td>\n",
       "      <td>['Case-based', 'quarterly or annual', 'Case-ba...</td>\n",
       "      <td>Q13</td>\n",
       "      <td>What is Case based surveillance?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Case-based surveillance involves ongoing and r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>The completeness of surveillance data should b...</td>\n",
       "      <td>booklet4</td>\n",
       "      <td>29-227</td>\n",
       "      <td>['units submitted reports', 'reporting units s...</td>\n",
       "      <td>Q1229</td>\n",
       "      <td>Where should completeness be evaluated in the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The completeness of surveillance data should b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1. Completeness of recorded data in the patien...</td>\n",
       "      <td>booklet4</td>\n",
       "      <td>29-227</td>\n",
       "      <td>['units submitted reports', 'reporting units s...</td>\n",
       "      <td>Q1230</td>\n",
       "      <td>Which dimensions of completeness are crucial i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1. Completeness of recorded data in the patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>To monitor and improve the completeness of cas...</td>\n",
       "      <td>booklet1</td>\n",
       "      <td>223-345</td>\n",
       "      <td>['case record unique', 'record unique identifi...</td>\n",
       "      <td>Q1236</td>\n",
       "      <td>How can the completeness of case reporting be ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To monitor and improve the completeness of cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Monitoring the timeliness and completeness of ...</td>\n",
       "      <td>booklet2</td>\n",
       "      <td>609-610</td>\n",
       "      <td>['disease surveillance systems', 'disease surv...</td>\n",
       "      <td>Q1239</td>\n",
       "      <td>Where should completeness and timeliness of re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monitoring the timeliness and completeness of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Community-based surveillance contributes to th...</td>\n",
       "      <td>booklet1</td>\n",
       "      <td>18-224</td>\n",
       "      <td>['Improving Cross Border', 'Based Surveillance...</td>\n",
       "      <td>Q1246</td>\n",
       "      <td>How does community-based surveillance contribu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Community-based surveillance contributes to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                answer      book paragraphs  \\\n",
       "0    An unusual event refers to any situation that ...  booklet1    313-423   \n",
       "1    CBS is a community-based surveillance system t...  booklet1     20-469   \n",
       "2    Training for VHCs should include the following...  booklet3   518-1687   \n",
       "3    Indicator-based surveillance (IBS) is a system...  booklet1     20-400   \n",
       "4    Case-based surveillance involves ongoing and r...  booklet1     20-345   \n",
       "..                                                 ...       ...        ...   \n",
       "494  The completeness of surveillance data should b...  booklet4     29-227   \n",
       "495  1. Completeness of recorded data in the patien...  booklet4     29-227   \n",
       "496  To monitor and improve the completeness of cas...  booklet1    223-345   \n",
       "497  Monitoring the timeliness and completeness of ...  booklet2    609-610   \n",
       "498  Community-based surveillance contributes to th...  booklet1     18-224   \n",
       "\n",
       "                                              keywords     ID  \\\n",
       "0    ['acute respiratory infection', 'acute respira...     Q4   \n",
       "1    ['Community Based Surveillance', 'subnational ...     Q5   \n",
       "2    ['high absenteeism', 'absenteeism at school', ...     Q9   \n",
       "3    ['Indicator-based surveillance', 'Surveillance...    Q10   \n",
       "4    ['Case-based', 'quarterly or annual', 'Case-ba...    Q13   \n",
       "..                                                 ...    ...   \n",
       "494  ['units submitted reports', 'reporting units s...  Q1229   \n",
       "495  ['units submitted reports', 'reporting units s...  Q1230   \n",
       "496  ['case record unique', 'record unique identifi...  Q1236   \n",
       "497  ['disease surveillance systems', 'disease surv...  Q1239   \n",
       "498  ['Improving Cross Border', 'Based Surveillance...  Q1246   \n",
       "\n",
       "                                              Question  None  \\\n",
       "0            What is the definition of \"unusual event\"   NaN   \n",
       "1          What is Community Based Surveillance (CBS)?   NaN   \n",
       "2    What kind of training should members of VHC re...   NaN   \n",
       "3          What is indicator based surveillance (IBS)?   NaN   \n",
       "4                     What is Case based surveillance?   NaN   \n",
       "..                                                 ...   ...   \n",
       "494  Where should completeness be evaluated in the ...   NaN   \n",
       "495  Which dimensions of completeness are crucial i...   NaN   \n",
       "496  How can the completeness of case reporting be ...   NaN   \n",
       "497  Where should completeness and timeliness of re...   NaN   \n",
       "498  How does community-based surveillance contribu...   NaN   \n",
       "\n",
       "                                          final_answer  \n",
       "0    An unusual event refers to any situation that ...  \n",
       "1    CBS is a community-based surveillance system t...  \n",
       "2    Training for VHCs should include the following...  \n",
       "3    Indicator-based surveillance (IBS) is a system...  \n",
       "4    Case-based surveillance involves ongoing and r...  \n",
       "..                                                 ...  \n",
       "494  The completeness of surveillance data should b...  \n",
       "495  1. Completeness of recorded data in the patien...  \n",
       "496  To monitor and improve the completeness of cas...  \n",
       "497  Monitoring the timeliness and completeness of ...  \n",
       "498  Community-based surveillance contributes to th...  \n",
       "\n",
       "[499 rows x 8 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses['answer'] = df_responses['final_answer']\n",
    "df_responses.drop(['None', 'final_answer'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.columns = ['question_answer', 'reference_document', 'paragraph(s)_number', 'keywords', 'ID', 'Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.melt(df_responses, id_vars=['ID'], value_vars=['question_answer', 'reference_document', 'paragraph(s)_number', \"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission['ID'] = df_submission['ID'] + '_' + df_submission['variable']\n",
    "df_submission.columns = [\"ID\", \"variable\", \"Target\"]\n",
    "df_submission = df_submission[['ID', \"Target\"]].set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(pwd + \"/data/submissions/submission_v4.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.1 ('zindi_llm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1b94373ed21143aa54ae29a501b4c41cca272fcc00b21ffb9f53282b803de8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
