{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/zindi_llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import torch\n",
    "from utils.embeddings import Embedder\n",
    "from utils.preprocess import create_sentance_booklet, create_faise_index\n",
    "import faiss\n",
    "from utils.utils import search_content, read_booklets, retrieve_booklet_text\n",
    "from models.llama import llama\n",
    "from utils.response_generator import get_response, extract_keyword\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silly Mac that forces me to change the environmental variable to prevent issues running transformers\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd  = str(pathlib.Path().cwd().parent.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet = read_booklets((pwd + \"/data/data/booklets/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to create embeddings of the text, it is important to know how long text is since it can influence the tokenization for some models (can end up truncating text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Character lenght is:  18747\n"
     ]
    }
   ],
   "source": [
    "df_booklet['textLength'] = [len(text) for text in df_booklet['text']]\n",
    "print(\"Maximum Character lenght is: \", df_booklet['textLength'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There paragraphs are long, so we might need to consider spliting text on sentences to make them shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem like the following steps will have to be taken:\n",
    "\n",
    "- embed booklet\n",
    "- embed search phrase\n",
    "- use search phrase embedding to search for relevant text in booklet\n",
    "- retrive all relevant text from booklet\n",
    "- format search phrase and into prompt for LLM\n",
    "- Send promt to LLM and return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try simple model\n",
    "I will first use all-mpnet-base-v2  as the sentance embedder and then I will use Llama as the LLM .\n",
    "\n",
    "- Download: `wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_1.bin`\n",
    "- Then run: pip install llama-cpp-python==0.1.78\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Embed all sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Embedder(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not create embeddings uncomment below\n",
    "# booklet_embeddings = embedding_model.embed(df_booklet['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  np.save(file=(pwd + \"/data/data/resources/embeddings\" ), arr=booklet_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creat faiss index for search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not create the index uncomment below\n",
    "# fastIndex = create_faise_index(booklet_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index\n",
    "# faiss.write_index(fastIndex, pwd + \"/data/data/resources/paragraph_index.faiss\")\n",
    "# df_booklet.to_csv(pwd + \"/data/data/resources/booklet.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in index\n",
    "fastIndex = faiss.read_index( pwd + \"/data/data/resources/paragraph_index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Search embeddings and get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/altasaunders/Alta_projects/zindi_llm/llama-2-7b-chat.ggmlv3.q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 3 (mostly Q4_1)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4017.35 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   71.84 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm_model = llama(pwd + \"/llama-2-7b-chat.ggmlv3.q4_1.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment if you want to test on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv(pwd +  \"/data/data/Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# rouge_scores = []\n",
    "# for question in df_train.head(1)['Question Text'].values.tolist():\n",
    "#     print(\"start\")\n",
    "#     df_search_results = search_content(query=question, df_sentances=df_booklet, index=fastIndex, embedder=embedding_model, k=5)\n",
    "#     response = get_response(text=question, llm=llm_model, df_matches=df_search_results)\n",
    "#     response[\"keywords\"] = extract_keyword(str(df_search_results['text'].values), top_n=6)\n",
    "#     scores = scorer.score(response['answer'], question)\n",
    "#     rouge_scores.append(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(pwd +  \"/data/data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   101.72 ms /   117 runs   (    0.87 ms per token,  1150.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56059.09 ms /   395 tokens (  141.92 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:        eval time = 87869.04 ms /   116 runs   (  757.49 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 144283.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.77 ms /   128 runs   (    0.87 ms per token,  1145.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34200.52 ms /   272 tokens (  125.74 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:        eval time = 97837.84 ms /   127 runs   (  770.38 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 132433.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   107.54 ms /   124 runs   (    0.87 ms per token,  1153.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24266.92 ms /   202 tokens (  120.13 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:        eval time = 93616.27 ms /   123 runs   (  761.11 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 118272.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.38 ms /   128 runs   (    0.87 ms per token,  1149.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29921.18 ms /   246 tokens (  121.63 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:        eval time = 96889.16 ms /   127 runs   (  762.91 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 127200.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.29 ms /   128 runs   (    0.87 ms per token,  1150.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26429.94 ms /   220 tokens (  120.14 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:        eval time = 95286.52 ms /   127 runs   (  750.29 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 122110.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.92 ms /   128 runs   (    0.87 ms per token,  1153.97 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16988.72 ms /   150 tokens (  113.26 ms per token,     8.83 tokens per second)\n",
      "llama_print_timings:        eval time = 94643.18 ms /   127 runs   (  745.22 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 112022.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.00 ms /   128 runs   (    0.87 ms per token,  1153.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 33408.49 ms /   273 tokens (  122.38 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:        eval time = 95941.56 ms /   127 runs   (  755.45 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 129737.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    94.87 ms /   109 runs   (    0.87 ms per token,  1148.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54813.15 ms /   394 tokens (  139.12 ms per token,     7.19 tokens per second)\n",
      "llama_print_timings:        eval time = 81337.51 ms /   108 runs   (  753.13 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 136486.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.09 ms /   128 runs   (    0.87 ms per token,  1152.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28574.22 ms /   234 tokens (  122.11 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:        eval time = 94831.26 ms /   127 runs   (  746.70 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 123791.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.04 ms /   128 runs   (    0.87 ms per token,  1152.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35868.05 ms /   284 tokens (  126.30 ms per token,     7.92 tokens per second)\n",
      "llama_print_timings:        eval time = 94843.46 ms /   127 runs   (  746.80 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 131098.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     7.92 ms /     9 runs   (    0.88 ms per token,  1136.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10405.25 ms /    87 tokens (  119.60 ms per token,     8.36 tokens per second)\n",
      "llama_print_timings:        eval time =  5929.27 ms /     8 runs   (  741.16 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 16361.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.19 ms /   128 runs   (    0.87 ms per token,  1151.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17363.47 ms /   149 tokens (  116.53 ms per token,     8.58 tokens per second)\n",
      "llama_print_timings:        eval time = 94121.52 ms /   127 runs   (  741.11 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 111874.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    83.88 ms /    97 runs   (    0.86 ms per token,  1156.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16635.78 ms /   145 tokens (  114.73 ms per token,     8.72 tokens per second)\n",
      "llama_print_timings:        eval time = 74196.35 ms /    96 runs   (  772.88 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 91128.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    71.65 ms /    83 runs   (    0.86 ms per token,  1158.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15677.76 ms /   137 tokens (  114.44 ms per token,     8.74 tokens per second)\n",
      "llama_print_timings:        eval time = 60619.09 ms /    82 runs   (  739.26 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 76553.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.57 ms /   128 runs   (    0.86 ms per token,  1157.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24097.90 ms /   207 tokens (  116.41 ms per token,     8.59 tokens per second)\n",
      "llama_print_timings:        eval time = 94295.17 ms /   127 runs   (  742.48 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 118795.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    85.51 ms /    99 runs   (    0.86 ms per token,  1157.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18095.94 ms /   159 tokens (  113.81 ms per token,     8.79 tokens per second)\n",
      "llama_print_timings:        eval time = 72617.73 ms /    98 runs   (  741.00 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 91015.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    88.16 ms /   102 runs   (    0.86 ms per token,  1157.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 55006.04 ms /   401 tokens (  137.17 ms per token,     7.29 tokens per second)\n",
      "llama_print_timings:        eval time = 75535.28 ms /   101 runs   (  747.87 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 130854.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    92.97 ms /   108 runs   (    0.86 ms per token,  1161.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54044.30 ms /   395 tokens (  136.82 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 81183.98 ms /   107 runs   (  758.73 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 135552.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.40 ms /   128 runs   (    0.86 ms per token,  1159.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27148.01 ms /   232 tokens (  117.02 ms per token,     8.55 tokens per second)\n",
      "llama_print_timings:        eval time = 94096.46 ms /   127 runs   (  740.92 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 121638.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   102.55 ms /   119 runs   (    0.86 ms per token,  1160.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51718.57 ms /   385 tokens (  134.33 ms per token,     7.44 tokens per second)\n",
      "llama_print_timings:        eval time = 88079.04 ms /   118 runs   (  746.43 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 140155.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    11.13 ms /    13 runs   (    0.86 ms per token,  1167.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13518.56 ms /   122 tokens (  110.81 ms per token,     9.02 tokens per second)\n",
      "llama_print_timings:        eval time =  8849.68 ms /    12 runs   (  737.47 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 22407.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.02 ms /   128 runs   (    0.86 ms per token,  1163.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46045.52 ms /   353 tokens (  130.44 ms per token,     7.67 tokens per second)\n",
      "llama_print_timings:        eval time = 94597.24 ms /   127 runs   (  744.86 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141029.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.31 ms /   128 runs   (    0.86 ms per token,  1160.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 41767.87 ms /   326 tokens (  128.12 ms per token,     7.81 tokens per second)\n",
      "llama_print_timings:        eval time = 94654.76 ms /   127 runs   (  745.31 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 136810.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   106.79 ms /   124 runs   (    0.86 ms per token,  1161.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51112.97 ms /   379 tokens (  134.86 ms per token,     7.41 tokens per second)\n",
      "llama_print_timings:        eval time = 91998.14 ms /   123 runs   (  747.95 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143485.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    57.96 ms /    67 runs   (    0.87 ms per token,  1155.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34396.27 ms /   278 tokens (  123.73 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:        eval time = 48987.91 ms /    66 runs   (  742.24 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 83588.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.63 ms /   128 runs   (    0.86 ms per token,  1157.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23876.25 ms /   205 tokens (  116.47 ms per token,     8.59 tokens per second)\n",
      "llama_print_timings:        eval time = 94100.60 ms /   127 runs   (  740.95 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 118374.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.45 ms /   128 runs   (    0.86 ms per token,  1158.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29351.09 ms /   245 tokens (  119.80 ms per token,     8.35 tokens per second)\n",
      "llama_print_timings:        eval time = 94360.83 ms /   127 runs   (  743.00 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 124105.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.22 ms /   128 runs   (    0.86 ms per token,  1161.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 37217.54 ms /   296 tokens (  125.73 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:        eval time = 94416.12 ms /   127 runs   (  743.43 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 132023.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    51.92 ms /    60 runs   (    0.87 ms per token,  1155.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19449.23 ms /   172 tokens (  113.08 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:        eval time = 43568.12 ms /    59 runs   (  738.44 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 63201.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.47 ms /   128 runs   (    0.86 ms per token,  1158.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35621.32 ms /   286 tokens (  124.55 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:        eval time = 94669.75 ms /   127 runs   (  745.43 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 130683.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.74 ms /   128 runs   (    0.87 ms per token,  1145.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17576.52 ms /   155 tokens (  113.40 ms per token,     8.82 tokens per second)\n",
      "llama_print_timings:        eval time = 97922.97 ms /   127 runs   (  771.05 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 115902.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.56 ms /   128 runs   (    0.86 ms per token,  1157.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47263.91 ms /   346 tokens (  136.60 ms per token,     7.32 tokens per second)\n",
      "llama_print_timings:        eval time = 96601.94 ms /   127 runs   (  760.65 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 144266.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    80.70 ms /    93 runs   (    0.87 ms per token,  1152.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 57730.58 ms /   411 tokens (  140.46 ms per token,     7.12 tokens per second)\n",
      "llama_print_timings:        eval time = 69360.48 ms /    92 runs   (  753.92 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 127376.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   104.79 ms /   120 runs   (    0.87 ms per token,  1145.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20577.04 ms /   167 tokens (  123.22 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:        eval time = 92914.01 ms /   119 runs   (  780.79 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 113867.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   105.56 ms /   121 runs   (    0.87 ms per token,  1146.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51753.41 ms /   382 tokens (  135.48 ms per token,     7.38 tokens per second)\n",
      "llama_print_timings:        eval time = 91666.67 ms /   120 runs   (  763.89 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 143792.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.23 ms /   128 runs   (    0.87 ms per token,  1150.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24116.84 ms /   203 tokens (  118.80 ms per token,     8.42 tokens per second)\n",
      "llama_print_timings:        eval time = 96288.64 ms /   127 runs   (  758.18 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 120809.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.82 ms /   128 runs   (    0.87 ms per token,  1155.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32370.63 ms /   261 tokens (  124.03 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:        eval time = 95932.54 ms /   127 runs   (  755.37 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 128700.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    23.50 ms /    27 runs   (    0.87 ms per token,  1149.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27503.75 ms /   224 tokens (  122.78 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:        eval time = 19578.39 ms /    26 runs   (  753.01 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 47165.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.80 ms /   128 runs   (    0.87 ms per token,  1155.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28503.87 ms /   236 tokens (  120.78 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:        eval time = 95383.04 ms /   127 runs   (  751.05 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 124286.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.12 ms /   128 runs   (    0.87 ms per token,  1151.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21831.44 ms /   184 tokens (  118.65 ms per token,     8.43 tokens per second)\n",
      "llama_print_timings:        eval time = 95407.70 ms /   127 runs   (  751.24 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 117639.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.06 ms /   128 runs   (    0.87 ms per token,  1152.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27311.58 ms /   229 tokens (  119.26 ms per token,     8.38 tokens per second)\n",
      "llama_print_timings:        eval time = 97152.71 ms /   127 runs   (  764.98 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 124867.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.81 ms /   128 runs   (    0.87 ms per token,  1144.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42809.33 ms /   320 tokens (  133.78 ms per token,     7.48 tokens per second)\n",
      "llama_print_timings:        eval time = 99886.93 ms /   127 runs   (  786.51 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 143099.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.28 ms /   128 runs   (    0.87 ms per token,  1150.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17122.56 ms /   143 tokens (  119.74 ms per token,     8.35 tokens per second)\n",
      "llama_print_timings:        eval time = 96317.39 ms /   127 runs   (  758.40 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 113843.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.52 ms /   128 runs   (    0.87 ms per token,  1147.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29557.54 ms /   236 tokens (  125.24 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:        eval time = 98411.88 ms /   127 runs   (  774.90 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 128370.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.63 ms /   128 runs   (    0.86 ms per token,  1157.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46530.42 ms /   343 tokens (  135.66 ms per token,     7.37 tokens per second)\n",
      "llama_print_timings:        eval time = 95707.01 ms /   127 runs   (  753.60 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 142632.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.84 ms /   128 runs   (    0.87 ms per token,  1154.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30875.40 ms /   248 tokens (  124.50 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:        eval time = 96451.83 ms /   127 runs   (  759.46 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 127727.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    64.75 ms /    75 runs   (    0.86 ms per token,  1158.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 55796.87 ms /   400 tokens (  139.49 ms per token,     7.17 tokens per second)\n",
      "llama_print_timings:        eval time = 57212.74 ms /    74 runs   (  773.15 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 113240.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.70 ms /   128 runs   (    0.86 ms per token,  1156.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30839.85 ms /   246 tokens (  125.37 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:        eval time = 96950.96 ms /   127 runs   (  763.39 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 128190.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    75.87 ms /    87 runs   (    0.87 ms per token,  1146.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28737.48 ms /   235 tokens (  122.29 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:        eval time = 67719.46 ms /    86 runs   (  787.44 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 96730.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    79.29 ms /    92 runs   (    0.86 ms per token,  1160.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18974.38 ms /   161 tokens (  117.85 ms per token,     8.49 tokens per second)\n",
      "llama_print_timings:        eval time = 68032.37 ms /    91 runs   (  747.61 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 87300.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.28 ms /   128 runs   (    0.86 ms per token,  1160.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16420.91 ms /   143 tokens (  114.83 ms per token,     8.71 tokens per second)\n",
      "llama_print_timings:        eval time = 93958.78 ms /   127 runs   (  739.83 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 110780.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    72.38 ms /    84 runs   (    0.86 ms per token,  1160.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24878.13 ms /   213 tokens (  116.80 ms per token,     8.56 tokens per second)\n",
      "llama_print_timings:        eval time = 61432.91 ms /    83 runs   (  740.16 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 86576.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    56.73 ms /    66 runs   (    0.86 ms per token,  1163.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19710.01 ms /   173 tokens (  113.93 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:        eval time = 47952.65 ms /    65 runs   (  737.73 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 67868.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20541.86 ms /   179 tokens (  114.76 ms per token,     8.71 tokens per second)\n",
      "llama_print_timings:        eval time = 93988.38 ms /   127 runs   (  740.07 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 114931.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.98 ms /   128 runs   (    0.86 ms per token,  1163.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12466.88 ms /   111 tokens (  112.31 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:        eval time = 93644.45 ms /   127 runs   (  737.36 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106510.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.17 ms /   128 runs   (    0.86 ms per token,  1161.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25274.87 ms /   215 tokens (  117.56 ms per token,     8.51 tokens per second)\n",
      "llama_print_timings:        eval time = 94247.42 ms /   127 runs   (  742.11 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 119926.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    82.65 ms /    96 runs   (    0.86 ms per token,  1161.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56814.78 ms /   407 tokens (  139.59 ms per token,     7.16 tokens per second)\n",
      "llama_print_timings:        eval time = 71013.71 ms /    95 runs   (  747.51 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 128123.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.01 ms /   128 runs   (    0.86 ms per token,  1163.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15943.69 ms /   141 tokens (  113.08 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:        eval time = 93792.64 ms /   127 runs   (  738.52 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 110135.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    16.45 ms /    19 runs   (    0.87 ms per token,  1154.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13995.37 ms /   125 tokens (  111.96 ms per token,     8.93 tokens per second)\n",
      "llama_print_timings:        eval time = 13235.68 ms /    18 runs   (  735.32 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 27287.91 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.98 ms /   128 runs   (    0.86 ms per token,  1163.89 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15342.46 ms /    21 tokens (  730.59 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93257.65 ms /   127 runs   (  734.31 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108996.36 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.31 ms /   128 runs   (    0.86 ms per token,  1160.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5721.73 ms /    43 tokens (  133.06 ms per token,     7.52 tokens per second)\n",
      "llama_print_timings:        eval time = 93792.66 ms /   127 runs   (  738.52 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 99906.13 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    61.15 ms /    71 runs   (    0.86 ms per token,  1161.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4609.45 ms /    32 tokens (  144.05 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:        eval time = 51343.18 ms /    70 runs   (  733.47 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 56166.09 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.42 ms /   128 runs   (    0.86 ms per token,  1159.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17656.19 ms /    24 tokens (  735.67 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93554.15 ms /   127 runs   (  736.65 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111598.40 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.03 ms /   128 runs   (    0.86 ms per token,  1163.29 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10180.97 ms /    14 tokens (  727.21 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93219.97 ms /   127 runs   (  734.02 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 103791.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.60 ms /   128 runs   (    0.86 ms per token,  1157.29 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28692.27 ms /   241 tokens (  119.06 ms per token,     8.40 tokens per second)\n",
      "llama_print_timings:        eval time = 96661.21 ms /   127 runs   (  761.11 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 125755.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.50 ms /   128 runs   (    0.86 ms per token,  1158.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26943.12 ms /   227 tokens (  118.69 ms per token,     8.43 tokens per second)\n",
      "llama_print_timings:        eval time = 94385.57 ms /   127 runs   (  743.19 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 121728.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.35 ms /   128 runs   (    0.86 ms per token,  1159.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28076.61 ms /   235 tokens (  119.47 ms per token,     8.37 tokens per second)\n",
      "llama_print_timings:        eval time = 94346.53 ms /   127 runs   (  742.89 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 122823.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.16 ms /   128 runs   (    0.86 ms per token,  1161.97 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27804.66 ms /   236 tokens (  117.82 ms per token,     8.49 tokens per second)\n",
      "llama_print_timings:        eval time = 94173.43 ms /   127 runs   (  741.52 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 122381.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.15 ms /   128 runs   (    0.86 ms per token,  1162.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13823.55 ms /   123 tokens (  112.39 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:        eval time = 93954.96 ms /   127 runs   (  739.80 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 108181.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.90 ms /   128 runs   (    0.86 ms per token,  1164.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18022.76 ms /   161 tokens (  111.94 ms per token,     8.93 tokens per second)\n",
      "llama_print_timings:        eval time = 93895.10 ms /   127 runs   (  739.33 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 112319.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    92.68 ms /   108 runs   (    0.86 ms per token,  1165.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53862.11 ms /   394 tokens (  136.71 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 79988.06 ms /   107 runs   (  747.55 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 134181.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.43 ms /   128 runs   (    0.86 ms per token,  1159.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12495.95 ms /   112 tokens (  111.57 ms per token,     8.96 tokens per second)\n",
      "llama_print_timings:        eval time = 93646.79 ms /   127 runs   (  737.38 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106540.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.12 ms /   128 runs   (    0.86 ms per token,  1162.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23176.80 ms /   201 tokens (  115.31 ms per token,     8.67 tokens per second)\n",
      "llama_print_timings:        eval time = 94037.75 ms /   127 runs   (  740.45 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 117617.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.44 ms /   128 runs   (    0.86 ms per token,  1159.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24707.60 ms /   212 tokens (  116.55 ms per token,     8.58 tokens per second)\n",
      "llama_print_timings:        eval time = 94105.79 ms /   127 runs   (  740.99 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 119215.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    55.14 ms /    64 runs   (    0.86 ms per token,  1160.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62933.73 ms /   439 tokens (  143.36 ms per token,     6.98 tokens per second)\n",
      "llama_print_timings:        eval time = 47204.42 ms /    63 runs   (  749.28 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 110334.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   102.61 ms /   119 runs   (    0.86 ms per token,  1159.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22215.14 ms /   192 tokens (  115.70 ms per token,     8.64 tokens per second)\n",
      "llama_print_timings:        eval time = 87614.97 ms /   118 runs   (  742.50 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 110205.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    20.53 ms /    24 runs   (    0.86 ms per token,  1169.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 70918.10 ms /   480 tokens (  147.75 ms per token,     6.77 tokens per second)\n",
      "llama_print_timings:        eval time = 17213.03 ms /    23 runs   (  748.39 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 88203.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    53.79 ms /    60 runs   (    0.90 ms per token,  1115.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64237.18 ms /   444 tokens (  144.68 ms per token,     6.91 tokens per second)\n",
      "llama_print_timings:        eval time = 45312.15 ms /    59 runs   (  768.00 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 109740.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.85 ms /   128 runs   (    0.86 ms per token,  1165.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13170.85 ms /   115 tokens (  114.53 ms per token,     8.73 tokens per second)\n",
      "llama_print_timings:        eval time = 93684.63 ms /   127 runs   (  737.67 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107252.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    57.93 ms /    67 runs   (    0.86 ms per token,  1156.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61944.12 ms /   436 tokens (  142.07 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:        eval time = 49345.18 ms /    66 runs   (  747.65 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 111494.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    99.09 ms /   114 runs   (    0.87 ms per token,  1150.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53140.06 ms /   390 tokens (  136.26 ms per token,     7.34 tokens per second)\n",
      "llama_print_timings:        eval time = 84810.87 ms /   113 runs   (  750.54 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 138305.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   112.57 ms /   128 runs   (    0.88 ms per token,  1137.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29180.59 ms /   243 tokens (  120.08 ms per token,     8.33 tokens per second)\n",
      "llama_print_timings:        eval time = 94378.81 ms /   127 runs   (  743.14 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 123967.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.93 ms /   128 runs   (    0.86 ms per token,  1164.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24630.80 ms /   210 tokens (  117.29 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:        eval time = 94096.50 ms /   127 runs   (  740.92 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 119132.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.00 ms /   128 runs   (    0.86 ms per token,  1163.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26334.66 ms /   223 tokens (  118.09 ms per token,     8.47 tokens per second)\n",
      "llama_print_timings:        eval time = 94176.07 ms /   127 runs   (  741.54 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 120915.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    12.03 ms /    14 runs   (    0.86 ms per token,  1163.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10500.69 ms /    94 tokens (  111.71 ms per token,     8.95 tokens per second)\n",
      "llama_print_timings:        eval time =  9549.54 ms /    13 runs   (  734.58 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 20092.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.22 ms /   128 runs   (    0.86 ms per token,  1161.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35789.55 ms /   289 tokens (  123.84 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:        eval time = 94366.89 ms /   127 runs   (  743.05 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 130556.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.95 ms /   128 runs   (    0.86 ms per token,  1164.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24510.89 ms /   212 tokens (  115.62 ms per token,     8.65 tokens per second)\n",
      "llama_print_timings:        eval time = 94090.49 ms /   127 runs   (  740.87 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 119004.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    52.49 ms /    61 runs   (    0.86 ms per token,  1162.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 62591.75 ms /   440 tokens (  142.25 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:        eval time = 44893.79 ms /    60 runs   (  748.23 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 107672.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.84 ms /   128 runs   (    0.86 ms per token,  1165.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22219.84 ms /   192 tokens (  115.73 ms per token,     8.64 tokens per second)\n",
      "llama_print_timings:        eval time = 94398.31 ms /   127 runs   (  743.29 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 117021.90 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.96 ms /   128 runs   (    0.86 ms per token,  1164.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14606.86 ms /    20 tokens (  730.34 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93680.52 ms /   127 runs   (  737.64 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108681.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.97 ms /   128 runs   (    0.86 ms per token,  1164.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40406.24 ms /   319 tokens (  126.67 ms per token,     7.89 tokens per second)\n",
      "llama_print_timings:        eval time = 94514.09 ms /   127 runs   (  744.21 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 135317.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    94.63 ms /   110 runs   (    0.86 ms per token,  1162.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20377.51 ms /   178 tokens (  114.48 ms per token,     8.74 tokens per second)\n",
      "llama_print_timings:        eval time = 80675.35 ms /   109 runs   (  740.14 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 101401.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   108.22 ms /   126 runs   (    0.86 ms per token,  1164.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21123.14 ms /   185 tokens (  114.18 ms per token,     8.76 tokens per second)\n",
      "llama_print_timings:        eval time = 92461.14 ms /   125 runs   (  739.69 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 113981.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.08 ms /   128 runs   (    0.86 ms per token,  1162.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 38637.85 ms /   305 tokens (  126.68 ms per token,     7.89 tokens per second)\n",
      "llama_print_timings:        eval time = 94465.64 ms /   127 runs   (  743.82 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133502.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.18 ms /   128 runs   (    0.86 ms per token,  1161.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12403.46 ms /   112 tokens (  110.75 ms per token,     9.03 tokens per second)\n",
      "llama_print_timings:        eval time = 93923.14 ms /   127 runs   (  739.55 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 106726.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.00 ms /   128 runs   (    0.86 ms per token,  1163.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23323.86 ms /   200 tokens (  116.62 ms per token,     8.57 tokens per second)\n",
      "llama_print_timings:        eval time = 94013.68 ms /   127 runs   (  740.27 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 117744.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    80.08 ms /    93 runs   (    0.86 ms per token,  1161.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13902.09 ms /   125 tokens (  111.22 ms per token,     8.99 tokens per second)\n",
      "llama_print_timings:        eval time = 67804.60 ms /    92 runs   (  737.01 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 81994.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.88 ms /   128 runs   (    0.86 ms per token,  1164.89 tokens per second)\n",
      "llama_print_timings: prompt eval time = 38472.63 ms /   304 tokens (  126.55 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time = 94627.49 ms /   127 runs   (  745.10 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133500.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.96 ms /   128 runs   (    0.86 ms per token,  1164.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10766.00 ms /    96 tokens (  112.15 ms per token,     8.92 tokens per second)\n",
      "llama_print_timings:        eval time = 93518.69 ms /   127 runs   (  736.37 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104683.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    45.78 ms /    53 runs   (    0.86 ms per token,  1157.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11318.37 ms /   102 tokens (  110.96 ms per token,     9.01 tokens per second)\n",
      "llama_print_timings:        eval time = 38347.28 ms /    52 runs   (  737.45 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 49828.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.89 ms /   128 runs   (    0.86 ms per token,  1164.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14698.76 ms /   131 tokens (  112.20 ms per token,     8.91 tokens per second)\n",
      "llama_print_timings:        eval time = 93765.78 ms /   127 runs   (  738.31 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 108866.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    58.68 ms /    68 runs   (    0.86 ms per token,  1158.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61454.56 ms /   434 tokens (  141.60 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:        eval time = 50254.32 ms /    67 runs   (  750.06 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 111917.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.12 ms /   128 runs   (    0.86 ms per token,  1162.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27484.96 ms /   231 tokens (  118.98 ms per token,     8.40 tokens per second)\n",
      "llama_print_timings:        eval time = 94210.87 ms /   127 runs   (  741.82 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 122100.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.91 ms /   128 runs   (    0.86 ms per token,  1164.62 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24208.93 ms /   210 tokens (  115.28 ms per token,     8.67 tokens per second)\n",
      "llama_print_timings:        eval time = 94064.38 ms /   127 runs   (  740.66 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 118678.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    98.92 ms /   115 runs   (    0.86 ms per token,  1162.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24625.09 ms /   212 tokens (  116.16 ms per token,     8.61 tokens per second)\n",
      "llama_print_timings:        eval time = 84441.68 ms /   114 runs   (  740.72 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 109430.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.24 ms /   128 runs   (    0.86 ms per token,  1161.08 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17595.44 ms /   156 tokens (  112.79 ms per token,     8.87 tokens per second)\n",
      "llama_print_timings:        eval time = 93809.23 ms /   127 runs   (  738.66 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 111808.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.10 ms /   128 runs   (    0.86 ms per token,  1162.62 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18097.76 ms /   161 tokens (  112.41 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:        eval time = 93802.74 ms /   127 runs   (  738.60 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 112304.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.98 ms /   128 runs   (    0.86 ms per token,  1163.85 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20056.61 ms /   176 tokens (  113.96 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:        eval time = 94249.00 ms /   127 runs   (  742.12 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 114711.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.13 ms /   128 runs   (    0.86 ms per token,  1162.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32281.45 ms /   266 tokens (  121.36 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:        eval time = 94417.47 ms /   127 runs   (  743.44 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 127100.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    45.51 ms /    53 runs   (    0.86 ms per token,  1164.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19730.72 ms /   169 tokens (  116.75 ms per token,     8.57 tokens per second)\n",
      "llama_print_timings:        eval time = 38400.11 ms /    52 runs   (  738.46 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 58295.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   105.79 ms /   123 runs   (    0.86 ms per token,  1162.65 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16176.03 ms /   145 tokens (  111.56 ms per token,     8.96 tokens per second)\n",
      "llama_print_timings:        eval time = 90071.02 ms /   122 runs   (  738.29 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 106635.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.26 ms /   128 runs   (    0.86 ms per token,  1160.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 38275.36 ms /   305 tokens (  125.49 ms per token,     7.97 tokens per second)\n",
      "llama_print_timings:        eval time = 94517.55 ms /   127 runs   (  744.23 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133193.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.15 ms /   128 runs   (    0.86 ms per token,  1162.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11105.34 ms /    99 tokens (  112.18 ms per token,     8.91 tokens per second)\n",
      "llama_print_timings:        eval time = 93593.36 ms /   127 runs   (  736.96 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105096.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.20 ms /   128 runs   (    0.86 ms per token,  1161.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12294.04 ms /   110 tokens (  111.76 ms per token,     8.95 tokens per second)\n",
      "llama_print_timings:        eval time = 93778.29 ms /   127 runs   (  738.41 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 106474.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    25.01 ms /    29 runs   (    0.86 ms per token,  1159.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23081.24 ms /   199 tokens (  115.99 ms per token,     8.62 tokens per second)\n",
      "llama_print_timings:        eval time = 20723.11 ms /    28 runs   (  740.11 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 43895.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11399.72 ms /   102 tokens (  111.76 ms per token,     8.95 tokens per second)\n",
      "llama_print_timings:        eval time = 93787.08 ms /   127 runs   (  738.48 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 105587.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   108.33 ms /   126 runs   (    0.86 ms per token,  1163.08 tokens per second)\n",
      "llama_print_timings: prompt eval time = 50514.81 ms /   378 tokens (  133.64 ms per token,     7.48 tokens per second)\n",
      "llama_print_timings:        eval time = 93336.50 ms /   125 runs   (  746.69 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 144242.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    10.32 ms /    12 runs   (    0.86 ms per token,  1162.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18810.14 ms /   166 tokens (  113.31 ms per token,     8.83 tokens per second)\n",
      "llama_print_timings:        eval time =  8102.17 ms /    11 runs   (  736.56 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 26948.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    55.04 ms /    64 runs   (    0.86 ms per token,  1162.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25076.71 ms /   214 tokens (  117.18 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:        eval time = 46639.92 ms /    63 runs   (  740.32 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 71919.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.08 ms /   128 runs   (    0.86 ms per token,  1162.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20369.75 ms /   178 tokens (  114.44 ms per token,     8.74 tokens per second)\n",
      "llama_print_timings:        eval time = 93938.77 ms /   127 runs   (  739.68 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 114716.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    60.23 ms /    70 runs   (    0.86 ms per token,  1162.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25875.23 ms /   218 tokens (  118.69 ms per token,     8.43 tokens per second)\n",
      "llama_print_timings:        eval time = 51050.90 ms /    69 runs   (  739.87 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 77149.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.01 ms /   128 runs   (    0.86 ms per token,  1163.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16847.50 ms /   148 tokens (  113.83 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:        eval time = 93745.57 ms /   127 runs   (  738.15 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 110996.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    96.33 ms /   112 runs   (    0.86 ms per token,  1162.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 53472.49 ms /   391 tokens (  136.76 ms per token,     7.31 tokens per second)\n",
      "llama_print_timings:        eval time = 83037.31 ms /   111 runs   (  748.08 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 136857.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    21.55 ms /    25 runs   (    0.86 ms per token,  1160.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23122.86 ms /   199 tokens (  116.20 ms per token,     8.61 tokens per second)\n",
      "llama_print_timings:        eval time = 17708.83 ms /    24 runs   (  737.87 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 40909.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    97.57 ms /   113 runs   (    0.86 ms per token,  1158.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52862.60 ms /   391 tokens (  135.20 ms per token,     7.40 tokens per second)\n",
      "llama_print_timings:        eval time = 83789.87 ms /   112 runs   (  748.12 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 137003.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.30 ms /   128 runs   (    0.86 ms per token,  1160.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20154.10 ms /   176 tokens (  114.51 ms per token,     8.73 tokens per second)\n",
      "llama_print_timings:        eval time = 93864.70 ms /   127 runs   (  739.09 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 114427.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.07 ms /   128 runs   (    0.86 ms per token,  1162.85 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20364.38 ms /   180 tokens (  113.14 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:        eval time = 93908.74 ms /   127 runs   (  739.44 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 114680.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.12 ms /   128 runs   (    0.86 ms per token,  1162.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11289.75 ms /   101 tokens (  111.78 ms per token,     8.95 tokens per second)\n",
      "llama_print_timings:        eval time = 93616.44 ms /   127 runs   (  737.14 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105306.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.22 ms /   128 runs   (    0.86 ms per token,  1161.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17857.83 ms /   159 tokens (  112.31 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:        eval time = 93870.69 ms /   127 runs   (  739.14 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 112134.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    26.64 ms /    31 runs   (    0.86 ms per token,  1163.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24542.05 ms /   212 tokens (  115.76 ms per token,     8.64 tokens per second)\n",
      "llama_print_timings:        eval time = 22195.33 ms /    30 runs   (  739.84 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 46836.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    80.88 ms /    94 runs   (    0.86 ms per token,  1162.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15291.14 ms /   137 tokens (  111.61 ms per token,     8.96 tokens per second)\n",
      "llama_print_timings:        eval time = 68602.18 ms /    93 runs   (  737.66 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 84186.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.12 ms /   128 runs   (    0.86 ms per token,  1162.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14415.07 ms /   129 tokens (  111.74 ms per token,     8.95 tokens per second)\n",
      "llama_print_timings:        eval time = 93694.56 ms /   127 runs   (  737.75 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108512.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.31 ms /   128 runs   (    0.86 ms per token,  1160.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28791.90 ms /   243 tokens (  118.49 ms per token,     8.44 tokens per second)\n",
      "llama_print_timings:        eval time = 94209.21 ms /   127 runs   (  741.80 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 123407.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   102.34 ms /   119 runs   (    0.86 ms per token,  1162.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12419.89 ms /   112 tokens (  110.89 ms per token,     9.02 tokens per second)\n",
      "llama_print_timings:        eval time = 87303.67 ms /   118 runs   (  739.86 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 100095.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.19 ms /   128 runs   (    0.86 ms per token,  1161.67 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13600.92 ms /   124 tokens (  109.68 ms per token,     9.12 tokens per second)\n",
      "llama_print_timings:        eval time = 93731.00 ms /   127 runs   (  738.04 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 107735.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    79.85 ms /    93 runs   (    0.86 ms per token,  1164.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12543.16 ms /   113 tokens (  111.00 ms per token,     9.01 tokens per second)\n",
      "llama_print_timings:        eval time = 67845.58 ms /    92 runs   (  737.45 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 80677.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.88 ms /   128 runs   (    0.86 ms per token,  1164.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14322.64 ms /   127 tokens (  112.78 ms per token,     8.87 tokens per second)\n",
      "llama_print_timings:        eval time = 93633.49 ms /   127 runs   (  737.27 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108359.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    84.67 ms /    97 runs   (    0.87 ms per token,  1145.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 55876.97 ms /   406 tokens (  137.63 ms per token,     7.27 tokens per second)\n",
      "llama_print_timings:        eval time = 71902.63 ms /    96 runs   (  748.99 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 128083.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12379.01 ms /   107 tokens (  115.69 ms per token,     8.64 tokens per second)\n",
      "llama_print_timings:        eval time = 93659.28 ms /   127 runs   (  737.47 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106441.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.15 ms /   128 runs   (    0.86 ms per token,  1162.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 33004.87 ms /   272 tokens (  121.34 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:        eval time = 94322.06 ms /   127 runs   (  742.69 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 127731.29 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.37 ms /   128 runs   (    0.86 ms per token,  1159.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13915.59 ms /    19 tokens (  732.40 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93308.90 ms /   127 runs   (  734.72 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107625.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    19.76 ms /    23 runs   (    0.86 ms per token,  1164.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71861.05 ms /   488 tokens (  147.26 ms per token,     6.79 tokens per second)\n",
      "llama_print_timings:        eval time = 16458.69 ms /    22 runs   (  748.12 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 88390.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   112.07 ms /   128 runs   (    0.88 ms per token,  1142.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17935.58 ms /   158 tokens (  113.52 ms per token,     8.81 tokens per second)\n",
      "llama_print_timings:        eval time = 94307.16 ms /   127 runs   (  742.58 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 112659.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   103.45 ms /   120 runs   (    0.86 ms per token,  1159.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51781.62 ms /   381 tokens (  135.91 ms per token,     7.36 tokens per second)\n",
      "llama_print_timings:        eval time = 88937.93 ms /   119 runs   (  747.38 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141094.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    82.56 ms /    96 runs   (    0.86 ms per token,  1162.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26557.91 ms /   223 tokens (  119.09 ms per token,     8.40 tokens per second)\n",
      "llama_print_timings:        eval time = 70315.28 ms /    95 runs   (  740.16 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 97179.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   107.50 ms /   125 runs   (    0.86 ms per token,  1162.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43512.82 ms /   336 tokens (  129.50 ms per token,     7.72 tokens per second)\n",
      "llama_print_timings:        eval time = 92455.90 ms /   124 runs   (  745.61 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 136360.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.89 ms /   128 runs   (    0.86 ms per token,  1164.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 38253.65 ms /   303 tokens (  126.25 ms per token,     7.92 tokens per second)\n",
      "llama_print_timings:        eval time = 94484.09 ms /   127 runs   (  743.97 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133141.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    78.31 ms /    91 runs   (    0.86 ms per token,  1162.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23266.82 ms /   199 tokens (  116.92 ms per token,     8.55 tokens per second)\n",
      "llama_print_timings:        eval time = 66538.82 ms /    90 runs   (  739.32 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 90096.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    28.34 ms /    33 runs   (    0.86 ms per token,  1164.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 69348.76 ms /   471 tokens (  147.24 ms per token,     6.79 tokens per second)\n",
      "llama_print_timings:        eval time = 23946.35 ms /    32 runs   (  748.32 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 93397.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   113.10 ms /   128 runs   (    0.88 ms per token,  1131.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45460.50 ms /   345 tokens (  131.77 ms per token,     7.59 tokens per second)\n",
      "llama_print_timings:        eval time = 95718.54 ms /   127 runs   (  753.69 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 141592.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.31 ms /   128 runs   (    0.86 ms per token,  1160.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14272.70 ms /   127 tokens (  112.38 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:        eval time = 93729.18 ms /   127 runs   (  738.03 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 108407.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.49 ms /   128 runs   (    0.86 ms per token,  1158.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48561.34 ms /   366 tokens (  132.68 ms per token,     7.54 tokens per second)\n",
      "llama_print_timings:        eval time = 95179.64 ms /   127 runs   (  749.45 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 144142.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.03 ms /   128 runs   (    0.86 ms per token,  1163.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 38384.47 ms /   306 tokens (  125.44 ms per token,     7.97 tokens per second)\n",
      "llama_print_timings:        eval time = 94485.65 ms /   127 runs   (  743.98 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133273.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.22 ms /   128 runs   (    0.86 ms per token,  1161.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15574.29 ms /   138 tokens (  112.86 ms per token,     8.86 tokens per second)\n",
      "llama_print_timings:        eval time = 93769.25 ms /   127 runs   (  738.34 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 109752.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.39 ms /   128 runs   (    0.86 ms per token,  1159.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14485.81 ms /   129 tokens (  112.29 ms per token,     8.91 tokens per second)\n",
      "llama_print_timings:        eval time = 93957.74 ms /   127 runs   (  739.82 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 108850.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.23 ms /   128 runs   (    0.86 ms per token,  1161.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14688.81 ms /   130 tokens (  112.99 ms per token,     8.85 tokens per second)\n",
      "llama_print_timings:        eval time = 93831.71 ms /   127 runs   (  738.83 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 108927.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.17 ms /   128 runs   (    0.86 ms per token,  1161.89 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25642.04 ms /   220 tokens (  116.55 ms per token,     8.58 tokens per second)\n",
      "llama_print_timings:        eval time = 94155.25 ms /   127 runs   (  741.38 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 120207.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.15 ms /   128 runs   (    0.86 ms per token,  1162.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22407.31 ms /   194 tokens (  115.50 ms per token,     8.66 tokens per second)\n",
      "llama_print_timings:        eval time = 93930.82 ms /   127 runs   (  739.61 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 116749.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    45.00 ms /    52 runs   (    0.87 ms per token,  1155.61 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65172.83 ms /   452 tokens (  144.19 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:        eval time = 38180.22 ms /    51 runs   (  748.63 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 103514.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.05 ms /   128 runs   (    0.86 ms per token,  1163.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24902.67 ms /   211 tokens (  118.02 ms per token,     8.47 tokens per second)\n",
      "llama_print_timings:        eval time = 94053.66 ms /   127 runs   (  740.58 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 119369.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    60.03 ms /    70 runs   (    0.86 ms per token,  1166.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61383.35 ms /   434 tokens (  141.44 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:        eval time = 51621.73 ms /    69 runs   (  748.14 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 113222.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.04 ms /   128 runs   (    0.86 ms per token,  1163.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22287.94 ms /   191 tokens (  116.69 ms per token,     8.57 tokens per second)\n",
      "llama_print_timings:        eval time = 93887.80 ms /   127 runs   (  739.27 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 116587.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.38 ms /   128 runs   (    0.86 ms per token,  1159.62 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13888.31 ms /   123 tokens (  112.91 ms per token,     8.86 tokens per second)\n",
      "llama_print_timings:        eval time = 93633.04 ms /   127 runs   (  737.27 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107928.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    79.00 ms /    92 runs   (    0.86 ms per token,  1164.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56933.53 ms /   410 tokens (  138.86 ms per token,     7.20 tokens per second)\n",
      "llama_print_timings:        eval time = 67998.05 ms /    91 runs   (  747.23 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 125218.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     9.47 ms /    11 runs   (    0.86 ms per token,  1161.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73191.10 ms /   493 tokens (  148.46 ms per token,     6.74 tokens per second)\n",
      "llama_print_timings:        eval time =  7487.73 ms /    10 runs   (  748.77 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 80712.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    56.33 ms /    62 runs   (    0.91 ms per token,  1100.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64194.56 ms /   442 tokens (  145.24 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:        eval time = 47194.17 ms /    61 runs   (  773.67 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 111592.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    66.18 ms /    77 runs   (    0.86 ms per token,  1163.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34600.22 ms /   277 tokens (  124.91 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:        eval time = 56433.93 ms /    76 runs   (  742.55 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 91278.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.64 ms /   128 runs   (    0.86 ms per token,  1156.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26492.42 ms /   224 tokens (  118.27 ms per token,     8.46 tokens per second)\n",
      "llama_print_timings:        eval time = 94681.56 ms /   127 runs   (  745.52 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 121590.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    94.59 ms /   110 runs   (    0.86 ms per token,  1162.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40768.84 ms /   320 tokens (  127.40 ms per token,     7.85 tokens per second)\n",
      "llama_print_timings:        eval time = 81087.99 ms /   109 runs   (  743.93 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 122205.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.24 ms /   128 runs   (    0.86 ms per token,  1161.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34390.43 ms /   278 tokens (  123.71 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:        eval time = 94450.32 ms /   127 runs   (  743.70 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 129249.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.07 ms /   128 runs   (    0.86 ms per token,  1162.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 33256.07 ms /   271 tokens (  122.72 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:        eval time = 94385.68 ms /   127 runs   (  743.19 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 128050.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.07 ms /   128 runs   (    0.86 ms per token,  1162.89 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23745.79 ms /   205 tokens (  115.83 ms per token,     8.63 tokens per second)\n",
      "llama_print_timings:        eval time = 94144.47 ms /   127 runs   (  741.30 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 118306.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.53 ms /   128 runs   (    0.86 ms per token,  1158.08 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34289.84 ms /   278 tokens (  123.34 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:        eval time = 94365.95 ms /   127 runs   (  743.04 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 129064.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.38 ms /   128 runs   (    0.86 ms per token,  1159.65 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20712.65 ms /   181 tokens (  114.43 ms per token,     8.74 tokens per second)\n",
      "llama_print_timings:        eval time = 93886.50 ms /   127 runs   (  739.26 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 115013.05 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    21.57 ms /    25 runs   (    0.86 ms per token,  1158.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21121.27 ms /    29 tokens (  728.32 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 17578.41 ms /    24 runs   (  732.43 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 38777.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.29 ms /   128 runs   (    0.86 ms per token,  1160.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35798.69 ms /   290 tokens (  123.44 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:        eval time = 94396.10 ms /   127 runs   (  743.28 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 130602.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.15 ms /   128 runs   (    0.86 ms per token,  1162.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26227.38 ms /   223 tokens (  117.61 ms per token,     8.50 tokens per second)\n",
      "llama_print_timings:        eval time = 94254.03 ms /   127 runs   (  742.16 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 120895.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.37 ms /   128 runs   (    0.86 ms per token,  1159.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 38756.55 ms /   308 tokens (  125.83 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:        eval time = 94506.79 ms /   127 runs   (  744.15 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133670.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.45 ms /   128 runs   (    0.86 ms per token,  1158.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24168.61 ms /   207 tokens (  116.76 ms per token,     8.56 tokens per second)\n",
      "llama_print_timings:        eval time = 94446.66 ms /   127 runs   (  743.67 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 119031.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.02 ms /   128 runs   (    0.86 ms per token,  1163.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16531.17 ms /   148 tokens (  111.70 ms per token,     8.95 tokens per second)\n",
      "llama_print_timings:        eval time = 93835.80 ms /   127 runs   (  738.86 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 110777.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.06 ms /   128 runs   (    0.86 ms per token,  1162.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16798.64 ms /   150 tokens (  111.99 ms per token,     8.93 tokens per second)\n",
      "llama_print_timings:        eval time = 93921.57 ms /   127 runs   (  739.54 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 111131.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.87 ms /   128 runs   (    0.86 ms per token,  1165.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19890.09 ms /   175 tokens (  113.66 ms per token,     8.80 tokens per second)\n",
      "llama_print_timings:        eval time = 94013.53 ms /   127 runs   (  740.26 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 114316.02 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.96 ms /   128 runs   (    0.86 ms per token,  1164.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16883.77 ms /    23 tokens (  734.08 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93274.44 ms /   127 runs   (  734.44 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 110561.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.21 ms /   128 runs   (    0.86 ms per token,  1161.39 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19018.31 ms /   168 tokens (  113.20 ms per token,     8.83 tokens per second)\n",
      "llama_print_timings:        eval time = 93871.82 ms /   127 runs   (  739.15 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 113303.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.79 ms /   128 runs   (    0.86 ms per token,  1165.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18100.03 ms /   159 tokens (  113.84 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:        eval time = 93846.15 ms /   127 runs   (  738.95 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 112358.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     2.62 ms /     3 runs   (    0.87 ms per token,  1145.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74992.97 ms /   501 tokens (  149.69 ms per token,     6.68 tokens per second)\n",
      "llama_print_timings:        eval time =  1494.51 ms /     2 runs   (  747.26 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 76496.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   112.24 ms /   128 runs   (    0.88 ms per token,  1140.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18009.82 ms /   160 tokens (  112.56 ms per token,     8.88 tokens per second)\n",
      "llama_print_timings:        eval time = 94606.38 ms /   127 runs   (  744.93 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 113038.00 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.79 ms /   128 runs   (    0.86 ms per token,  1165.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16852.97 ms /    23 tokens (  732.74 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93354.29 ms /   127 runs   (  735.07 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 110609.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    57.63 ms /    67 runs   (    0.86 ms per token,  1162.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18828.76 ms /   166 tokens (  113.43 ms per token,     8.82 tokens per second)\n",
      "llama_print_timings:        eval time = 48694.01 ms /    66 runs   (  737.79 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 67735.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   103.93 ms /   121 runs   (    0.86 ms per token,  1164.26 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51406.41 ms /   383 tokens (  134.22 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:        eval time = 89721.58 ms /   120 runs   (  747.68 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141508.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.21 ms /   128 runs   (    0.86 ms per token,  1161.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 42749.84 ms /   331 tokens (  129.15 ms per token,     7.74 tokens per second)\n",
      "llama_print_timings:        eval time = 94612.66 ms /   127 runs   (  744.98 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 137767.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    94.66 ms /   110 runs   (    0.86 ms per token,  1162.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27003.94 ms /   228 tokens (  118.44 ms per token,     8.44 tokens per second)\n",
      "llama_print_timings:        eval time = 80755.31 ms /   109 runs   (  740.87 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 108113.30 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.35 ms /   128 runs   (    0.86 ms per token,  1159.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16105.45 ms /    22 tokens (  732.07 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93248.98 ms /   127 runs   (  734.24 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 109757.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.37 ms /   128 runs   (    0.86 ms per token,  1159.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 38488.11 ms /   307 tokens (  125.37 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:        eval time = 94805.68 ms /   127 runs   (  746.50 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 133701.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.19 ms /   128 runs   (    0.86 ms per token,  1161.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12202.43 ms /   109 tokens (  111.95 ms per token,     8.93 tokens per second)\n",
      "llama_print_timings:        eval time = 93616.40 ms /   127 runs   (  737.14 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106227.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.36 ms /   128 runs   (    0.86 ms per token,  1159.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13458.74 ms /   119 tokens (  113.10 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:        eval time = 94044.07 ms /   127 runs   (  740.50 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 107911.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    63.83 ms /    74 runs   (    0.86 ms per token,  1159.26 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14504.32 ms /   129 tokens (  112.44 ms per token,     8.89 tokens per second)\n",
      "llama_print_timings:        eval time = 53767.75 ms /    73 runs   (  736.54 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 68504.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.19 ms /   128 runs   (    0.86 ms per token,  1161.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10766.68 ms /    95 tokens (  113.33 ms per token,     8.82 tokens per second)\n",
      "llama_print_timings:        eval time = 93548.85 ms /   127 runs   (  736.61 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104721.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    66.06 ms /    77 runs   (    0.86 ms per token,  1165.61 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16113.38 ms /   141 tokens (  114.28 ms per token,     8.75 tokens per second)\n",
      "llama_print_timings:        eval time = 56111.22 ms /    76 runs   (  738.31 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 72469.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.07 ms /   128 runs   (    0.86 ms per token,  1162.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10518.86 ms /    93 tokens (  113.11 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:        eval time = 93624.13 ms /   127 runs   (  737.20 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104548.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    11.15 ms /    13 runs   (    0.86 ms per token,  1166.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14305.11 ms /   129 tokens (  110.89 ms per token,     9.02 tokens per second)\n",
      "llama_print_timings:        eval time =  8819.12 ms /    12 runs   (  734.93 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 23164.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    56.01 ms /    65 runs   (    0.86 ms per token,  1160.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18539.74 ms /   164 tokens (  113.05 ms per token,     8.85 tokens per second)\n",
      "llama_print_timings:        eval time = 47369.27 ms /    64 runs   (  740.14 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 66115.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    74.13 ms /    86 runs   (    0.86 ms per token,  1160.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10928.09 ms /    97 tokens (  112.66 ms per token,     8.88 tokens per second)\n",
      "llama_print_timings:        eval time = 62590.10 ms /    85 runs   (  736.35 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 73788.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.92 ms /   128 runs   (    0.86 ms per token,  1164.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26198.16 ms /   223 tokens (  117.48 ms per token,     8.51 tokens per second)\n",
      "llama_print_timings:        eval time = 94103.34 ms /   127 runs   (  740.97 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 120713.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.11 ms /   128 runs   (    0.86 ms per token,  1162.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21901.35 ms /   190 tokens (  115.27 ms per token,     8.68 tokens per second)\n",
      "llama_print_timings:        eval time = 93923.11 ms /   127 runs   (  739.55 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 116239.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.14 ms /   128 runs   (    0.86 ms per token,  1162.13 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21304.45 ms /   186 tokens (  114.54 ms per token,     8.73 tokens per second)\n",
      "llama_print_timings:        eval time = 93945.60 ms /   127 runs   (  739.73 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 115663.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.25 ms /   128 runs   (    0.86 ms per token,  1161.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19269.28 ms /   169 tokens (  114.02 ms per token,     8.77 tokens per second)\n",
      "llama_print_timings:        eval time = 93920.63 ms /   127 runs   (  739.53 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 113603.85 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.07 ms /   128 runs   (    0.86 ms per token,  1162.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15356.34 ms /    21 tokens (  731.25 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93252.58 ms /   127 runs   (  734.27 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 109010.90 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.79 ms /   128 runs   (    0.86 ms per token,  1165.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5813.70 ms /    43 tokens (  135.20 ms per token,     7.40 tokens per second)\n",
      "llama_print_timings:        eval time = 93372.94 ms /   127 runs   (  735.22 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 99588.21 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.24 ms /   128 runs   (    0.86 ms per token,  1161.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4905.23 ms /    34 tokens (  144.27 ms per token,     6.93 tokens per second)\n",
      "llama_print_timings:        eval time = 93266.45 ms /   127 runs   (  734.38 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98562.63 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     1.71 ms /     2 runs   (    0.85 ms per token,  1170.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6029.45 ms /    46 tokens (  131.08 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:        eval time =   732.22 ms /     1 runs   (  732.22 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time =  6767.04 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.35 ms /   128 runs   (    0.86 ms per token,  1159.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5433.65 ms /    41 tokens (  132.53 ms per token,     7.55 tokens per second)\n",
      "llama_print_timings:        eval time = 93352.95 ms /   127 runs   (  735.06 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 99177.42 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    36.13 ms /    42 runs   (    0.86 ms per token,  1162.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14644.81 ms /    20 tokens (  732.24 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 30029.93 ms /    41 runs   (  732.44 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 44802.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    10.28 ms /    12 runs   (    0.86 ms per token,  1166.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17172.87 ms /   154 tokens (  111.51 ms per token,     8.97 tokens per second)\n",
      "llama_print_timings:        eval time =  8091.20 ms /    11 runs   (  735.56 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 25300.19 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.10 ms /   128 runs   (    0.86 ms per token,  1162.61 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10170.53 ms /    14 tokens (  726.47 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93574.01 ms /   127 runs   (  736.80 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104139.49 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.13 ms /   128 runs   (    0.86 ms per token,  1162.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13194.99 ms /    18 tokens (  733.06 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93284.68 ms /   127 runs   (  734.53 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106873.15 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3629.71 ms /     5 tokens (  725.94 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93286.25 ms /   127 runs   (  734.54 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 97311.14 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.17 ms /   128 runs   (    0.86 ms per token,  1161.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2204.58 ms /     3 tokens (  734.86 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93333.80 ms /   127 runs   (  734.91 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 95934.67 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.06 ms /   128 runs   (    0.86 ms per token,  1163.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4365.51 ms /     6 tokens (  727.59 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93464.08 ms /   127 runs   (  735.94 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98224.00 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.95 ms /   128 runs   (    0.86 ms per token,  1164.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7996.74 ms /    11 tokens (  726.98 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93228.67 ms /   127 runs   (  734.08 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 101619.48 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.03 ms /   128 runs   (    0.86 ms per token,  1163.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10912.57 ms /    15 tokens (  727.50 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93323.20 ms /   127 runs   (  734.83 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104630.41 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.16 ms /   128 runs   (    0.86 ms per token,  1162.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7269.37 ms /    10 tokens (  726.94 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93360.39 ms /   127 runs   (  735.12 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 101025.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    12.87 ms /    15 runs   (    0.86 ms per token,  1165.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73559.22 ms /   496 tokens (  148.30 ms per token,     6.74 tokens per second)\n",
      "llama_print_timings:        eval time = 10514.96 ms /    14 runs   (  751.07 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 84119.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    41.45 ms /    46 runs   (    0.90 ms per token,  1109.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 66312.60 ms /   450 tokens (  147.36 ms per token,     6.79 tokens per second)\n",
      "llama_print_timings:        eval time = 34672.78 ms /    45 runs   (  770.51 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 101133.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    55.79 ms /    62 runs   (    0.90 ms per token,  1111.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61956.34 ms /   429 tokens (  144.42 ms per token,     6.92 tokens per second)\n",
      "llama_print_timings:        eval time = 46470.81 ms /    61 runs   (  761.82 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 108626.65 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    73.04 ms /    85 runs   (    0.86 ms per token,  1163.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17594.83 ms /    24 tokens (  733.12 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 61717.10 ms /    84 runs   (  734.73 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 79573.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    37.03 ms /    43 runs   (    0.86 ms per token,  1161.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67576.86 ms /   468 tokens (  144.39 ms per token,     6.93 tokens per second)\n",
      "llama_print_timings:        eval time = 31451.82 ms /    42 runs   (  748.85 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 99161.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    13.76 ms /    16 runs   (    0.86 ms per token,  1162.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9883.20 ms /    86 tokens (  114.92 ms per token,     8.70 tokens per second)\n",
      "llama_print_timings:        eval time = 11047.31 ms /    15 runs   (  736.49 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 20979.60 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.43 ms /   128 runs   (    0.86 ms per token,  1159.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10174.02 ms /    14 tokens (  726.72 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93343.19 ms /   127 runs   (  734.99 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 103916.75 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4361.28 ms /     6 tokens (  726.88 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93331.92 ms /   127 runs   (  734.90 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98093.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    35.29 ms /    41 runs   (    0.86 ms per token,  1161.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17307.43 ms /   156 tokens (  110.95 ms per token,     9.01 tokens per second)\n",
      "llama_print_timings:        eval time = 29449.15 ms /    40 runs   (  736.23 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 46884.06 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.36 ms /   128 runs   (    0.86 ms per token,  1159.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10180.77 ms /    14 tokens (  727.20 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93262.95 ms /   127 runs   (  734.35 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 103843.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    52.50 ms /    61 runs   (    0.86 ms per token,  1162.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18341.50 ms /   164 tokens (  111.84 ms per token,     8.94 tokens per second)\n",
      "llama_print_timings:        eval time = 44277.49 ms /    60 runs   (  737.96 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 62810.20 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     1.71 ms /     2 runs   (    0.85 ms per token,  1170.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17597.40 ms /    24 tokens (  733.22 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time =   736.83 ms /     1 runs   (  736.83 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 18339.87 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.55 ms /   128 runs   (    0.86 ms per token,  1157.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8713.81 ms /    12 tokens (  726.15 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93626.38 ms /   127 runs   (  737.22 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 102725.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    37.13 ms /    43 runs   (    0.86 ms per token,  1158.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 38199.87 ms /   306 tokens (  124.84 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:        eval time = 31198.81 ms /    42 runs   (  742.83 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 69529.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    30.42 ms /    35 runs   (    0.87 ms per token,  1150.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15783.45 ms /   141 tokens (  111.94 ms per token,     8.93 tokens per second)\n",
      "llama_print_timings:        eval time = 25096.87 ms /    34 runs   (  738.14 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 40983.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    82.92 ms /    96 runs   (    0.86 ms per token,  1157.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14285.57 ms /   128 tokens (  111.61 ms per token,     8.96 tokens per second)\n",
      "llama_print_timings:        eval time = 70122.69 ms /    95 runs   (  738.13 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 84697.44 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.58 ms /   128 runs   (    0.86 ms per token,  1157.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10910.04 ms /    15 tokens (  727.34 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93216.48 ms /   127 runs   (  733.99 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104511.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    79.36 ms /    92 runs   (    0.86 ms per token,  1159.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23355.00 ms /   204 tokens (  114.49 ms per token,     8.73 tokens per second)\n",
      "llama_print_timings:        eval time = 67331.49 ms /    91 runs   (  739.91 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 90973.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    14.61 ms /    17 runs   (    0.86 ms per token,  1163.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14471.75 ms /   129 tokens (  112.18 ms per token,     8.91 tokens per second)\n",
      "llama_print_timings:        eval time = 11768.12 ms /    16 runs   (  735.51 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 26289.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.30 ms /   128 runs   (    0.86 ms per token,  1160.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9865.28 ms /    86 tokens (  114.71 ms per token,     8.72 tokens per second)\n",
      "llama_print_timings:        eval time = 93622.01 ms /   127 runs   (  737.18 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 103873.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    41.52 ms /    48 runs   (    0.86 ms per token,  1156.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 66328.93 ms /   456 tokens (  145.46 ms per token,     6.87 tokens per second)\n",
      "llama_print_timings:        eval time = 35183.05 ms /    47 runs   (  748.58 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 101656.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    18.96 ms /    22 runs   (    0.86 ms per token,  1160.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17379.46 ms /   152 tokens (  114.34 ms per token,     8.75 tokens per second)\n",
      "llama_print_timings:        eval time = 15465.87 ms /    21 runs   (  736.47 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 32911.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    90.23 ms /   104 runs   (    0.87 ms per token,  1152.62 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10361.62 ms /    91 tokens (  113.86 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:        eval time = 76015.45 ms /   103 runs   (  738.01 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 86691.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    41.43 ms /    48 runs   (    0.86 ms per token,  1158.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10090.13 ms /    89 tokens (  113.37 ms per token,     8.82 tokens per second)\n",
      "llama_print_timings:        eval time = 34671.73 ms /    47 runs   (  737.70 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 44905.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    11.25 ms /    13 runs   (    0.87 ms per token,  1155.97 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17873.82 ms /   160 tokens (  111.71 ms per token,     8.95 tokens per second)\n",
      "llama_print_timings:        eval time =  8832.02 ms /    12 runs   (  736.00 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 26744.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    21.61 ms /    25 runs   (    0.86 ms per token,  1156.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10233.55 ms /    90 tokens (  113.71 ms per token,     8.79 tokens per second)\n",
      "llama_print_timings:        eval time = 17654.63 ms /    24 runs   (  735.61 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 27963.16 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    80.25 ms /    93 runs   (    0.86 ms per token,  1158.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11633.86 ms /    16 tokens (  727.12 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 67553.12 ms /    92 runs   (  734.27 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 79468.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    75.08 ms /    87 runs   (    0.86 ms per token,  1158.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20910.68 ms /   185 tokens (  113.03 ms per token,     8.85 tokens per second)\n",
      "llama_print_timings:        eval time = 63533.58 ms /    86 runs   (  738.76 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 84711.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    22.46 ms /    26 runs   (    0.86 ms per token,  1157.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10316.07 ms /    90 tokens (  114.62 ms per token,     8.72 tokens per second)\n",
      "llama_print_timings:        eval time = 18357.30 ms /    25 runs   (  734.29 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 28750.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    30.22 ms /    35 runs   (    0.86 ms per token,  1158.29 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10199.60 ms /    90 tokens (  113.33 ms per token,     8.82 tokens per second)\n",
      "llama_print_timings:        eval time = 25161.32 ms /    34 runs   (  740.04 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 35465.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    31.87 ms /    37 runs   (    0.86 ms per token,  1161.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10141.02 ms /    89 tokens (  113.94 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:        eval time = 26486.45 ms /    36 runs   (  735.73 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 36737.78 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14648.60 ms /    20 tokens (  732.43 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93244.50 ms /   127 runs   (  734.21 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108280.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    92.98 ms /   108 runs   (    0.86 ms per token,  1161.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10849.12 ms /    97 tokens (  111.85 ms per token,     8.94 tokens per second)\n",
      "llama_print_timings:        eval time = 78768.56 ms /   107 runs   (  736.15 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 89945.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    93.80 ms /   109 runs   (    0.86 ms per token,  1162.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12394.22 ms /   111 tokens (  111.66 ms per token,     8.96 tokens per second)\n",
      "llama_print_timings:        eval time = 79576.28 ms /   108 runs   (  736.82 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 92301.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    68.81 ms /    80 runs   (    0.86 ms per token,  1162.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 59748.04 ms /   424 tokens (  140.92 ms per token,     7.10 tokens per second)\n",
      "llama_print_timings:        eval time = 59032.23 ms /    79 runs   (  747.24 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 119021.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.21 ms /   128 runs   (    0.86 ms per token,  1161.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13117.17 ms /   115 tokens (  114.06 ms per token,     8.77 tokens per second)\n",
      "llama_print_timings:        eval time = 93675.29 ms /   127 runs   (  737.60 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107185.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    76.82 ms /    89 runs   (    0.86 ms per token,  1158.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19531.74 ms /   173 tokens (  112.90 ms per token,     8.86 tokens per second)\n",
      "llama_print_timings:        eval time = 65067.10 ms /    88 runs   (  739.40 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 84873.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    33.72 ms /    39 runs   (    0.86 ms per token,  1156.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10321.70 ms /    92 tokens (  112.19 ms per token,     8.91 tokens per second)\n",
      "llama_print_timings:        eval time = 27947.28 ms /    38 runs   (  735.45 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 38386.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    53.46 ms /    62 runs   (    0.86 ms per token,  1159.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10259.61 ms /    90 tokens (  114.00 ms per token,     8.77 tokens per second)\n",
      "llama_print_timings:        eval time = 45252.92 ms /    61 runs   (  741.85 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 55700.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    71.62 ms /    83 runs   (    0.86 ms per token,  1158.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28667.92 ms /   242 tokens (  118.46 ms per token,     8.44 tokens per second)\n",
      "llama_print_timings:        eval time = 60763.28 ms /    82 runs   (  741.02 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 89689.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    12.97 ms /    15 runs   (    0.86 ms per token,  1156.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10634.43 ms /    94 tokens (  113.13 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:        eval time = 10285.04 ms /    14 runs   (  734.65 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 20964.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    12.13 ms /    14 runs   (    0.87 ms per token,  1153.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22653.63 ms /   196 tokens (  115.58 ms per token,     8.65 tokens per second)\n",
      "llama_print_timings:        eval time =  9585.63 ms /    13 runs   (  737.36 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 32281.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    12.97 ms /    15 runs   (    0.86 ms per token,  1156.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10261.32 ms /    90 tokens (  114.01 ms per token,     8.77 tokens per second)\n",
      "llama_print_timings:        eval time = 10334.47 ms /    14 runs   (  738.18 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 20640.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.12 ms /   128 runs   (    0.86 ms per token,  1162.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10091.81 ms /    89 tokens (  113.39 ms per token,     8.82 tokens per second)\n",
      "llama_print_timings:        eval time = 93486.60 ms /   127 runs   (  736.11 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 103970.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    13.79 ms /    16 runs   (    0.86 ms per token,  1159.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10452.07 ms /    93 tokens (  112.39 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:        eval time = 11051.73 ms /    15 runs   (  736.78 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 21552.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    20.60 ms /    24 runs   (    0.86 ms per token,  1164.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14383.24 ms /   129 tokens (  111.50 ms per token,     8.97 tokens per second)\n",
      "llama_print_timings:        eval time = 16951.27 ms /    23 runs   (  737.01 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 31406.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    55.81 ms /    65 runs   (    0.86 ms per token,  1164.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16738.56 ms /   148 tokens (  113.10 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:        eval time = 47219.68 ms /    64 runs   (  737.81 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 64156.38 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   104.29 ms /   121 runs   (    0.86 ms per token,  1160.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13988.03 ms /    19 tokens (  736.21 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 88099.90 ms /   120 runs   (  734.17 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 102456.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    42.27 ms /    49 runs   (    0.86 ms per token,  1159.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11177.89 ms /   101 tokens (  110.67 ms per token,     9.04 tokens per second)\n",
      "llama_print_timings:        eval time = 35379.48 ms /    48 runs   (  737.07 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 46704.61 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.21 ms /   128 runs   (    0.86 ms per token,  1161.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13883.33 ms /    19 tokens (  730.70 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93254.95 ms /   127 runs   (  734.29 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107527.50 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.31 ms /   128 runs   (    0.86 ms per token,  1160.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14606.87 ms /    20 tokens (  730.34 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93401.20 ms /   127 runs   (  735.44 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108397.98 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.40 ms /   128 runs   (    0.86 ms per token,  1159.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16111.05 ms /    22 tokens (  732.32 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93238.90 ms /   127 runs   (  734.16 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 109737.35 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.39 ms /   128 runs   (    0.86 ms per token,  1159.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17605.63 ms /    24 tokens (  733.57 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93216.93 ms /   127 runs   (  733.99 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111211.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    51.01 ms /    59 runs   (    0.86 ms per token,  1156.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64414.78 ms /   452 tokens (  142.51 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:        eval time = 43406.77 ms /    58 runs   (  748.39 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 108000.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    67.10 ms /    78 runs   (    0.86 ms per token,  1162.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17290.98 ms /   153 tokens (  113.01 ms per token,     8.85 tokens per second)\n",
      "llama_print_timings:        eval time = 56914.99 ms /    77 runs   (  739.16 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 74445.04 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     1.73 ms /     2 runs   (    0.86 ms per token,  1156.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17598.94 ms /    24 tokens (  733.29 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time =   729.83 ms /     1 runs   (  729.83 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 18334.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    40.53 ms /    47 runs   (    0.86 ms per token,  1159.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67091.40 ms /   464 tokens (  144.59 ms per token,     6.92 tokens per second)\n",
      "llama_print_timings:        eval time = 34448.17 ms /    46 runs   (  748.87 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 101681.04 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.42 ms /   128 runs   (    0.86 ms per token,  1159.26 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18187.90 ms /    25 tokens (  727.52 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93295.88 ms /   127 runs   (  734.61 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111873.52 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11638.18 ms /    16 tokens (  727.39 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93227.83 ms /   127 runs   (  734.08 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105256.69 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.50 ms /   128 runs   (    0.86 ms per token,  1158.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12527.02 ms /    17 tokens (  736.88 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93681.16 ms /   127 runs   (  737.65 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106600.53 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.20 ms /   128 runs   (    0.86 ms per token,  1161.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14593.29 ms /    20 tokens (  729.66 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93194.71 ms /   127 runs   (  733.82 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108179.17 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    25.79 ms /    30 runs   (    0.86 ms per token,  1163.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12570.27 ms /    17 tokens (  739.43 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 21215.26 ms /    29 runs   (  731.56 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 33875.96 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    60.42 ms /    70 runs   (    0.86 ms per token,  1158.61 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12537.24 ms /    17 tokens (  737.48 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 50683.32 ms /    69 runs   (  734.54 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 63435.07 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     1.73 ms /     2 runs   (    0.86 ms per token,  1157.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18167.95 ms /    25 tokens (  726.72 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time =   729.73 ms /     1 runs   (  729.73 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 18903.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.41 ms /   128 runs   (    0.86 ms per token,  1159.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18234.44 ms /    25 tokens (  729.38 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93226.00 ms /   127 runs   (  734.06 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111852.30 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.35 ms /   128 runs   (    0.86 ms per token,  1159.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13222.78 ms /    18 tokens (  734.60 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93169.98 ms /   127 runs   (  733.62 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106787.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    88.02 ms /   102 runs   (    0.86 ms per token,  1158.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15283.76 ms /   138 tokens (  110.75 ms per token,     9.03 tokens per second)\n",
      "llama_print_timings:        eval time = 74458.75 ms /   101 runs   (  737.22 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 90059.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.16 ms /   128 runs   (    0.86 ms per token,  1161.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16451.15 ms /   146 tokens (  112.68 ms per token,     8.87 tokens per second)\n",
      "llama_print_timings:        eval time = 93965.87 ms /   127 runs   (  739.89 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 110818.62 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    95.68 ms /   111 runs   (    0.86 ms per token,  1160.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16172.64 ms /    22 tokens (  735.12 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 80676.11 ms /   110 runs   (  733.42 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 97190.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.51 ms /   128 runs   (    0.86 ms per token,  1158.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16849.71 ms /   152 tokens (  110.85 ms per token,     9.02 tokens per second)\n",
      "llama_print_timings:        eval time = 93863.10 ms /   127 runs   (  739.08 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 111113.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.55 ms /   128 runs   (    0.86 ms per token,  1157.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24269.75 ms /   209 tokens (  116.12 ms per token,     8.61 tokens per second)\n",
      "llama_print_timings:        eval time = 94162.76 ms /   127 runs   (  741.44 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 118839.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    38.87 ms /    45 runs   (    0.86 ms per token,  1157.62 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65931.82 ms /   457 tokens (  144.27 ms per token,     6.93 tokens per second)\n",
      "llama_print_timings:        eval time = 32948.41 ms /    44 runs   (  748.83 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 99018.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.32 ms /   128 runs   (    0.86 ms per token,  1160.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12547.26 ms /    17 tokens (  738.07 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 93233.15 ms /   127 runs   (  734.12 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106176.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    78.07 ms /    90 runs   (    0.87 ms per token,  1152.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58261.85 ms /   421 tokens (  138.39 ms per token,     7.23 tokens per second)\n",
      "llama_print_timings:        eval time = 66561.54 ms /    89 runs   (  747.88 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 125101.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.52 ms /   128 runs   (    0.86 ms per token,  1158.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26630.40 ms /   223 tokens (  119.42 ms per token,     8.37 tokens per second)\n",
      "llama_print_timings:        eval time = 94155.56 ms /   127 runs   (  741.38 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 121190.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    10.30 ms /    12 runs   (    0.86 ms per token,  1164.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73035.84 ms /   492 tokens (  148.45 ms per token,     6.74 tokens per second)\n",
      "llama_print_timings:        eval time =  8231.72 ms /    11 runs   (  748.34 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 81304.23 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   113.11 ms /   128 runs   (    0.88 ms per token,  1131.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5234.34 ms /    38 tokens (  137.75 ms per token,     7.26 tokens per second)\n",
      "llama_print_timings:        eval time = 94161.57 ms /   127 runs   (  741.43 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 99800.56 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.65 ms /   128 runs   (    0.86 ms per token,  1156.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13926.96 ms /    19 tokens (  733.00 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93236.52 ms /   127 runs   (  734.15 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107559.74 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.35 ms /   128 runs   (    0.86 ms per token,  1159.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19628.03 ms /    27 tokens (  726.96 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93601.23 ms /   127 runs   (  737.02 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 113625.94 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.54 ms /   128 runs   (    0.86 ms per token,  1157.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4991.73 ms /    36 tokens (  138.66 ms per token,     7.21 tokens per second)\n",
      "llama_print_timings:        eval time = 93307.56 ms /   127 runs   (  734.71 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98696.84 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22562.31 ms /    31 tokens (  727.82 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93363.90 ms /   127 runs   (  735.15 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 116324.16 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    50.96 ms /    59 runs   (    0.86 ms per token,  1157.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4638.81 ms /    32 tokens (  144.96 ms per token,     6.90 tokens per second)\n",
      "llama_print_timings:        eval time = 42529.40 ms /    58 runs   (  733.27 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 47350.02 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.32 ms /   128 runs   (    0.86 ms per token,  1160.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17609.93 ms /    24 tokens (  733.75 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93265.90 ms /   127 runs   (  734.38 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111266.31 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.06 ms /   128 runs   (    0.86 ms per token,  1163.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19632.69 ms /    27 tokens (  727.14 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93376.34 ms /   127 runs   (  735.25 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 113398.64 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.15 ms /   128 runs   (    0.86 ms per token,  1162.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19697.23 ms /    27 tokens (  729.53 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93400.43 ms /   127 runs   (  735.44 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 113488.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    70.53 ms /    82 runs   (    0.86 ms per token,  1162.66 tokens per second)\n",
      "llama_print_timings: prompt eval time = 28801.19 ms /   244 tokens (  118.04 ms per token,     8.47 tokens per second)\n",
      "llama_print_timings:        eval time = 60038.65 ms /    81 runs   (  741.22 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 89093.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.57 ms /   128 runs   (    0.86 ms per token,  1157.62 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18868.43 ms /   168 tokens (  112.31 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:        eval time = 93950.81 ms /   127 runs   (  739.77 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 113217.25 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.68 ms /   128 runs   (    0.86 ms per token,  1156.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8721.53 ms /    12 tokens (  726.79 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93179.71 ms /   127 runs   (  733.70 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 102292.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.81 ms /   128 runs   (    0.87 ms per token,  1155.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27831.13 ms /   235 tokens (  118.43 ms per token,     8.44 tokens per second)\n",
      "llama_print_timings:        eval time = 94168.17 ms /   127 runs   (  741.48 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 122399.33 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.56 ms /   128 runs   (    0.86 ms per token,  1157.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12543.01 ms /    17 tokens (  737.82 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93263.53 ms /   127 runs   (  734.36 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106197.24 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.18 ms /   128 runs   (    0.86 ms per token,  1161.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4631.08 ms /    32 tokens (  144.72 ms per token,     6.91 tokens per second)\n",
      "llama_print_timings:        eval time = 93297.48 ms /   127 runs   (  734.63 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98319.38 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.15 ms /   128 runs   (    0.86 ms per token,  1162.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15386.69 ms /    21 tokens (  732.70 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93625.21 ms /   127 runs   (  737.21 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 109402.97 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.64 ms /   128 runs   (    0.86 ms per token,  1156.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20361.67 ms /    28 tokens (  727.20 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93330.88 ms /   127 runs   (  734.89 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 114083.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.23 ms /   128 runs   (    0.86 ms per token,  1161.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16463.85 ms /   148 tokens (  111.24 ms per token,     8.99 tokens per second)\n",
      "llama_print_timings:        eval time = 93706.44 ms /   127 runs   (  737.85 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 110568.71 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.36 ms /   128 runs   (    0.86 ms per token,  1159.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16849.75 ms /    23 tokens (  732.60 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93233.74 ms /   127 runs   (  734.12 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 110475.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    51.77 ms /    60 runs   (    0.86 ms per token,  1158.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64488.89 ms /   451 tokens (  142.99 ms per token,     6.99 tokens per second)\n",
      "llama_print_timings:        eval time = 44249.39 ms /    59 runs   (  749.99 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 108923.38 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.08 ms /   128 runs   (    0.86 ms per token,  1162.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19646.06 ms /    27 tokens (  727.63 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93245.80 ms /   127 runs   (  734.22 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 113284.07 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.04 ms /   128 runs   (    0.86 ms per token,  1163.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17613.94 ms /    24 tokens (  733.91 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93274.97 ms /   127 runs   (  734.45 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 111282.13 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.61 ms /   128 runs   (    0.86 ms per token,  1157.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21087.25 ms /    29 tokens (  727.15 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93332.92 ms /   127 runs   (  734.90 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 114813.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.70 ms /   128 runs   (    0.86 ms per token,  1156.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17063.06 ms /   153 tokens (  111.52 ms per token,     8.97 tokens per second)\n",
      "llama_print_timings:        eval time = 94071.57 ms /   127 runs   (  740.72 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 111536.04 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.31 ms /   128 runs   (    0.86 ms per token,  1160.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20378.60 ms /    28 tokens (  727.81 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93232.72 ms /   127 runs   (  734.12 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 114009.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.77 ms /   128 runs   (    0.87 ms per token,  1155.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27893.40 ms /   234 tokens (  119.20 ms per token,     8.39 tokens per second)\n",
      "llama_print_timings:        eval time = 94142.80 ms /   127 runs   (  741.28 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 122450.04 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.33 ms /   128 runs   (    0.86 ms per token,  1160.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19672.50 ms /    27 tokens (  728.61 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93257.33 ms /   127 runs   (  734.31 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 113323.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     4.32 ms /     5 runs   (    0.86 ms per token,  1158.48 tokens per second)\n",
      "llama_print_timings: prompt eval time = 75636.51 ms /   506 tokens (  149.48 ms per token,     6.69 tokens per second)\n",
      "llama_print_timings:        eval time =  2996.76 ms /     4 runs   (  749.19 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 78649.09 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.98 ms /   128 runs   (    0.87 ms per token,  1153.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18172.40 ms /    25 tokens (  726.90 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93636.17 ms /   127 runs   (  737.29 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 112206.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     6.88 ms /     8 runs   (    0.86 ms per token,  1162.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 75125.51 ms /   503 tokens (  149.35 ms per token,     6.70 tokens per second)\n",
      "llama_print_timings:        eval time =  5237.87 ms /     7 runs   (  748.27 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 80387.58 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.78 ms /   128 runs   (    0.87 ms per token,  1145.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5024.70 ms /    35 tokens (  143.56 ms per token,     6.97 tokens per second)\n",
      "llama_print_timings:        eval time = 93781.78 ms /   127 runs   (  738.44 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 99207.65 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.56 ms /   128 runs   (    0.86 ms per token,  1157.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18208.55 ms /    25 tokens (  728.34 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93512.94 ms /   127 runs   (  736.32 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 112117.48 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.02 ms /   128 runs   (    0.86 ms per token,  1163.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21827.38 ms /    30 tokens (  727.58 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93485.19 ms /   127 runs   (  736.10 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 115709.44 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.35 ms /   128 runs   (    0.86 ms per token,  1159.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5147.77 ms /    37 tokens (  139.13 ms per token,     7.19 tokens per second)\n",
      "llama_print_timings:        eval time = 93252.14 ms /   127 runs   (  734.27 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98796.81 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.25 ms /   128 runs   (    0.86 ms per token,  1160.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5261.35 ms /    38 tokens (  138.46 ms per token,     7.22 tokens per second)\n",
      "llama_print_timings:        eval time = 93340.03 ms /   127 runs   (  734.96 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98998.84 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.64 ms /   128 runs   (    0.86 ms per token,  1156.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5608.47 ms /    42 tokens (  133.54 ms per token,     7.49 tokens per second)\n",
      "llama_print_timings:        eval time = 93388.37 ms /   127 runs   (  735.34 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 99394.69 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.47 ms /   128 runs   (    0.86 ms per token,  1158.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4768.84 ms /    33 tokens (  144.51 ms per token,     6.92 tokens per second)\n",
      "llama_print_timings:        eval time = 93295.78 ms /   127 runs   (  734.61 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98462.74 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.29 ms /   128 runs   (    0.86 ms per token,  1160.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6375.46 ms /    52 tokens (  122.61 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:        eval time = 93445.50 ms /   127 runs   (  735.79 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 100221.20 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.33 ms /   128 runs   (    0.86 ms per token,  1160.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8537.52 ms /    74 tokens (  115.37 ms per token,     8.67 tokens per second)\n",
      "llama_print_timings:        eval time = 93461.71 ms /   127 runs   (  735.92 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 102388.63 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.33 ms /   128 runs   (    0.86 ms per token,  1160.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7654.81 ms /    63 tokens (  121.50 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:        eval time = 93354.69 ms /   127 runs   (  735.08 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 101396.68 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.42 ms /   128 runs   (    0.86 ms per token,  1159.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5637.06 ms /    42 tokens (  134.22 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:        eval time = 93419.84 ms /   127 runs   (  735.59 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 99443.92 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    10.44 ms /    12 runs   (    0.87 ms per token,  1149.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4654.15 ms /    32 tokens (  145.44 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:        eval time =  8055.90 ms /    11 runs   (  732.35 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 12745.46 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.24 ms /   128 runs   (    0.86 ms per token,  1161.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16114.58 ms /    22 tokens (  732.48 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93373.51 ms /   127 runs   (  735.22 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 109874.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     2.61 ms /     3 runs   (    0.87 ms per token,  1149.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 75746.23 ms /   508 tokens (  149.11 ms per token,     6.71 tokens per second)\n",
      "llama_print_timings:        eval time =  1500.77 ms /     2 runs   (  750.39 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 77255.41 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.66 ms /   128 runs   (    0.87 ms per token,  1146.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19710.59 ms /    27 tokens (  730.02 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93578.46 ms /   127 runs   (  736.84 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 113680.42 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    31.90 ms /    37 runs   (    0.86 ms per token,  1159.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18173.59 ms /    25 tokens (  726.94 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 26409.65 ms /    36 runs   (  733.60 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 44693.49 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    79.34 ms /    92 runs   (    0.86 ms per token,  1159.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14612.64 ms /    20 tokens (  730.63 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 66777.16 ms /    91 runs   (  733.81 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 81667.61 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.60 ms /   128 runs   (    0.86 ms per token,  1157.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13893.64 ms /    19 tokens (  731.24 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93253.72 ms /   127 runs   (  734.28 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107537.12 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   106.36 ms /   123 runs   (    0.86 ms per token,  1156.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17733.79 ms /    24 tokens (  738.91 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 89649.14 ms /   122 runs   (  734.83 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107757.33 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.76 ms /   128 runs   (    0.87 ms per token,  1155.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16882.85 ms /    23 tokens (  734.04 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93267.18 ms /   127 runs   (  734.39 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 110540.82 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.55 ms /   128 runs   (    0.86 ms per token,  1157.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7275.47 ms /    10 tokens (  727.55 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93172.52 ms /   127 runs   (  733.64 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 100842.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     7.78 ms /     9 runs   (    0.86 ms per token,  1156.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74517.17 ms /   502 tokens (  148.44 ms per token,     6.74 tokens per second)\n",
      "llama_print_timings:        eval time =  5982.65 ms /     8 runs   (  747.83 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 80527.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     9.85 ms /    11 runs   (    0.90 ms per token,  1116.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74375.03 ms /   492 tokens (  151.17 ms per token,     6.62 tokens per second)\n",
      "llama_print_timings:        eval time =  7660.49 ms /    10 runs   (  766.05 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 82070.14 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   113.50 ms /   128 runs   (    0.89 ms per token,  1127.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12773.78 ms /    17 tokens (  751.40 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:        eval time = 94181.03 ms /   127 runs   (  741.58 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 107358.14 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    39.86 ms /    46 runs   (    0.87 ms per token,  1154.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10907.91 ms /    15 tokens (  727.19 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 32976.26 ms /    45 runs   (  732.81 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 44025.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    20.87 ms /    24 runs   (    0.87 ms per token,  1150.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71857.86 ms /   487 tokens (  147.55 ms per token,     6.78 tokens per second)\n",
      "llama_print_timings:        eval time = 17201.20 ms /    23 runs   (  747.88 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 89131.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    22.10 ms /    24 runs   (    0.92 ms per token,  1085.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71914.06 ms /   480 tokens (  149.82 ms per token,     6.67 tokens per second)\n",
      "llama_print_timings:        eval time = 17788.98 ms /    23 runs   (  773.43 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 89779.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    15.17 ms /    17 runs   (    0.89 ms per token,  1120.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73224.60 ms /   486 tokens (  150.67 ms per token,     6.64 tokens per second)\n",
      "llama_print_timings:        eval time = 12322.79 ms /    16 runs   (  770.17 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 85599.34 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   113.34 ms /   128 runs   (    0.89 ms per token,  1129.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14215.68 ms /    19 tokens (  748.19 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:        eval time = 94066.60 ms /   127 runs   (  740.68 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 108675.14 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.71 ms /   128 runs   (    0.86 ms per token,  1156.21 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22533.09 ms /    31 tokens (  726.87 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93289.94 ms /   127 runs   (  734.57 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 116208.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    15.51 ms /    18 runs   (    0.86 ms per token,  1160.32 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72999.02 ms /   493 tokens (  148.07 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time = 12712.58 ms /    17 runs   (  747.80 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 85764.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    13.55 ms /    15 runs   (    0.90 ms per token,  1107.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73824.19 ms /   489 tokens (  150.97 ms per token,     6.62 tokens per second)\n",
      "llama_print_timings:        eval time = 10794.93 ms /    14 runs   (  771.07 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 84666.13 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   113.06 ms /   128 runs   (    0.88 ms per token,  1132.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17945.83 ms /    24 tokens (  747.74 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:        eval time = 96972.24 ms /   127 runs   (  763.56 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 115320.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    33.76 ms /    39 runs   (    0.87 ms per token,  1155.21 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68535.10 ms /   472 tokens (  145.20 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:        eval time = 28451.94 ms /    38 runs   (  748.74 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 97106.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    45.65 ms /    50 runs   (    0.91 ms per token,  1095.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 66134.76 ms /   454 tokens (  145.67 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:        eval time = 37657.39 ms /    49 runs   (  768.52 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 103953.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    36.61 ms /    40 runs   (    0.92 ms per token,  1092.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67694.98 ms /   460 tokens (  147.16 ms per token,     6.80 tokens per second)\n",
      "llama_print_timings:        eval time = 29826.76 ms /    39 runs   (  764.79 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 97647.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    14.80 ms /    17 runs   (    0.87 ms per token,  1148.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72494.41 ms /   485 tokens (  149.47 ms per token,     6.69 tokens per second)\n",
      "llama_print_timings:        eval time = 12028.68 ms /    16 runs   (  751.79 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 84574.93 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     9.11 ms /    10 runs   (    0.91 ms per token,  1098.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17685.38 ms /    24 tokens (  736.89 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time =  6662.66 ms /     9 runs   (  740.30 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 24379.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    23.76 ms /    27 runs   (    0.88 ms per token,  1136.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71718.20 ms /   484 tokens (  148.18 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time = 19672.73 ms /    26 runs   (  756.64 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 91472.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    11.63 ms /    13 runs   (    0.89 ms per token,  1117.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10924.20 ms /    94 tokens (  116.21 ms per token,     8.60 tokens per second)\n",
      "llama_print_timings:        eval time =  8959.70 ms /    12 runs   (  746.64 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 19924.67 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.53 ms /   128 runs   (    0.87 ms per token,  1147.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14628.25 ms /    20 tokens (  731.41 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93434.24 ms /   127 runs   (  735.70 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108454.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    22.38 ms /    26 runs   (    0.86 ms per token,  1161.80 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71379.67 ms /   485 tokens (  147.17 ms per token,     6.79 tokens per second)\n",
      "llama_print_timings:        eval time = 18704.03 ms /    25 runs   (  748.16 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 90161.39 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11627.28 ms /    16 tokens (  726.71 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93338.66 ms /   127 runs   (  734.95 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105353.54 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.19 ms /   128 runs   (    0.86 ms per token,  1161.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4357.51 ms /     6 tokens (  726.25 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93197.89 ms /   127 runs   (  733.84 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 97946.04 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.67 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15356.24 ms /    21 tokens (  731.25 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93240.96 ms /   127 runs   (  734.18 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108984.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    13.92 ms /    16 runs   (    0.87 ms per token,  1149.67 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73168.44 ms /   495 tokens (  147.82 ms per token,     6.77 tokens per second)\n",
      "llama_print_timings:        eval time = 11216.79 ms /    15 runs   (  747.79 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 84433.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    48.66 ms /    53 runs   (    0.92 ms per token,  1089.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65333.63 ms /   448 tokens (  145.83 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:        eval time = 40267.40 ms /    52 runs   (  774.37 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 105772.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    45.06 ms /    50 runs   (    0.90 ms per token,  1109.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 66273.61 ms /   454 tokens (  145.98 ms per token,     6.85 tokens per second)\n",
      "llama_print_timings:        eval time = 37493.64 ms /    49 runs   (  765.18 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 103924.61 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.60 ms /   128 runs   (    0.86 ms per token,  1157.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16900.36 ms /    23 tokens (  734.80 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93176.74 ms /   127 runs   (  733.68 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 110466.09 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.62 ms /   128 runs   (    0.86 ms per token,  1157.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9443.30 ms /    13 tokens (  726.41 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93258.04 ms /   127 runs   (  734.32 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 103092.75 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.88 ms /   128 runs   (    0.87 ms per token,  1154.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13214.07 ms /    18 tokens (  734.12 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93129.62 ms /   127 runs   (  733.30 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106734.48 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.83 ms /   128 runs   (    0.87 ms per token,  1154.89 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20371.63 ms /    28 tokens (  727.56 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93367.42 ms /   127 runs   (  735.18 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 114132.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    34.55 ms /    40 runs   (    0.86 ms per token,  1157.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68675.02 ms /   471 tokens (  145.81 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:        eval time = 29237.58 ms /    39 runs   (  749.68 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 98034.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    14.54 ms /    16 runs   (    0.91 ms per token,  1100.26 tokens per second)\n",
      "llama_print_timings: prompt eval time = 73986.78 ms /   488 tokens (  151.61 ms per token,     6.60 tokens per second)\n",
      "llama_print_timings:        eval time = 11527.83 ms /    15 runs   (  768.52 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 85565.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   115.04 ms /   128 runs   (    0.90 ms per token,  1112.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9662.89 ms /    82 tokens (  117.84 ms per token,     8.49 tokens per second)\n",
      "llama_print_timings:        eval time = 94785.01 ms /   127 runs   (  746.34 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 104862.45 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    19.92 ms /    23 runs   (    0.87 ms per token,  1154.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8749.28 ms /    12 tokens (  729.11 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 16099.44 ms /    22 runs   (  731.79 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time = 24919.73 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.27 ms /   128 runs   (    0.87 ms per token,  1150.39 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13442.82 ms /    18 tokens (  746.82 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:        eval time = 93278.24 ms /   127 runs   (  734.47 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107115.27 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.40 ms /   128 runs   (    0.86 ms per token,  1159.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12449.46 ms /    17 tokens (  732.32 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93223.28 ms /   127 runs   (  734.04 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 106059.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.11 ms /   128 runs   (    0.86 ms per token,  1162.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21320.42 ms /   187 tokens (  114.01 ms per token,     8.77 tokens per second)\n",
      "llama_print_timings:        eval time = 93834.59 ms /   127 runs   (  738.86 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 115555.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.22 ms /   128 runs   (    0.86 ms per token,  1161.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35120.94 ms /   286 tokens (  122.80 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:        eval time = 94370.30 ms /   127 runs   (  743.07 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 129888.07 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.11 ms /   128 runs   (    0.86 ms per token,  1162.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7981.67 ms /    11 tokens (  725.61 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93232.34 ms /   127 runs   (  734.11 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 101602.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    13.82 ms /    16 runs   (    0.86 ms per token,  1157.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11006.89 ms /   100 tokens (  110.07 ms per token,     9.09 tokens per second)\n",
      "llama_print_timings:        eval time = 11018.31 ms /    15 runs   (  734.55 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 22072.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    61.87 ms /    72 runs   (    0.86 ms per token,  1163.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13590.61 ms /   122 tokens (  111.40 ms per token,     8.98 tokens per second)\n",
      "llama_print_timings:        eval time = 52269.36 ms /    71 runs   (  736.19 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 66077.30 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.99 ms /   128 runs   (    0.86 ms per token,  1163.75 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11613.32 ms /    16 tokens (  725.83 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93179.29 ms /   127 runs   (  733.70 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105182.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    47.53 ms /    55 runs   (    0.86 ms per token,  1157.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 64742.08 ms /   456 tokens (  141.98 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:        eval time = 40467.79 ms /    54 runs   (  749.40 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 105377.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    24.47 ms /    28 runs   (    0.87 ms per token,  1144.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 66896.29 ms /   461 tokens (  145.11 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:        eval time = 20510.16 ms /    27 runs   (  759.64 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 87493.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    37.70 ms /    41 runs   (    0.92 ms per token,  1087.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 67881.88 ms /   463 tokens (  146.61 ms per token,     6.82 tokens per second)\n",
      "llama_print_timings:        eval time = 30678.50 ms /    40 runs   (  766.96 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 98691.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    33.95 ms /    38 runs   (    0.89 ms per token,  1119.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68495.56 ms /   466 tokens (  146.99 ms per token,     6.80 tokens per second)\n",
      "llama_print_timings:        eval time = 28426.30 ms /    37 runs   (  768.28 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 97041.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     8.68 ms /    10 runs   (    0.87 ms per token,  1151.81 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68174.68 ms /   465 tokens (  146.61 ms per token,     6.82 tokens per second)\n",
      "llama_print_timings:        eval time =  6741.36 ms /     9 runs   (  749.04 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 74946.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    27.78 ms /    31 runs   (    0.90 ms per token,  1115.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 69925.40 ms /   472 tokens (  148.15 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time = 23302.45 ms /    30 runs   (  776.75 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 93327.97 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   113.61 ms /   128 runs   (    0.89 ms per token,  1126.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9713.06 ms /    13 tokens (  747.16 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:        eval time = 94363.99 ms /   127 runs   (  743.02 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 104486.75 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   109.94 ms /   128 runs   (    0.86 ms per token,  1164.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10908.94 ms /    15 tokens (  727.26 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:        eval time = 93132.38 ms /   127 runs   (  733.33 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 104438.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     6.03 ms /     7 runs   (    0.86 ms per token,  1161.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74895.96 ms /   504 tokens (  148.60 ms per token,     6.73 tokens per second)\n",
      "llama_print_timings:        eval time =  4492.89 ms /     6 runs   (  748.82 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 79409.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     6.21 ms /     7 runs   (    0.89 ms per token,  1126.67 tokens per second)\n",
      "llama_print_timings: prompt eval time = 74581.90 ms /   495 tokens (  150.67 ms per token,     6.64 tokens per second)\n",
      "llama_print_timings:        eval time =  4583.19 ms /     6 runs   (  763.86 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 79185.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     6.31 ms /     7 runs   (    0.90 ms per token,  1110.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 75890.28 ms /   497 tokens (  152.70 ms per token,     6.55 tokens per second)\n",
      "llama_print_timings:        eval time =  4560.08 ms /     6 runs   (  760.01 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 80472.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     0.87 ms /     1 runs   (    0.87 ms per token,  1149.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 77266.75 ms /   503 tokens (  153.61 ms per token,     6.51 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 77269.88 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   117.65 ms /   128 runs   (    0.92 ms per token,  1087.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11202.96 ms /    15 tokens (  746.86 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:        eval time = 95821.68 ms /   127 runs   (  754.50 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 107438.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     1.71 ms /     2 runs   (    0.86 ms per token,  1166.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 75838.44 ms /   509 tokens (  148.99 ms per token,     6.71 tokens per second)\n",
      "llama_print_timings:        eval time =   746.22 ms /     1 runs   (  746.22 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 76590.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =     3.61 ms /     4 runs   (    0.90 ms per token,  1108.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 75793.01 ms /   499 tokens (  151.89 ms per token,     6.58 tokens per second)\n",
      "llama_print_timings:        eval time =  2287.74 ms /     3 runs   (  762.58 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 78093.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   116.60 ms /   128 runs   (    0.91 ms per token,  1097.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31825.38 ms /   255 tokens (  124.81 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:        eval time = 97005.13 ms /   127 runs   (  763.82 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 129250.74 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.09 ms /   128 runs   (    0.86 ms per token,  1162.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5124.09 ms /    38 tokens (  134.84 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:        eval time = 93366.18 ms /   127 runs   (  735.17 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 98881.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    24.17 ms /    28 runs   (    0.86 ms per token,  1158.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 70537.24 ms /   483 tokens (  146.04 ms per token,     6.85 tokens per second)\n",
      "llama_print_timings:        eval time = 20191.42 ms /    27 runs   (  747.83 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 90813.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    45.56 ms /    50 runs   (    0.91 ms per token,  1097.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65972.99 ms /   454 tokens (  145.31 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:        eval time = 37988.32 ms /    49 runs   (  775.27 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 104122.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    33.87 ms /    38 runs   (    0.89 ms per token,  1122.04 tokens per second)\n",
      "llama_print_timings: prompt eval time = 68582.69 ms /   466 tokens (  147.17 ms per token,     6.79 tokens per second)\n",
      "llama_print_timings:        eval time = 28054.82 ms /    37 runs   (  758.24 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 96756.95 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.45 ms /   128 runs   (    0.86 ms per token,  1158.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16038.94 ms /    22 tokens (  729.04 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time = 93294.16 ms /   127 runs   (  734.60 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 109727.01 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   112.39 ms /   128 runs   (    0.88 ms per token,  1138.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12499.97 ms /    17 tokens (  735.29 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 100002.24 ms /   127 runs   (  787.42 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time = 112915.59 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   120.35 ms /   128 runs   (    0.94 ms per token,  1063.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17034.46 ms /    23 tokens (  740.63 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 100842.39 ms /   127 runs   (  794.03 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 118309.93 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    82.03 ms /    46 runs   (    1.78 ms per token,   560.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 290693.53 ms /    22 tokens (13213.34 ms per token,     0.08 tokens per second)\n",
      "llama_print_timings:        eval time = 1503554.17 ms /    45 runs   (33412.31 ms per token,     0.03 tokens per second)\n",
      "llama_print_timings:       total time = 1794544.03 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    15.58 ms /    18 runs   (    0.87 ms per token,  1155.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16873.22 ms /    23 tokens (  733.62 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 12891.15 ms /    17 runs   (  758.30 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:       total time = 29817.79 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.52 ms /   128 runs   (    0.86 ms per token,  1158.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11788.30 ms /    16 tokens (  736.77 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93592.12 ms /   127 runs   (  736.95 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 105776.17 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.04 ms /   128 runs   (    0.87 ms per token,  1152.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17690.80 ms /    24 tokens (  737.12 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 96894.47 ms /   127 runs   (  762.95 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 114979.51 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   113.44 ms /   128 runs   (    0.89 ms per token,  1128.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21235.78 ms /    28 tokens (  758.42 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:        eval time = 113358.87 ms /   127 runs   (  892.59 ms per token,     1.12 tokens per second)\n",
      "llama_print_timings:       total time = 135013.62 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   112.58 ms /   128 runs   (    0.88 ms per token,  1136.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19962.80 ms /    27 tokens (  739.36 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:        eval time = 97954.35 ms /   127 runs   (  771.29 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 118320.46 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.34 ms /   128 runs   (    0.86 ms per token,  1160.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23524.52 ms /    31 tokens (  758.86 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:        eval time = 94185.04 ms /   127 runs   (  741.61 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 118102.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.40 ms /   128 runs   (    0.86 ms per token,  1159.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24566.32 ms /   210 tokens (  116.98 ms per token,     8.55 tokens per second)\n",
      "llama_print_timings:        eval time = 94332.42 ms /   127 runs   (  742.77 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 119314.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    20.29 ms /    23 runs   (    0.88 ms per token,  1133.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 71743.65 ms /   481 tokens (  149.16 ms per token,     6.70 tokens per second)\n",
      "llama_print_timings:        eval time = 17028.66 ms /    22 runs   (  774.03 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 88846.17 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   112.41 ms /   128 runs   (    0.88 ms per token,  1138.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4730.56 ms /    33 tokens (  143.35 ms per token,     6.98 tokens per second)\n",
      "llama_print_timings:        eval time = 94765.17 ms /   127 runs   (  746.18 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 99904.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.67 ms /   128 runs   (    0.86 ms per token,  1156.61 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26578.35 ms /   226 tokens (  117.60 ms per token,     8.50 tokens per second)\n",
      "llama_print_timings:        eval time = 94343.72 ms /   127 runs   (  742.86 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 121326.76 ms\n",
      "llama_tokenize_with_model: too many tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.70 ms /   128 runs   (    0.86 ms per token,  1156.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16858.29 ms /    23 tokens (  732.97 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:        eval time = 93738.11 ms /   127 runs   (  738.10 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 110995.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    95.46 ms /   111 runs   (    0.86 ms per token,  1162.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40126.78 ms /   314 tokens (  127.79 ms per token,     7.83 tokens per second)\n",
      "llama_print_timings:        eval time = 81943.38 ms /   110 runs   (  744.94 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 122420.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.64 ms /   128 runs   (    0.86 ms per token,  1156.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 38586.48 ms /   305 tokens (  126.51 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time = 97240.09 ms /   127 runs   (  765.67 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 136230.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.76 ms /   128 runs   (    0.87 ms per token,  1155.65 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34385.44 ms /   274 tokens (  125.49 ms per token,     7.97 tokens per second)\n",
      "llama_print_timings:        eval time = 98914.15 ms /   127 runs   (  778.85 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:       total time = 133710.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.66 ms /   128 runs   (    0.86 ms per token,  1156.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19995.17 ms /   171 tokens (  116.93 ms per token,     8.55 tokens per second)\n",
      "llama_print_timings:        eval time = 96691.18 ms /   127 runs   (  761.35 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time = 117103.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    96.26 ms /   110 runs   (    0.88 ms per token,  1142.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17858.39 ms /   149 tokens (  119.85 ms per token,     8.34 tokens per second)\n",
      "llama_print_timings:        eval time = 84612.40 ms /   109 runs   (  776.26 ms per token,     1.29 tokens per second)\n",
      "llama_print_timings:       total time = 102824.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.64 ms /   128 runs   (    0.86 ms per token,  1156.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27404.20 ms /   222 tokens (  123.44 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:        eval time = 95450.93 ms /   127 runs   (  751.58 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 123272.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.10 ms /   128 runs   (    0.86 ms per token,  1162.54 tokens per second)\n",
      "llama_print_timings: prompt eval time = 43506.79 ms /   337 tokens (  129.10 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 94698.09 ms /   127 runs   (  745.65 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 138610.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    52.60 ms /    60 runs   (    0.88 ms per token,  1140.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 63340.50 ms /   444 tokens (  142.66 ms per token,     7.01 tokens per second)\n",
      "llama_print_timings:        eval time = 44511.20 ms /    59 runs   (  754.43 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 108043.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    32.44 ms /    36 runs   (    0.90 ms per token,  1109.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 69329.85 ms /   468 tokens (  148.14 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time = 26903.47 ms /    35 runs   (  768.67 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 96348.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    91.11 ms /   106 runs   (    0.86 ms per token,  1163.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30664.40 ms /   249 tokens (  123.15 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:        eval time = 77959.18 ms /   105 runs   (  742.47 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 108958.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   100.32 ms /   116 runs   (    0.86 ms per token,  1156.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 52897.66 ms /   388 tokens (  136.33 ms per token,     7.33 tokens per second)\n",
      "llama_print_timings:        eval time = 88522.01 ms /   115 runs   (  769.76 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time = 141792.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    55.85 ms /    65 runs   (    0.86 ms per token,  1163.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 36093.82 ms /   289 tokens (  124.89 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:        eval time = 47602.31 ms /    64 runs   (  743.79 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 83906.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    63.28 ms /    72 runs   (    0.88 ms per token,  1137.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61389.26 ms /   432 tokens (  142.10 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:        eval time = 53408.50 ms /    71 runs   (  752.23 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 115024.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    82.90 ms /    96 runs   (    0.86 ms per token,  1157.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9580.53 ms /    82 tokens (  116.84 ms per token,     8.56 tokens per second)\n",
      "llama_print_timings:        eval time = 70657.79 ms /    95 runs   (  743.77 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 80542.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.25 ms /   128 runs   (    0.86 ms per token,  1160.98 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22204.06 ms /   191 tokens (  116.25 ms per token,     8.60 tokens per second)\n",
      "llama_print_timings:        eval time = 94005.00 ms /   127 runs   (  740.20 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 116625.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    73.04 ms /    84 runs   (    0.87 ms per token,  1150.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58691.53 ms /   419 tokens (  140.08 ms per token,     7.14 tokens per second)\n",
      "llama_print_timings:        eval time = 62217.33 ms /    83 runs   (  749.61 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 121170.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   102.56 ms /   119 runs   (    0.86 ms per token,  1160.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 36976.68 ms /   292 tokens (  126.63 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time = 87705.11 ms /   118 runs   (  743.26 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 125057.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   107.77 ms /   125 runs   (    0.86 ms per token,  1159.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16825.90 ms /   149 tokens (  112.93 ms per token,     8.86 tokens per second)\n",
      "llama_print_timings:        eval time = 91559.88 ms /   124 runs   (  738.39 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 108782.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.99 ms /   128 runs   (    0.87 ms per token,  1153.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48823.06 ms /   351 tokens (  139.10 ms per token,     7.19 tokens per second)\n",
      "llama_print_timings:        eval time = 95334.76 ms /   127 runs   (  750.67 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 144566.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    56.09 ms /    65 runs   (    0.86 ms per token,  1158.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11857.53 ms /   105 tokens (  112.93 ms per token,     8.86 tokens per second)\n",
      "llama_print_timings:        eval time = 47253.05 ms /    64 runs   (  738.33 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 59315.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.50 ms /   128 runs   (    0.86 ms per token,  1158.33 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15115.45 ms /   133 tokens (  113.65 ms per token,     8.80 tokens per second)\n",
      "llama_print_timings:        eval time = 95205.12 ms /   127 runs   (  749.65 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 110737.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.44 ms /   128 runs   (    0.86 ms per token,  1159.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19657.12 ms /   172 tokens (  114.29 ms per token,     8.75 tokens per second)\n",
      "llama_print_timings:        eval time = 94164.40 ms /   127 runs   (  741.45 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 114233.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.29 ms /   128 runs   (    0.86 ms per token,  1160.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 48707.08 ms /   367 tokens (  132.72 ms per token,     7.53 tokens per second)\n",
      "llama_print_timings:        eval time = 94851.99 ms /   127 runs   (  746.87 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 143962.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.17 ms /   128 runs   (    0.86 ms per token,  1161.80 tokens per second)\n",
      "llama_print_timings: prompt eval time = 30921.04 ms /   254 tokens (  121.74 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:        eval time = 94509.68 ms /   127 runs   (  744.17 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 125842.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    11.19 ms /    13 runs   (    0.86 ms per token,  1161.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12463.24 ms /   111 tokens (  112.28 ms per token,     8.91 tokens per second)\n",
      "llama_print_timings:        eval time =  8824.30 ms /    12 runs   (  735.36 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 21327.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.58 ms /   128 runs   (    0.86 ms per token,  1157.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26793.21 ms /   229 tokens (  117.00 ms per token,     8.55 tokens per second)\n",
      "llama_print_timings:        eval time = 94293.90 ms /   127 runs   (  742.47 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 121493.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   105.11 ms /   122 runs   (    0.86 ms per token,  1160.69 tokens per second)\n",
      "llama_print_timings: prompt eval time = 51578.60 ms /   382 tokens (  135.02 ms per token,     7.41 tokens per second)\n",
      "llama_print_timings:        eval time = 90389.57 ms /   121 runs   (  747.02 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 142347.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    87.96 ms /   102 runs   (    0.86 ms per token,  1159.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21535.37 ms /   188 tokens (  114.55 ms per token,     8.73 tokens per second)\n",
      "llama_print_timings:        eval time = 74743.93 ms /   101 runs   (  740.04 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 96604.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    62.05 ms /    72 runs   (    0.86 ms per token,  1160.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 61319.84 ms /   432 tokens (  141.94 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:        eval time = 53166.44 ms /    71 runs   (  748.82 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 114708.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.33 ms /   128 runs   (    0.86 ms per token,  1160.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21801.87 ms /   187 tokens (  116.59 ms per token,     8.58 tokens per second)\n",
      "llama_print_timings:        eval time = 94231.24 ms /   127 runs   (  741.98 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 116443.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.40 ms /   128 runs   (    0.86 ms per token,  1159.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9474.07 ms /    82 tokens (  115.54 ms per token,     8.66 tokens per second)\n",
      "llama_print_timings:        eval time = 93753.49 ms /   127 runs   (  738.22 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 103635.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.30 ms /   128 runs   (    0.86 ms per token,  1160.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14891.38 ms /   133 tokens (  111.97 ms per token,     8.93 tokens per second)\n",
      "llama_print_timings:        eval time = 93828.91 ms /   127 runs   (  738.81 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 109125.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.48 ms /   128 runs   (    0.86 ms per token,  1158.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18559.01 ms /   163 tokens (  113.86 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:        eval time = 93903.25 ms /   127 runs   (  739.40 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 112870.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    80.45 ms /    93 runs   (    0.87 ms per token,  1155.98 tokens per second)\n",
      "llama_print_timings: prompt eval time = 56764.39 ms /   411 tokens (  138.11 ms per token,     7.24 tokens per second)\n",
      "llama_print_timings:        eval time = 68924.97 ms /    92 runs   (  749.18 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time = 125977.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26261.73 ms /   222 tokens (  118.30 ms per token,     8.45 tokens per second)\n",
      "llama_print_timings:        eval time = 94195.49 ms /   127 runs   (  741.70 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 120866.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    40.59 ms /    47 runs   (    0.86 ms per token,  1158.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 65900.23 ms /   457 tokens (  144.20 ms per token,     6.93 tokens per second)\n",
      "llama_print_timings:        eval time = 34425.20 ms /    46 runs   (  748.37 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 100469.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.62 ms /   128 runs   (    0.86 ms per token,  1157.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 37580.53 ms /   297 tokens (  126.53 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time = 94685.32 ms /   127 runs   (  745.55 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 132670.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    75.29 ms /    87 runs   (    0.87 ms per token,  1155.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17366.11 ms /   153 tokens (  113.50 ms per token,     8.81 tokens per second)\n",
      "llama_print_timings:        eval time = 63496.99 ms /    86 runs   (  738.34 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 81138.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    58.02 ms /    67 runs   (    0.87 ms per token,  1154.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20418.80 ms /   178 tokens (  114.71 ms per token,     8.72 tokens per second)\n",
      "llama_print_timings:        eval time = 48774.99 ms /    66 runs   (  739.02 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 69406.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.18 ms /   128 runs   (    0.86 ms per token,  1161.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23168.56 ms /   199 tokens (  116.42 ms per token,     8.59 tokens per second)\n",
      "llama_print_timings:        eval time = 94029.00 ms /   127 runs   (  740.39 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 117611.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.34 ms /   128 runs   (    0.86 ms per token,  1160.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15827.58 ms /   139 tokens (  113.87 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:        eval time = 94070.31 ms /   127 runs   (  740.71 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 110305.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.22 ms /   128 runs   (    0.86 ms per token,  1161.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17567.60 ms /   157 tokens (  111.90 ms per token,     8.94 tokens per second)\n",
      "llama_print_timings:        eval time = 93890.97 ms /   127 runs   (  739.30 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 111866.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.68 ms /   128 runs   (    0.86 ms per token,  1156.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15913.74 ms /   142 tokens (  112.07 ms per token,     8.92 tokens per second)\n",
      "llama_print_timings:        eval time = 93817.19 ms /   127 runs   (  738.72 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 110137.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    11.24 ms /    13 runs   (    0.86 ms per token,  1156.89 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11830.40 ms /   103 tokens (  114.86 ms per token,     8.71 tokens per second)\n",
      "llama_print_timings:        eval time =  8812.86 ms /    12 runs   (  734.41 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 20683.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.44 ms /   128 runs   (    0.86 ms per token,  1159.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 36797.09 ms /   294 tokens (  125.16 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:        eval time = 94394.09 ms /   127 runs   (  743.26 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 131596.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.49 ms /   128 runs   (    0.86 ms per token,  1158.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16249.84 ms /   145 tokens (  112.07 ms per token,     8.92 tokens per second)\n",
      "llama_print_timings:        eval time = 93772.04 ms /   127 runs   (  738.36 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 110429.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    20.60 ms /    24 runs   (    0.86 ms per token,  1164.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27473.37 ms /   232 tokens (  118.42 ms per token,     8.44 tokens per second)\n",
      "llama_print_timings:        eval time = 17077.42 ms /    23 runs   (  742.50 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 44627.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.79 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15333.85 ms /   137 tokens (  111.93 ms per token,     8.93 tokens per second)\n",
      "llama_print_timings:        eval time = 93773.98 ms /   127 runs   (  738.38 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 109515.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.43 ms /   128 runs   (    0.86 ms per token,  1159.13 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23304.34 ms /   201 tokens (  115.94 ms per token,     8.63 tokens per second)\n",
      "llama_print_timings:        eval time = 94134.69 ms /   127 runs   (  741.22 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 117851.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.56 ms /   128 runs   (    0.86 ms per token,  1157.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 37354.47 ms /   299 tokens (  124.93 ms per token,     8.00 tokens per second)\n",
      "llama_print_timings:        eval time = 94546.32 ms /   127 runs   (  744.46 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 132306.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.52 ms /   128 runs   (    0.86 ms per token,  1158.15 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15504.26 ms /   138 tokens (  112.35 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:        eval time = 93821.25 ms /   127 runs   (  738.75 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 109732.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.30 ms /   128 runs   (    0.86 ms per token,  1160.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 37092.07 ms /   297 tokens (  124.89 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:        eval time = 94602.54 ms /   127 runs   (  744.90 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 132101.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    70.61 ms /    82 runs   (    0.86 ms per token,  1161.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15258.02 ms /   137 tokens (  111.37 ms per token,     8.98 tokens per second)\n",
      "llama_print_timings:        eval time = 59735.32 ms /    81 runs   (  737.47 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 75250.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    37.87 ms /    44 runs   (    0.86 ms per token,  1161.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18662.71 ms /   165 tokens (  113.11 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:        eval time = 31762.62 ms /    43 runs   (  738.67 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 50562.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.37 ms /   128 runs   (    0.86 ms per token,  1159.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14218.34 ms /   128 tokens (  111.08 ms per token,     9.00 tokens per second)\n",
      "llama_print_timings:        eval time = 93702.30 ms /   127 runs   (  737.81 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108327.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    76.58 ms /    89 runs   (    0.86 ms per token,  1162.12 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18046.23 ms /   159 tokens (  113.50 ms per token,     8.81 tokens per second)\n",
      "llama_print_timings:        eval time = 64993.31 ms /    88 runs   (  738.56 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 83323.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.34 ms /   128 runs   (    0.86 ms per token,  1160.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16094.23 ms /   143 tokens (  112.55 ms per token,     8.89 tokens per second)\n",
      "llama_print_timings:        eval time = 93812.77 ms /   127 runs   (  738.68 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 110316.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    50.71 ms /    59 runs   (    0.86 ms per token,  1163.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18534.32 ms /   164 tokens (  113.01 ms per token,     8.85 tokens per second)\n",
      "llama_print_timings:        eval time = 42862.95 ms /    58 runs   (  739.02 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 61583.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    83.86 ms /    97 runs   (    0.86 ms per token,  1156.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32229.89 ms /   264 tokens (  122.08 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:        eval time = 71487.36 ms /    96 runs   (  744.66 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 104027.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.29 ms /   128 runs   (    0.86 ms per token,  1160.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15650.04 ms /   140 tokens (  111.79 ms per token,     8.95 tokens per second)\n",
      "llama_print_timings:        eval time = 93757.03 ms /   127 runs   (  738.24 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 109814.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.31 ms /   128 runs   (    0.86 ms per token,  1160.35 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20631.47 ms /   181 tokens (  113.99 ms per token,     8.77 tokens per second)\n",
      "llama_print_timings:        eval time = 93981.89 ms /   127 runs   (  740.01 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 115025.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.53 ms /   128 runs   (    0.86 ms per token,  1158.05 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14625.99 ms /   130 tokens (  112.51 ms per token,     8.89 tokens per second)\n",
      "llama_print_timings:        eval time = 93704.23 ms /   127 runs   (  737.83 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108740.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.41 ms /   128 runs   (    0.86 ms per token,  1159.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25280.56 ms /   215 tokens (  117.58 ms per token,     8.50 tokens per second)\n",
      "llama_print_timings:        eval time = 94092.53 ms /   127 runs   (  740.89 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 119784.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.28 ms /   128 runs   (    0.86 ms per token,  1160.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25306.52 ms /   216 tokens (  117.16 ms per token,     8.54 tokens per second)\n",
      "llama_print_timings:        eval time = 94073.01 ms /   127 runs   (  740.73 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 119790.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.16 ms /   128 runs   (    0.86 ms per token,  1161.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45965.52 ms /   350 tokens (  131.33 ms per token,     7.61 tokens per second)\n",
      "llama_print_timings:        eval time = 94890.60 ms /   127 runs   (  747.17 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time = 141261.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.03 ms /   128 runs   (    0.86 ms per token,  1163.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17284.83 ms /   151 tokens (  114.47 ms per token,     8.74 tokens per second)\n",
      "llama_print_timings:        eval time = 93955.23 ms /   127 runs   (  739.80 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 111649.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   111.39 ms /   128 runs   (    0.87 ms per token,  1149.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13797.71 ms /   124 tokens (  111.27 ms per token,     8.99 tokens per second)\n",
      "llama_print_timings:        eval time = 93715.45 ms /   127 runs   (  737.92 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 107922.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    89.56 ms /   104 runs   (    0.86 ms per token,  1161.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16609.18 ms /   148 tokens (  112.22 ms per token,     8.91 tokens per second)\n",
      "llama_print_timings:        eval time = 76078.79 ms /   103 runs   (  738.63 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 93017.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =    38.79 ms /    45 runs   (    0.86 ms per token,  1160.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17979.68 ms /   160 tokens (  112.37 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:        eval time = 32486.63 ms /    44 runs   (  738.33 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 50607.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.19 ms /   128 runs   (    0.86 ms per token,  1161.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17050.47 ms /   152 tokens (  112.17 ms per token,     8.91 tokens per second)\n",
      "llama_print_timings:        eval time = 93911.34 ms /   127 runs   (  739.46 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 111372.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   128 runs   (    0.86 ms per token,  1160.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15279.72 ms /   135 tokens (  113.18 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:        eval time = 93748.46 ms /   127 runs   (  738.18 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 109436.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 56059.16 ms\n",
      "llama_print_timings:      sample time =   110.13 ms /   128 runs   (    0.86 ms per token,  1162.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14848.76 ms /   131 tokens (  113.35 ms per token,     8.82 tokens per second)\n",
      "llama_print_timings:        eval time = 93692.83 ms /   127 runs   (  737.74 ms per token,     1.36 tokens per second)\n",
      "llama_print_timings:       total time = 108949.91 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "responses = []\n",
    "i = 0\n",
    "for question in df_test['Question Text'].values.tolist():\n",
    "    df_search_results = search_content(query=question, df_sentances=df_booklet, index=fastIndex, embedder=embedding_model, k=5)\n",
    "    response = get_response(text=question, llm=llm_model, df_matches=df_search_results)\n",
    "    response[\"keywords\"] = extract_keyword(str(df_search_results['text'].values), top_n=5)\n",
    "    responses.append(response)\n",
    "    i += 1\n",
    "\n",
    "df_responses = pd.DataFrame(responses)\n",
    "df_responses['ID'] = df_test['ID']\n",
    "df_responses['Question'] = df_test['Question Text']\n",
    "df_responses[[\"Question\", \"question_answer\", \"reference_document\", \"paragraph(s)_number\", \"keywords\", \"ID\"]].to_csv(pwd + \"/data/data/answers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submissoion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.columns = ['question_answer', 'reference_document', 'paragraph(s)_number', 'keywords', 'ID', 'Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_answer</th>\n",
       "      <th>reference_document</th>\n",
       "      <th>paragraph(s)_number</th>\n",
       "      <th>keywords</th>\n",
       "      <th>ID</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Unexplained deaths - The sudden and unexpla...</td>\n",
       "      <td>TG Booklet 2</td>\n",
       "      <td>133-514</td>\n",
       "      <td>[unusual, community, time, sick, unusual death]</td>\n",
       "      <td>Q4</td>\n",
       "      <td>What is the definition of \"unusual event\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1. What is Community Based Surveillance (CBS)?...</td>\n",
       "      <td>TG Booklet 1</td>\n",
       "      <td>224-438</td>\n",
       "      <td>[Steps for establishing, formal surveillance s...</td>\n",
       "      <td>Q5</td>\n",
       "      <td>What is Community Based Surveillance (CBS)?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1. Clinical skills such as recognizing signs a...</td>\n",
       "      <td>TG Booklet 1</td>\n",
       "      <td>456-468</td>\n",
       "      <td>[reporting suspect cases, health staff, mentor...</td>\n",
       "      <td>Q9</td>\n",
       "      <td>What kind of training should members of VHC re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Based on the information provided, here is how...</td>\n",
       "      <td>TG Booklet 2</td>\n",
       "      <td>135-398</td>\n",
       "      <td>[Indicator-Based Surveillance, Surveillance, I...</td>\n",
       "      <td>Q10</td>\n",
       "      <td>What is indicator based surveillance (IBS)?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. Community Based Surveillance (CBS) CBS is a...</td>\n",
       "      <td>TG Booklet 1</td>\n",
       "      <td>81-435</td>\n",
       "      <td>[Case-based surveillance involves, Case-based ...</td>\n",
       "      <td>Q13</td>\n",
       "      <td>What is Case based surveillance?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     question_answer reference_document  \\\n",
       "0  1. Unexplained deaths - The sudden and unexpla...       TG Booklet 2   \n",
       "1  1. What is Community Based Surveillance (CBS)?...       TG Booklet 1   \n",
       "2  1. Clinical skills such as recognizing signs a...       TG Booklet 1   \n",
       "3  Based on the information provided, here is how...       TG Booklet 2   \n",
       "4  1. Community Based Surveillance (CBS) CBS is a...       TG Booklet 1   \n",
       "\n",
       "  paragraph(s)_number                                           keywords   ID  \\\n",
       "0             133-514    [unusual, community, time, sick, unusual death]   Q4   \n",
       "1             224-438  [Steps for establishing, formal surveillance s...   Q5   \n",
       "2             456-468  [reporting suspect cases, health staff, mentor...   Q9   \n",
       "3             135-398  [Indicator-Based Surveillance, Surveillance, I...  Q10   \n",
       "4              81-435  [Case-based surveillance involves, Case-based ...  Q13   \n",
       "\n",
       "                                            Question  \n",
       "0          What is the definition of \"unusual event\"  \n",
       "1        What is Community Based Surveillance (CBS)?  \n",
       "2  What kind of training should members of VHC re...  \n",
       "3        What is indicator based surveillance (IBS)?  \n",
       "4                   What is Case based surveillance?  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.melt(df_responses, id_vars=['ID'], value_vars=['question_answer', 'reference_document', 'paragraph(s)_number', \"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission['ID'] = df_submission['ID'] + '_' + df_submission['variable']\n",
    "df_submission.columns = [\"ID\", \"variable\", \"Target\"]\n",
    "df_submission = df_submission[['ID', \"Target\"]].set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(pwd + \"/data/submissions/submission_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.1 ('zindi_llm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1b94373ed21143aa54ae29a501b4c41cca272fcc00b21ffb9f53282b803de8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
