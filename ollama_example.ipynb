{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/zindi_llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import torch\n",
    "from utils.embeddings import Embedder\n",
    "from utils.preprocess import create_sentance_booklet, create_faise_index\n",
    "import faiss\n",
    "from utils.utils import search_content, read_booklets, retrieve_booklet_text, clean_text, reformat_abbreviations\n",
    "from models.ollama import Ollama\n",
    "from utils.response_generator import get_response, extract_keyword, get_paragraph_info, find_matching_paragraphs\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "import sentence_transformers\n",
    "from utils.vector_store import create_chunks, embed_booklets\n",
    "from utils.postprocessing import clean_book_info, format_keywords, first_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silly Mac that forces me to change the environmental variable to prevent issues running transformers\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd  = str(pathlib.Path().cwd().parent.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet = read_booklets((pwd + \"/data/data/booklets/\"))\n",
    "df_train = pd.read_csv(pwd + \"/data/data/Train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Clean some of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet['cleanText'] = df_booklet['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data\n",
    "We will refromat the abbreviation text. We will then also remove any other unicode characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = df_booklet[df_booklet['text'].str.lstrip().str.startswith(\"AAR\")].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx in abbreviations:\n",
    "    abrv_dict = reformat_abbreviations(df_booklet.iloc[indx]['text'])\n",
    "    new_abrv_text = [(key +\" : \"+ value) for key, value in  abrv_dict.items()]\n",
    "    abrv_df = pd.DataFrame({\"text\": new_abrv_text,\n",
    "                            \"cleanText\": new_abrv_text})\n",
    "    abrv_df['index'] = df_booklet.iloc[indx]['index']\n",
    "    abrv_df['book'] = df_booklet.iloc[indx]['book']\n",
    "    df_booklet = pd.concat([df_booklet, abrv_df[df_booklet.drop(\"level_0\", axis=1).columns]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove abbreviation indexes\n",
    "df_booklet = df_booklet.drop(abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all empty strings\n",
    "df_booklet = df_booklet[df_booklet['cleanText'] != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet = df_booklet.drop(['level_0', 'text'], axis=1)\n",
    "df_booklet.columns = ['paragraph',  'book', 'cleanText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed and Create Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet['cleanText'] = [x+\" \" for x in df_booklet['cleanText']]\n",
    "df_booklet.sort_values(by=['book', 'paragraph'], inplace=True)\n",
    "docs = \"\".join(df_booklet['cleanText'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet['len'] = [len(text) for text in df_booklet['cleanText'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get new splits \n",
    "sum = 0\n",
    "counter = 0\n",
    "splits = []\n",
    "\n",
    "for x in df_booklet['len']:\n",
    "    sum = sum + x\n",
    "    if sum == 500:\n",
    "        splits.append(counter)\n",
    "        counter = counter + 1\n",
    "        sum = 0\n",
    "    elif sum > 500:\n",
    "        counter = counter + 1\n",
    "        splits.append(counter)\n",
    "        sum = sum - 500\n",
    "    else:\n",
    "        splits.append(counter)\n",
    "\n",
    "\n",
    "df_booklet['group'] = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = df_booklet.groupby('group')['book'].agg(lambda x: list(set(x)))\n",
    "paragraphs = df_booklet.groupby('group')['paragraph'].agg(lambda x: sorted(list(set(x))))\n",
    "groups = list(set(df_booklet.group.values))\n",
    "text = df_booklet.groupby('group')['cleanText'].agg(lambda x: \"\".join(list(x)))\n",
    "\n",
    "df_booklet_new = pd.DataFrame({\"books\": books, \"paragraphs\": paragraphs, \"group\": groups, \"cleanText\":text}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets reformat the paragraphs. If they all appear in the same book we will only report the min and max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet_new['paragraphs'] = df_booklet_new.apply(lambda row: \",\".join([str(x) for x in row['paragraphs']]) if len(row['books']) > 1 else\n",
    "                                                                str(row['paragraphs'][0]) if (len(row['books']) == 1) and (len(row['paragraphs']) == 1)\n",
    "                                                                else \"-\".join([str(row[\"paragraphs\"][0]), str(row[\"paragraphs\"][-1])]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booklet_new['books'] = df_booklet_new.apply(lambda row: \",\".join(list(set(row['books']))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = embed_booklets(df_booklet_new,recreate_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem like the following steps will have to be taken:\n",
    "\n",
    "- embed booklet\n",
    "- embed search phrase\n",
    "- use search phrase embedding to search for relevant text in booklet\n",
    "- retrive all relevant text from booklet\n",
    "- format search phrase and into prompt for LLM\n",
    "- Send promt to LLM and return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "To use ollama install follow these instructions:\n",
    "\n",
    "- Download and install Ollama onto the available supported platforms (including Windows Subsystem for Linux) (https://ollama.com/)\n",
    "- Fetch available LLM model via ollama pull llama2 or ollama pull phi\n",
    "\n",
    "This will download the default tagged version of the model. Typically, the default points to the latest, smallest sized-parameter model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = Ollama(model=\"phi\", gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(pwd +  \"/data/data/Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = []\n",
    "responses = []\n",
    "for index, row in df_train.head(5).iterrows():\n",
    "    question = row['Question Text']\n",
    "    docs = vector_store.similarity_search_with_score(question, k=2)\n",
    "    booklet_matches = [doc[0].page_content for doc in docs]\n",
    "    response = get_response(text=question, llm=llm_model, booklet_matches=booklet_matches, text_column=\"text\")\n",
    "    keywords = llm_model.generate(f\"Generate keywords form the following text {response['answer']}\")\n",
    "    book_docs = vector_store.similarity_search(response['answer'], k=1)\n",
    "\n",
    "    response[\"book\"] = \"TG Booklet \" + book_docs[0].metadata[\"books\"][-1]\n",
    "    response[\"paragraph\"] = book_docs[0].metadata[\"paragraphs\"]\n",
    "    response['keywords'] = format_keywords(keywords)\n",
    "    responses.append(response)\n",
    "\n",
    "    scores = scorer.score(response['answer'], row['Question Answer'])\n",
    "    rouge_scores.append(scores['rouge1'][-1])\n",
    "    scores = scorer.score(response['keywords'], row['Keywords'])\n",
    "    rouge_scores.append(scores['rouge1'][-1])\n",
    "    scores = scorer.score(response['book'], row['Reference Document'])\n",
    "    rouge_scores.append(scores['rouge1'][-1])\n",
    "    scores = scorer.score(response['paragraph'], row['Paragraph(s) Number'])\n",
    "    rouge_scores.append(scores['rouge1'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(pwd +  \"/data/data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(columns=['answer', 'book', 'paragraphs', 'keywords', 'ID', 'Question'])\n",
    "df_submission.to_csv(pwd + \"/data/submissions/submission_v9_temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "responses = []\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    id = row['ID']\n",
    "    question = row['Question Text']\n",
    "    docs = vector_store.similarity_search_with_score(question, k=2)\n",
    "    booklet_matches = [doc[0].page_content for doc in docs]\n",
    "    response = get_response(text=question, llm=llm_model, booklet_matches=booklet_matches, text_column=\"text\")\n",
    "    keywords = llm_model.generate(f\"Generate keywords form the following text {response['answer']}\")\n",
    "    book_docs = vector_store.similarity_search(response['answer'], k=1)\n",
    "\n",
    "    response[\"book\"] = \"TG Booklet \" + book_docs[0].metadata[\"books\"][-1]\n",
    "    response[\"paragraph\"] = book_docs[0].metadata[\"paragraphs\"]\n",
    "    response['keywords'] = format_keywords(keywords)\n",
    "    df_responses = pd.DataFrame([response])\n",
    "    df_responses['ID'] = id\n",
    "    df_responses['Question'] = question\n",
    "    csv_file_path = pwd + \"/data/submissions/submission_v9_temp.csv\"\n",
    "    df_responses[['answer', 'book', 'paragraph', 'keywords', 'ID', 'Question']].to_csv(csv_file_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses = pd.read_csv(pwd +  \"/data/submissions/submission_v9_temp.csv\")\n",
    "df_responses.columns = ['answer', 'book', 'paragraphs', 'keywords', 'ID', 'Question', 'None']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submissoion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.drop(['None'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.columns = ['question_answer', 'reference_document', 'paragraph(s)_number', 'keywords', 'ID', 'Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.melt(df_responses, id_vars=['ID'], value_vars=['question_answer', 'reference_document', 'paragraph(s)_number', \"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission['ID'] = df_submission['ID'] + '_' + df_submission['variable']\n",
    "df_submission.columns = [\"ID\", \"variable\", \"Target\"]\n",
    "df_submission = df_submission[['ID', \"Target\"]].set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(pwd + \"/data/submissions/submission_v9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
